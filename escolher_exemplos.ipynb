{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d36cd2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: 1858 exemplos, classes: {0: 1273, -1: 417, 1: 168}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('train.csv')\n",
    "val = pd.read_csv('val.csv')\n",
    "train_oversampling = pd.read_csv('train_oversampling.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "y_train= train['classe']\n",
    "y_train= train_oversampling['classe']\n",
    "y_val=val['classe']\n",
    "y_test=test['classe']\n",
    "\n",
    "print(f\"Test set: {len(test)} exemplos, classes: {test['classe'].value_counts().to_dict()}\")\n",
    "#train_oversampling = pd.read_csv('train_oversampling.csv') acho que nao é preciso "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b236b954",
   "metadata": {},
   "source": [
    "# Identificar exemplos que representam bem cada classe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8494328b",
   "metadata": {},
   "source": [
    "## abordagem através da entropia da classificação de cada exemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602ae89e",
   "metadata": {},
   "source": [
    "classificar os exemplos de treino com todos os modelos de machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc6e8769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros: {'max_features': 2611, 'ngram_min': 1, 'ngram_max': 2}\n",
      "Melhor F1: 0.6037149861818661\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import optuna\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def objective_count_vec(trial, conj_train, conj_val):\n",
    "    # Parâmetros a otimizar\n",
    "    max_features = trial.suggest_int(\"max_features\", 500, 5000)\n",
    "    ngram_min = trial.suggest_int(\"ngram_min\", 1, 2)\n",
    "    ngram_max = trial.suggest_int(\"ngram_max\", ngram_min, 3)  # garante ngram_max >= ngram_min\n",
    "\n",
    "    # CountVectorizer com parâmetros sugeridos\n",
    "    c_vect = CountVectorizer(\n",
    "        token_pattern=r\"(?u)\\b\\w+\\b|[^\\w\\s]\",\n",
    "        ngram_range=(ngram_min, ngram_max),\n",
    "        strip_accents='unicode',\n",
    "        max_features=max_features\n",
    "    )\n",
    "\n",
    "    # Transformar textos\n",
    "    text_train = conj_train['sentences'].apply(lambda tokens: ' '.join(tokens))\n",
    "    text_val   = conj_val['sentences'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "    X_train = c_vect.fit_transform(text_train)\n",
    "    X_val   = c_vect.transform(text_val)\n",
    "\n",
    "    y_train = conj_train['classe'] \n",
    "    y_val   = conj_val['classe']\n",
    "\n",
    "    # Treinar Logistic Regression\n",
    "    model = LogisticRegression(max_iter=500)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_val)\n",
    "\n",
    "    # Métrica: F1 macro\n",
    "    score = f1_score(y_val, preds, average='macro')\n",
    "    return score \n",
    "\n",
    "def count_vec(train, val, test):\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective_count_vec(trial, train,val),\n",
    "                   n_trials=30, n_jobs=-1, show_progress_bar=False)\n",
    "    \n",
    "    best_params=study.best_params\n",
    "    print(\"Melhores parâmetros:\", study.best_params)\n",
    "    print(\"Melhor F1:\", study.best_value)\n",
    "    \n",
    "    c_vect = CountVectorizer(\n",
    "        token_pattern=r\"(?u)\\b\\w+\\b|[^\\w\\s]\",\n",
    "        ngram_range=(best_params['ngram_min'], best_params['ngram_max']),\n",
    "        strip_accents='unicode',\n",
    "        max_features=best_params['max_features']\n",
    "    )\n",
    "\n",
    "    # Transformar textos\n",
    "    text_train = train['sentences'].apply(lambda tokens: ' '.join(tokens))\n",
    "    text_val = val['sentences'].apply(lambda tokens: ' '.join(tokens))\n",
    "    text_test = test['sentences'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "    # Fit no treino, transform nos outros conjuntos\n",
    "    X_train = c_vect.fit_transform(text_train)\n",
    "    X_val = c_vect.transform(text_val)\n",
    "    X_test = c_vect.transform(text_test)\n",
    "\n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "X_train_count_vec, X_val_count_vec, _ = count_vec(train, val, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6800cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aplicar modelos de ML\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#NAIVE BAYES ------------------------------\n",
    "def objective_naive_MN(trial, X_train, X_val, y_train, y_val):\n",
    "    # Hiperparâmetros a otimizar\n",
    "    alpha = trial.suggest_float(\"alpha\", 1e-3, 2.0, log=True)\n",
    "    \n",
    "    # Modelo\n",
    "    clf = MultinomialNB(alpha=alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_val_pred = clf.predict(X_val)\n",
    "    f1 = f1_score(y_val, y_val_pred, average='weighted')  \n",
    "    \n",
    "    return f1\n",
    "def Naive_Bayes_MN(X_train, X_val, y_train, y_val, n_trials=30):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective_naive_MN(trial, X_train, X_val, y_train, y_val),\n",
    "                   n_trials=n_trials, n_jobs=-1, show_progress_bar=False)\n",
    "    \n",
    "    print(\"Melhores hiperparâmetros encontrados:\")\n",
    "    print(study.best_params)\n",
    "    print(f\"Melhor F1 (validação): {study.best_value:.4f}\")\n",
    "    \n",
    "    # Treinar o modelo final com o melhor alpha\n",
    "    best_model = MultinomialNB(**study.best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Previsão no conjunto de teste\n",
    "    y_test_pred = best_model.predict(X_train)\n",
    "    \n",
    "    return y_test_pred\n",
    "\n",
    "#SVM ---------------------\n",
    "def objective_svm(trial, X_train, X_val, y_train, y_val):\n",
    "\n",
    "    #Definir hiperparâmetros a testar \n",
    "    C = trial.suggest_float('C', 0.01, 10.0, log=True)\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly'])\n",
    "    gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "    degree = trial.suggest_int('degree', 2, 5) if kernel == 'poly' else 3\n",
    "\n",
    "    #Criar modelo SVM \n",
    "    model = SVC(C=C, kernel=kernel, gamma=gamma, degree=degree)\n",
    "    model = SVC(C=C, kernel=kernel, gamma=gamma, degree=degree)\n",
    "\n",
    "    #treinar modelo com conjunto de treino\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    #prever no conjunto de validação\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    #calcular f1\n",
    "    f1 = f1_score(y_val, y_pred, average='macro')\n",
    "\n",
    "    return f1\n",
    "\n",
    "def svm(X_train, X_val, y_train, y_val, n_trials=30):\n",
    "    # Chamar o Optuna para otimizar os parâmetros\n",
    "    objeto_para_otimizar = optuna.create_study(direction='maximize')\n",
    "    objeto_para_otimizar.optimize(lambda trial: objective_svm(trial, X_train, X_val, y_train, y_val),\n",
    "                   n_trials=n_trials, n_jobs=-1, show_progress_bar=False )\n",
    "\n",
    "    print(\"Melhores hiperparâmetros encontrados:\")\n",
    "    print(objeto_para_otimizar.best_params)\n",
    "\n",
    "    # Treinar o melhor modelo\n",
    "    best_model = SVC(**objeto_para_otimizar.best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    #  Avaliar no conjunto de teste\n",
    "    y_pred_test = best_model.predict(X_train)\n",
    "\n",
    "    return y_pred_test\n",
    "\n",
    "#KNN---------------------------------------\n",
    "def KNN(X_train, y_train, X_val, y_val):\n",
    "    k_values = range(1, 200,10)  # Determinar  intervalo de valores a testar para k\n",
    "    best_k = 1\n",
    "    best_f1=0\n",
    "    k_list = []\n",
    "    f1_list = [] \n",
    "\n",
    "    for k in k_values:\n",
    "\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)  \n",
    "        knn.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_KNN = knn.predict(X_val) \n",
    "        f1score = f1_score(y_val, y_pred_KNN, average= 'macro')\n",
    "\n",
    "        k_list.append(k)\n",
    "        f1_list.append(f1score)\n",
    "\n",
    "        if f1score > best_f1: \n",
    "            best_f1 = f1score\n",
    "            best_k = k\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=best_k)  \n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_KNN= knn.predict(X_train) \n",
    "    print(f\"Best k: {best_k}\")\n",
    "\n",
    "    return y_pred_KNN\n",
    "\n",
    "#DECISION TREE--------------------\n",
    "def objective_decision_tree(trial, X_train, X_val, y_train, y_val):\n",
    "    # Hiperparâmetros a otimizar\n",
    "    criterion = trial.suggest_categorical(\"criterion\", [\"entropy\", \"gini\"])\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 50)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "    ccp_alpha = trial.suggest_float(\"ccp_alpha\", 0.0, 0.01)\n",
    "    \n",
    "    # Criar o modelo com os parâmetros sugeridos\n",
    "    model = DecisionTreeClassifier(\n",
    "        criterion=criterion,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        ccp_alpha=ccp_alpha,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    f1 = f1_score(y_val, y_pred, average=\"macro\")\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "def decision_tree(X_train, y_train, X_val, y_val, n_trials=30):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective_decision_tree(trial, X_train, X_val, y_train, y_val),\n",
    "                   n_trials=n_trials, n_jobs=-1, show_progress_bar=False)\n",
    "\n",
    "    print(\"Melhores hiperparâmetros encontrados:\")\n",
    "    print(study.best_params)\n",
    "    print(\"Melhor F1 obtido:\", study.best_value)\n",
    "\n",
    "    # Treinar o modelo final com os melhores parâmetros\n",
    "    best_model = DecisionTreeClassifier(**study.best_params, random_state=42)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Prever no conjunto de teste\n",
    "    y_pred_test = best_model.predict(X_train)\n",
    "\n",
    "    return y_pred_test\n",
    "\n",
    "#RANDOM FLOREST---------------\n",
    "def objective_random_forest(trial, X_train, X_val, y_train, y_val):\n",
    "    # Hiperparâmetros a otimizar\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 300, step=50)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 5, 50)\n",
    "    max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None])\n",
    "    criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"])\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "\n",
    "    # Criar o modelo\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        max_features=max_features,\n",
    "        criterion=criterion,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    f1 = f1_score(y_val, y_pred, average=\"macro\")\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "def random_florest(X_train, y_train, X_val, y_val, n_trials=50):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective_random_forest(trial, X_train, X_val, y_train, y_val),\n",
    "                   n_trials=n_trials, n_jobs=-1, show_progress_bar=False)\n",
    "\n",
    "    print(\"Melhores hiperparâmetros Random Forest:\")\n",
    "    print(study.best_params)\n",
    "    print(\"Melhor F1 obtido:\", study.best_value)\n",
    "\n",
    "    best_model = RandomForestClassifier(**study.best_params, random_state=42, n_jobs=-1)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_test = best_model.predict(X_train)\n",
    "\n",
    "    return y_pred_test\n",
    "\n",
    "#NEURAL NETWORK------------------------\n",
    "\n",
    "def objective_NN(X_train, y_train, X_val, y_val, trial):\n",
    "    hidden_layer_sizes = trial.suggest_categorical('hidden_layer_sizes', [50, 100, 200])\n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'tanh'])\n",
    "    learning_rate_init = trial.suggest_float('learning_rate_init', 1e-4, 1e-2, log=True)\n",
    "\n",
    "    clf = MLPClassifier(\n",
    "        hidden_layer_sizes=hidden_layer_sizes,\n",
    "        activation=activation,\n",
    "        learning_rate_init=learning_rate_init,\n",
    "        max_iter=300,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_val)\n",
    "    f1 = f1_score(y_val, preds, average='macro')\n",
    "    return f1\n",
    "\n",
    "def neural_network(X_train, y_train, X_val, y_val):\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective_NN(X_train, y_train, X_val, y_val, trial), n_trials=30)\n",
    "    print(\"Melhores parâmetros:\", study.best_params)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_model = MLPClassifier(\n",
    "        hidden_layer_sizes=best_params['hidden_layer_sizes'],\n",
    "        activation=best_params['activation'],\n",
    "        learning_rate_init=best_params['learning_rate_init'],\n",
    "        max_iter=300,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    best_model.fit(X_train, y_train)\n",
    "    previsoes = best_model.predict(X_train)\n",
    "    \n",
    "    return previsoes\n",
    "\n",
    "#REGRESSÃO LOGÍSTICA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "import optuna\n",
    "\n",
    "def objective_LR(X_train, y_train, X_val, y_val, trial):\n",
    "    # Hiperparâmetros a otimizar\n",
    "    C = trial.suggest_float('C', 1e-3, 1e3, log=True)\n",
    "    penalty = trial.suggest_categorical('penalty', ['l1', 'l2'])\n",
    "    solver = trial.suggest_categorical('solver', ['liblinear', 'saga'])  # compatíveis com L1 e L2\n",
    "    max_iter = trial.suggest_int('max_iter', 100, 1000)\n",
    "\n",
    "    model = LogisticRegression(\n",
    "        C=C,\n",
    "        penalty=penalty,\n",
    "        solver=solver,\n",
    "        max_iter=max_iter,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_val)\n",
    "\n",
    "    f1 = f1_score(y_val, preds, average='macro')\n",
    "    return f1\n",
    "\n",
    "\n",
    "def logistic_regression(X_train, y_train, X_val, y_val):\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective_LR(X_train, y_train, X_val, y_val, trial), n_trials=30)\n",
    "\n",
    "    print(\"Melhores parâmetros encontrados:\")\n",
    "    print(study.best_params)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_model = LogisticRegression(\n",
    "        **best_params,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    previsoes = best_model.predict(X_train)\n",
    "\n",
    "    return previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2ae0f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores hiperparâmetros encontrados:\n",
      "{'alpha': 1.0339354147373747}\n",
      "Melhor F1 (validação): 0.7621\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'C': 1.5498565056234925, 'kernel': 'linear', 'gamma': 'scale'}\n",
      "Best k: 1\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'criterion': 'entropy', 'max_depth': 15, 'min_samples_split': 19, 'min_samples_leaf': 3, 'ccp_alpha': 0.0011693454017834804}\n",
      "Melhor F1 obtido: 0.5479818820164344\n",
      "Melhores hiperparâmetros Random Forest:\n",
      "{'n_estimators': 150, 'max_depth': 34, 'max_features': None, 'criterion': 'entropy', 'min_samples_split': 6, 'min_samples_leaf': 2}\n",
      "Melhor F1 obtido: 0.5514269905269964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros: {'hidden_layer_sizes': 50, 'activation': 'relu', 'learning_rate_init': 0.00010394738464373503}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros encontrados:\n",
      "{'C': 2.9655647850643545, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 986}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Count Vectorizer\n",
    "predicted_NB = Naive_Bayes_MN(X_train_count_vec,X_val_count_vec,y_train,y_val)\n",
    "predicted_SVM= svm(X_train_count_vec, X_val_count_vec, y_train, y_val, n_trials=30) \n",
    "predicted_KNN= KNN(X_train_count_vec, y_train, X_val_count_vec, y_val) \n",
    "predicted_DT= decision_tree(X_train_count_vec, y_train, X_val_count_vec, y_val) \n",
    "predicted_RF= random_florest(X_train_count_vec, y_train, X_val_count_vec, y_val) \n",
    "predicted_NN= neural_network(X_train_count_vec, y_train, X_val_count_vec, y_val) \n",
    "predicted_LR= logistic_regression(X_train_count_vec, y_train, X_val_count_vec, y_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b705fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''predicts = pd.DataFrame({'predicted_train_NB': predicted_NB,\n",
    "            'predicted_train_SVM': predicted_SVM,\n",
    "            'predicted_train_KNN': predicted_KNN,\n",
    "            'predicted_train_DT': predicted_DT,\n",
    "            'predicted_train_RF': predicted_RF,\n",
    "            'predicted_train_NN': predicted_NN,\n",
    "            'predicted_train_LR': predicted_LR})\n",
    "predicts[\"Real\"] = y_train.values  \n",
    "predicts[\"ID\"] = predicts.index\n",
    "predicts.to_csv('predicts_train.csv')\n",
    "'''\n",
    "predicts = pd.read_csv('predicts_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b559e7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classe 0:\n",
      "     ID                                          sentences  Real  Entropia\n",
      "0     0  Neymar, que a partir de 20 de novembro vai lid...     0  0.000000\n",
      "1  3463  Em 1997, viu sua relação com os britânicos ser...     0  0.543564\n",
      "2  3417  Como outros países, o gigante asiático também ...     0  0.543564\n",
      "3  3418  A Virginia Tech, com 26 mil alunos, fica a 390...     0  0.543564\n",
      "4  3419  Mais tarde, em 1999, ele reprisaria o papel e ...     0  0.543564\n",
      "Valores distintos de entropia: 7\n",
      "Entropia 0 nesta classe: 1\n",
      "\n",
      "Classe -1:\n",
      "    ID                                          sentences  Real  Entropia\n",
      "0  207                       O que nós queremos com isso?    -1  0.543564\n",
      "1  217  A gente nunca imaginou que poderia estar do la...    -1  0.543564\n",
      "2  219  \"Webb, agora, está voando sozinho\", informou a...    -1  0.543564\n",
      "3  220  \"Obstáculos:\"No início da campanha o noticiári...    -1  0.543564\n",
      "4  221  \"É sobre isso, é sobre o idealismo da família,...    -1  0.543564\n",
      "Valores distintos de entropia: 7\n",
      "Entropia 0 nesta classe: 0\n",
      "\n",
      "Classe 1:\n",
      "     ID                                          sentences  Real  Entropia\n",
      "0  3460  Durante a campanha, Moro voltou a flertar com ...     1  0.543564\n",
      "1  3375  O troco veio logo em seguida, com Vagner Love ...     1  0.543564\n",
      "2    82  Aos 31, Kaká pegou sobra na entrada da área e ...     1  0.543564\n",
      "3    64  A emenda segue agora para votação na Câmara, o...     1  0.543564\n",
      "4   651  A deputada federal eleita Rosangela Moro (Uniã...     1  0.543564\n",
      "Valores distintos de entropia: 8\n",
      "Entropia 0 nesta classe: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def analisar_entropia(train, predicts):\n",
    "    # Criar dataframe base\n",
    "    merged = pd.DataFrame()\n",
    "    merged[\"Real\"] = train[\"classe\"]\n",
    "    merged[\"sentences\"] = train[\"sentences\"]\n",
    "\n",
    "    # Adicionar previsões dos modelos\n",
    "    for nome, preds in predicts.items():\n",
    "        merged[nome] = preds\n",
    "\n",
    "    merged[\"ID\"] = merged.index\n",
    "\n",
    "    # Função para calcular entropia de uma linha de previsões\n",
    "    def calcular_entropia(preds_linha):\n",
    "        _, contagens = np.unique(preds_linha, return_counts=True)\n",
    "        probs = contagens / contagens.sum()\n",
    "        return entropy(probs, base=2)\n",
    "\n",
    "    # Calcular entropia para cada linha\n",
    "    pred_cols = [c for c in merged.columns if c not in [\"Real\", \"sentences\", \"ID\"]]\n",
    "    merged[\"Entropia\"] = merged[pred_cols].apply(lambda r: calcular_entropia(r.values), axis=1)\n",
    "\n",
    "    # Criar DataFrames por classe\n",
    "    dfs_por_classe = {}\n",
    "    for classe in merged[\"Real\"].unique():\n",
    "        df_c = merged[merged[\"Real\"] == classe][[\"ID\", \"sentences\", \"Real\", \"Entropia\"]]\n",
    "        df_c = df_c.sort_values(by=\"Entropia\", ascending=True).reset_index(drop=True)\n",
    "        dfs_por_classe[classe] = df_c\n",
    "\n",
    "    for classe, df_c in dfs_por_classe.items():\n",
    "        print(f\"\\nClasse {classe}:\")\n",
    "        print(df_c.head())\n",
    "        print(f\"Valores distintos de entropia: {df_c['Entropia'].nunique()}\")\n",
    "        print(f\"Entropia 0 nesta classe: {sum(df_c['Entropia'] == 0)}\")\n",
    "\n",
    "    return dfs_por_classe\n",
    "\n",
    "# Exemplo de uso:\n",
    "dfs_por_classe = analisar_entropia(train, predicts)\n",
    "df_facto = dfs_por_classe.get(0)\n",
    "df_vies = dfs_por_classe.get(1)\n",
    "df_citacao = dfs_por_classe.get(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84698eac",
   "metadata": {},
   "source": [
    "### Abordagem com o k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "23728131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "def kmeans(sentences, classes_reais, n_clusters=3):\n",
    "    # Vetorização com CountVectorizer\n",
    "    c_vect = CountVectorizer(\n",
    "        token_pattern=r'\\\"|[A-Za-zÀ-ÿ]+|[^\\w\\s]', \n",
    "        ngram_range=(1, 3),    \n",
    "        strip_accents='unicode',\n",
    "        max_features=1000\n",
    "    )\n",
    "    \n",
    "    vetores = c_vect.fit_transform(sentences).toarray() \n",
    "\n",
    "    # aplicar KMeans \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(vetores)\n",
    "\n",
    "    centroids = kmeans.cluster_centers_\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"sentenca\": sentences,\n",
    "        \"classe_real\": classes_reais,\n",
    "        \"cluster\": labels\n",
    "    })\n",
    "\n",
    "    print(\"\\nClasses por cluster\")\n",
    "    dist = df.groupby([\"cluster\", \"classe_real\"]).size().unstack(fill_value=0)\n",
    "    dist.head()\n",
    "\n",
    "    # classe dominante por cluster\n",
    "    classe_do_cluster = dist.idxmax(axis=1)\n",
    "    print(\"Classe atribuída a cada cluster pela maioria:\")\n",
    "    print(classe_do_cluster, \"\\n\")\n",
    "\n",
    "    # frases mais representativas por cluster)\n",
    "    print(\"\\nFrases mais proximas de cada cluster por ordem\")\n",
    "\n",
    "    for c in range(n_clusters):\n",
    "        ind = np.where(labels == c)[0]     \n",
    "        #calcular distancias de cada frase ao centroide                    \n",
    "        distancias = np.linalg.norm(vetores[ind] - centroids[c], axis=1)\n",
    "        top3 = ind[np.argsort(distancias)[:3]]\n",
    "\n",
    "        print(f\"\\nCluster {c} (classe provável: {classe_do_cluster[c]})\\n\")\n",
    "        for i, idx in enumerate(top3):\n",
    "            print(f\"{i+1}. {sentences[idx]} Classe real: = {classes_reais[idx]}\")\n",
    "\n",
    "    return df, classe_do_cluster, centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54d0c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classes por cluster\n",
      "Classe atribuída a cada cluster pela maioria:\n",
      "cluster\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "dtype: int64 \n",
      "\n",
      "\n",
      "Frases mais proximas de cada cluster por ordem\n",
      "\n",
      "Cluster 0 (classe provável: 0)\n",
      "\n",
      "1. Em 2012, motivada pelo declínio de Berlusconi, criou a própria agremiação. Classe real: = 0\n",
      "2. Em 2014, o livro virou peça de teatro, estrelada por Nicette Bruno. Classe real: = 0\n",
      "3. A plataforma Conecte SUS, que fornece o certificado nacional de vacinação, também saiu do ar. Classe real: = 0\n",
      "\n",
      "Cluster 1 (classe provável: 0)\n",
      "\n",
      "1. O medalhista olímpico, que fez parte do quarteto das eliminatórias pela manhã, contou com a ajuda dos amigos de seleção Bruno Fratus, Matheus Santana, Marcelo Chierighini e João de Lucca para garantir sua 19ª medalha. Classe real: = 0\n",
      "2. Por exemplo, os integralistas se opõem à teologia da libertação, uma espécie de campo progressista da religião, e ao aborto, inclusive em caso de estupro. Classe real: = 0\n",
      "3. Autora de grandes sucessos, como Perdas e Ganhos, de 2003, que vendeu quase um milhão de exemplares, ela lançou em 2020 Classe real: = 0\n",
      "\n",
      "Cluster 2 (classe provável: 0)\n",
      "\n",
      "1. Leia abaixo. Classe real: = 0\n",
      "2. Hogwarts. Classe real: = 0\n",
      "3. Entrei. Classe real: = -1\n"
     ]
    }
   ],
   "source": [
    "sentences = train[\"sentences\"].astype(str).tolist()\n",
    "classes = train[\"classe\"].tolist()\n",
    "\n",
    "df_clusters, classe_cluster, centroides = kmeans(sentences, classes, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb2351a",
   "metadata": {},
   "source": [
    "Abordagem com a distância dos vetores do Count Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "467a14cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Index  classe  rank                                           sentenca  \\\n",
      "0   3309       0     1                  Artigo 14, presunção de inocência   \n",
      "1   1904       0     2            Novamente, Kaká chutou de fora da área.   \n",
      "2   1798       0     3       Ameaças de Putin enfraquecem o tabu nuclear.   \n",
      "3   1932      -1     1                            É padrão Zâmbia, Gabão.   \n",
      "4    324      -1     2   Mostrei toda a minha indignação ali\", completou.   \n",
      "5     61      -1     3                Não sejam tocados, abracem a causa.   \n",
      "6   1260       1     1  Aos 19, o atacante do CSKA perdeu grande chanc...   \n",
      "7   1113       1     2                    Aos 32min, Kaká tentou de novo.   \n",
      "8   2290       1     3                       A posição de Taiwan é firme:   \n",
      "\n",
      "   distancia_centroide  \n",
      "0             2.026610  \n",
      "1             2.106179  \n",
      "2             2.184103  \n",
      "3             1.877806  \n",
      "4             2.006368  \n",
      "5             2.010203  \n",
      "6             2.249491  \n",
      "7             2.391749  \n",
      "8             2.547488  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def Count_vec_centroid(sentences, classes, top_n=3):\n",
    "    \n",
    "    # Vetorização\n",
    "    vectorizer = CountVectorizer(token_pattern=r'\\\"|[A-Za-zÀ-ÿ]+|[^\\w\\s]', \n",
    "        ngram_range=(1, 3),    \n",
    "        strip_accents='unicode',\n",
    "        max_features=1000)\n",
    "    X = vectorizer.fit_transform(sentences).toarray() \n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        \"sentenca\": sentences,\n",
    "        \"classe\": classes\n",
    "    })\n",
    "    \n",
    "    rows = []\n",
    "\n",
    "    for classe in df[\"classe\"].unique():\n",
    "        idx_classe = df[df[\"classe\"] == classe].index\n",
    "        vetores_classe = X[idx_classe]\n",
    "        \n",
    "        # Calcula centróide da classe\n",
    "        centroide = vetores_classe.mean(axis=0)\n",
    "        \n",
    "        # Distância Euclidiana ao centróide\n",
    "        distancias = np.linalg.norm(vetores_classe - centroide, axis=1)\n",
    "        \n",
    "        # Top frases mais próximas do centróide\n",
    "        top_idx = idx_classe[np.argsort(distancias)[:top_n]]\n",
    "\n",
    "        for i, idx in enumerate(top_idx):\n",
    "            rows.append({\n",
    "                \"Index\": idx,\n",
    "                \"classe\": int(classe),\n",
    "                \"rank\": i+1,\n",
    "                \"sentenca\": df.loc[idx, \"sentenca\"],\n",
    "                \"distancia_centroide\": distancias[np.argsort(distancias)[:top_n]][i]\n",
    "            })\n",
    "    \n",
    "    resultado_df = pd.DataFrame(rows)\n",
    "    return resultado_df\n",
    "\n",
    "sentences = train[\"sentences\"].astype(str).tolist()\n",
    "classes = train[\"classe\"].tolist()\n",
    "top_frases = Count_vec_centroid(sentences, classes, top_n=3)\n",
    "print(top_frases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b669b29f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2297ab36",
   "metadata": {},
   "source": [
    "abordagem da entropia com mais vetorizadores e modelos de ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cb95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT = pd.read_csv('predict_DT.csv')\n",
    "KNN = pd.read_csv('predict_KNN.csv')\n",
    "NB = pd.read_csv('predict_NB.csv')\n",
    "NN = pd.read_csv('predict_NN.csv')\n",
    "SVM = pd.read_csv('predict_SVM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e4e0fd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelos = {\n",
    "    \"NB\": NB,\n",
    "    \"SVM\": SVM,\n",
    "    \"KNN\": KNN,\n",
    "    \"DT\": DT,\n",
    "    \"NN\": NN\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "562acf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "\n",
    "def calcular_entropia(preds_linha):\n",
    "    _, contagens = np.unique(preds_linha, return_counts=True)\n",
    "    probs = contagens / contagens.sum()\n",
    "    return entropy(probs, base=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f83539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Index  classe  rank                                           sentenca  \\\n",
      "0   3309       0     1                  Artigo 14, presunção de inocência   \n",
      "1   1904       0     2            Novamente, Kaká chutou de fora da área.   \n",
      "2   1798       0     3       Ameaças de Putin enfraquecem o tabu nuclear.   \n",
      "3   1932      -1     1                            É padrão Zâmbia, Gabão.   \n",
      "4    324      -1     2   Mostrei toda a minha indignação ali\", completou.   \n",
      "5     61      -1     3                Não sejam tocados, abracem a causa.   \n",
      "6   1260       1     1  Aos 19, o atacante do CSKA perdeu grande chanc...   \n",
      "7   1113       1     2                    Aos 32min, Kaká tentou de novo.   \n",
      "8   2290       1     3                       A posição de Taiwan é firme:   \n",
      "\n",
      "   distancia_centroide  Entropia  Acertos  Acertos_frac  \n",
      "0             2.026610  0.210842       29      0.966667  \n",
      "1             2.106179  0.699843       26      0.866667  \n",
      "2             2.184103  1.052982       22      0.733333  \n",
      "3             1.877806  0.881291       21      0.700000  \n",
      "4             2.006368  0.851933       24      0.800000  \n",
      "5             2.010203  0.468996       27      0.900000  \n",
      "6             2.249491  1.032268       21      0.700000  \n",
      "7             2.391749  1.463458       16      0.533333  \n",
      "8             2.547488  1.429473       14      0.466667  \n"
     ]
    }
   ],
   "source": [
    "entropias = []\n",
    "acertos_frac = []\n",
    "\n",
    "for idx in top_frases[\"Index\"]:\n",
    "    preds_linha = []\n",
    "    acerto = 0\n",
    "    total = 0\n",
    "    \n",
    "    classe_real = top_frases.loc[top_frases[\"Index\"] == idx, \"classe\"].values[0]\n",
    "    \n",
    "    for nome, df in modelos.items():\n",
    "        cols = [c for c in df.columns if c != \"Real\"]\n",
    "        valores = df.loc[idx, cols].values\n",
    "        preds_linha.extend(valores)\n",
    "        \n",
    "        acerto += sum(valores == classe_real)\n",
    "        total += len(cols)\n",
    "    \n",
    "    entropias.append(calcular_entropia(preds_linha))\n",
    "    acertos_frac.append(acerto / total)  \n",
    "\n",
    "top_frases[\"Entropia\"] = entropias\n",
    "top_frases[\"Acertos_frac\"] = acertos_frac\n",
    "\n",
    "print(top_frases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ef340e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facto:\n",
      "                                           sentences  Real  Entropia\n",
      "0  Em 2012, motivada pelo declínio de Berlusconi,...     0       0.0\n",
      "1  Formulador do Real, André Lara Resende declara...     0       0.0\n",
      "2  O presidente de honra da Beija-Flor de Nilópol...     0       0.0\n",
      "3  Ele não recebeu o convite para participar da r...     0       0.0\n",
      "4  Com o resultado, a inflação acumulada no ano d...     0       0.0\n",
      "Entropia 0 nesta classe: 102\n",
      "\n",
      "vies\n",
      "                                           sentences  Real  Entropia\n",
      "0  O gol deu a tranqüilidade necessária ao time b...     1  0.783777\n",
      "1  Na semana passada, Lira manobrou para facilita...     1  0.783777\n",
      "2  Brasil deslancha no fim e goleia Equador em su...     1  0.783777\n",
      "3  Com sua escrita potente, elogiada pela imortal...     1  0.836641\n",
      "4  Livre de marcação, o atacante --após seis part...     1  0.836641\n",
      "Entropia 0 nesta classe: 0\n",
      "\n",
      "citacao\n",
      "                                sentences  Real  Entropia\n",
      "0  ?Isso não é um blefe?, destacou Putin.    -1       0.0\n",
      "1                   \"Onde está a polícia?    -1       0.0\n",
      "2                    Que vergonha é essa?    -1       0.0\n",
      "3            O que nós queremos com isso?    -1       0.0\n",
      "4     \"O que vale mais, a arte ou a vida?    -1       0.0\n",
      "Entropia 0 nesta classe: 6\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "\n",
    "#juntar as previsões horizontalmente\n",
    "merged = pd.DataFrame({\n",
    "    \"Real\": NB[\"Real\"],      # coluna y reais             \n",
    "    \"NB\":  NB.iloc[:,1],\n",
    "    \"SVM\": SVM.iloc[:,1],\n",
    "    \"KNN\": KNN.iloc[:,1],\n",
    "    \"DT\": DT.iloc[:,1],    #colunas dos previstos\n",
    "    \"NN\": NN.iloc[:,1],\n",
    "})\n",
    "\n",
    "merged[\"ID\"] = merged.index #guarda o indice p depois ir buscar a frase\n",
    "\n",
    "#entropia = - somatório (prob_classe . log (prob_classe))\n",
    "#prob_classe ( n vezes classificada / todas as classificações)\n",
    "def calcular_entropia(preds):\n",
    "    _, contagens = np.unique(preds, return_counts=True)\n",
    "    probs = contagens / contagens.sum()\n",
    "    return entropy(probs, base=2)\n",
    "\n",
    "# Para cada modelo, seleciona todas as colunas de previsão exceto \"Real\"\n",
    "dfs = []\n",
    "for df, nome in zip([NB, SVM, KNN, DT, NN],\n",
    "                    [\"NB\",\"SVM\",\"KNN\",\"DT\",\"NN\"]):\n",
    "    cols = [c for c in df.columns if c != \"Real\"]\n",
    "    # renomeia as colunas para não haver conflito\n",
    "    df_renamed = df[cols].add_prefix(f\"{nome}_\")\n",
    "    dfs.append(df_renamed)\n",
    "\n",
    "# concatena horizontalmente\n",
    "merged = pd.concat(dfs, axis=1)\n",
    "merged[\"Real\"] = NB[\"Real\"].values   # y real\n",
    "merged[\"sentences\"] = train[\"sentences\"]\n",
    "\n",
    "# Calcular entropia para cada linha\n",
    "pred_cols = [c for c in merged.columns if c not in [\"Real\", \"sentences\"]]\n",
    "merged[\"Entropia\"] = merged[pred_cols].apply(lambda r: calcular_entropia(r.values), axis=1)\n",
    "\n",
    "# Criar DataFrames separados por classe\n",
    "dfs_por_classe = {}\n",
    "for classe in merged[\"Real\"].unique():\n",
    "    df_c = merged[merged[\"Real\"] == classe][[\"sentences\",\"Real\",\"Entropia\"]]\n",
    "    df_c = df_c.sort_values(by=\"Entropia\", ascending=True).reset_index(drop=True)\n",
    "    dfs_por_classe[classe] = df_c\n",
    "\n",
    "# Extrair dataframes\n",
    "df_facto   = dfs_por_classe.get(0)\n",
    "df_vies    = dfs_por_classe.get(1)\n",
    "df_citacao = dfs_por_classe.get(-1)\n",
    "\n",
    "print(\"facto:\")\n",
    "print(df_facto.head())\n",
    "print(f\"Entropia 0 nesta classe: {sum(df_facto['Entropia'] == 0)}\")\n",
    "\n",
    "print(\"\\nvies\")\n",
    "print(df_vies.head())\n",
    "print(f\"Entropia 0 nesta classe: {sum(df_vies['Entropia'] == 0)}\")\n",
    "\n",
    "print(\"\\ncitacao\")\n",
    "print(df_citacao.head())\n",
    "print(f\"Entropia 0 nesta classe: {sum(df_citacao['Entropia'] == 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe521e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_over = pd.read_csv('predicts_svm_over.csv')\n",
    "DT_over = pd.read_csv('predict_DT_over.csv')\n",
    "KNN_over = pd.read_csv('predict_KNN_over.csv')\n",
    "NB_over= pd.read_csv('predict_NB_over.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a5940566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facto:\n",
      "                                           sentences  Real  Entropia\n",
      "0  Ambos os trens se dirigiam ao sul e levavam mo...     0  0.391244\n",
      "1  O Deic diz que prendeu o garçom e o grafiteiro...     0  0.391244\n",
      "2  As ações são julgadas no plenário virtual do S...     0  0.391244\n",
      "3  Ramos Pereira disse que o encontro \"terá desdo...     0  0.391244\n",
      "4  A Reuters não conseguiu aferir a veracidade da...     0  0.391244\n",
      "Valores distintos de entropia: 107\n",
      "Entropia 0 nesta classe: 0\n",
      "\n",
      "vies\n",
      "                                           sentences  Real  Entropia\n",
      "0  Aos 14, porém, Juan lançou Vagner Love, que ap...     1  0.391244\n",
      "1         Em tom de desafio, Almeida Lima respondeu:     1  0.391244\n",
      "2  O telescópio James Webb, da agência aeroespaci...     1  0.391244\n",
      "3  O Senado tem freado a análise de propostas vot...     1  0.391244\n",
      "4                  A partida começou muito amarrada.     1  0.391244\n",
      "Valores distintos de entropia: 84\n",
      "Entropia 0 nesta classe: 0\n",
      "\n",
      "citacao\n",
      "                                           sentences  Real  Entropia\n",
      "0  \"Continuo entendendo que cada representação de...    -1  0.391244\n",
      "1  Aí eu te pergunto, menina bonitinha se arruman...    -1  0.391244\n",
      "2  Isso não vai alterar o nosso procedimento na C...    -1  0.391244\n",
      "3  Quero ser um marido e pai melhor\", disse Oscar...    -1  0.391244\n",
      "4               \"A eleição vai para o segundo turno.    -1  0.391244\n",
      "Valores distintos de entropia: 97\n",
      "Entropia 0 nesta classe: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "\n",
    "#juntar as previsões horizontalmente\n",
    "merged = pd.DataFrame({\n",
    "    \"Real\": NB_over[\"Real\"],      # coluna y reais             \n",
    "    \"NB\":  NB_over.iloc[:,1],\n",
    "    \"SVM\": svm_over.iloc[:,1],\n",
    "    \"KNN\": KNN_over.iloc[:,1],\n",
    "    \"DT\": DT_over.iloc[:,1],    #colunas dos previstos\n",
    "})\n",
    "\n",
    "merged[\"ID\"] = merged.index #guarda o indice p depois ir buscar a frase\n",
    "\n",
    "#entropia = - somatório (prob_classe . log (prob_classe))\n",
    "#prob_classe ( n vezes classificada / todas as classificações)\n",
    "def calcular_entropia(preds):\n",
    "    _, contagens = np.unique(preds, return_counts=True)\n",
    "    probs = contagens / contagens.sum()\n",
    "    return entropy(probs, base=2)\n",
    "\n",
    "# Para cada modelo, seleciona todas as colunas de previsão exceto \"Real\"\n",
    "dfs = []\n",
    "for df, nome in zip([NB_over, svm_over, KNN_over, DT_over],\n",
    "                    [\"NB_over\",\"svm_over\",\"KNN_over\",\"DT_over\"]):\n",
    "    cols = [c for c in df.columns if c != \"Real\"]\n",
    "    # renomeia as colunas para não haver conflito\n",
    "    df_renamed = df[cols].add_prefix(f\"{nome}_\")\n",
    "    dfs.append(df_renamed)\n",
    "\n",
    "# concatena horizontalmente\n",
    "merged = pd.concat(dfs, axis=1)\n",
    "merged[\"Real\"] = NB_over[\"Real\"].values   # y real\n",
    "merged[\"sentences\"] = train[\"sentences\"]\n",
    "\n",
    "# Calcular entropia para cada linha\n",
    "pred_cols = [c for c in merged.columns if c not in [\"Real\", \"sentences\"]]\n",
    "merged[\"Entropia\"] = merged[pred_cols].apply(lambda r: calcular_entropia(r.values), axis=1)\n",
    "\n",
    "# Criar DataFrames separados por classe\n",
    "dfs_por_classe = {}\n",
    "for classe in merged[\"Real\"].unique():\n",
    "    df_c = merged[merged[\"Real\"] == classe][[\"sentences\",\"Real\",\"Entropia\"]]\n",
    "    df_c = df_c.sort_values(by=\"Entropia\", ascending=True).reset_index(drop=True)\n",
    "    dfs_por_classe[classe] = df_c\n",
    "\n",
    "# Extrair dataframes\n",
    "df_facto   = dfs_por_classe.get(0)\n",
    "df_vies    = dfs_por_classe.get(1)\n",
    "df_citacao = dfs_por_classe.get(-1)\n",
    "\n",
    "print(\"facto:\")\n",
    "print(df_facto.head())\n",
    "print(f\"Valores distintos de entropia: {df_facto['Entropia'].nunique()}\")\n",
    "print(f\"Entropia 0 nesta classe: {sum(df_facto['Entropia'] == 0)}\")\n",
    "\n",
    "print(\"\\nvies\")\n",
    "print(df_vies.head())\n",
    "print(f\"Valores distintos de entropia: {df_vies['Entropia'].nunique()}\")\n",
    "print(f\"Entropia 0 nesta classe: {sum(df_vies['Entropia'] == 0)}\")\n",
    "\n",
    "print(\"\\ncitacao\")\n",
    "print(df_citacao.head())\n",
    "print(f\"Valores distintos de entropia: {df_citacao['Entropia'].nunique()}\")\n",
    "print(f\"Entropia 0 nesta classe: {sum(df_citacao['Entropia'] == 0)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
