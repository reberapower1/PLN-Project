{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5874fce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import optuna\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fddcf97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file          0\n",
       "id_sente      0\n",
       "id_article    0\n",
       "domain        0\n",
       "year          0\n",
       "sentences     0\n",
       "classe        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#carregar dataset\n",
    "df = pd.read_csv('factnews_dataset.csv')\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa80a44",
   "metadata": {},
   "source": [
    "# Pré-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b710fb",
   "metadata": {},
   "source": [
    "divisão dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16d457cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#criar grupo treino, validação e teste\\ntrain_df, test_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df[\\'classe\\'])\\ntrain_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df[\\'classe\\'])\\n\\n#guardar conjuntos em memória\\ntrain_df.to_csv(\"train.csv\", index=False) \\nval_df.to_csv(\"val.csv\", index=False)\\ntest_df.to_csv(\"test.csv\", index=False)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#criar grupo treino, validação e teste\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['classe'])\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['classe'])\n",
    "\n",
    "#guardar conjuntos em memória\n",
    "train_df.to_csv(\"train.csv\", index=False) \n",
    "val_df.to_csv(\"val.csv\", index=False)\n",
    "test_df.to_csv(\"test.csv\", index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9319ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.read_csv('train.csv')\n",
    "val= pd.read_csv('val.csv')\n",
    "test= pd.read_csv('test.csv')\n",
    "\n",
    "train_oversampling= pd.read_csv('train_oversampling.csv')\n",
    "val_oversampling= pd.read_csv('val_oversampling.csv')\n",
    "test_oversampling= pd.read_csv('test_oversampling.csv')\n",
    "\n",
    "y_train= train['classe']\n",
    "y_train_oversampling= train_oversampling['classe']\n",
    "\n",
    "y_val=val['classe']\n",
    "y_val_oversampling = val_oversampling['classe']\n",
    "\n",
    "y_test= test['classe']\n",
    "y_test_oversampling = test_oversampling['classe']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767f6d97",
   "metadata": {},
   "source": [
    "Pré-processamento da primeira meta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b071f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('factnews_dataset.csv')\n",
    "def pre_processamento_meta1(df):\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    #criar coluna tokens\n",
    "    df['tokens'] = df['sentences'].apply(lambda x: nltk.word_tokenize(str(x).lower()))\n",
    "    # Tokenizar sem stopwords\n",
    "    df['tokens'] = df['tokens'].apply(lambda toks: [t for t in toks if t not in stop_words])\n",
    "    return df\n",
    "\n",
    "train_meta1 = pre_processamento_meta1(train)\n",
    "val_meta1 = pre_processamento_meta1(val)\n",
    "test_meta1= pre_processamento_meta1(test)\n",
    "\n",
    "y_train_meta1=train_meta1['classe']\n",
    "y_val_meta1=val_meta1['classe']\n",
    "y_test_meta1=test_meta1['classe']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1708b8",
   "metadata": {},
   "source": [
    "balanceamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f65b63e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.read_csv('train.csv')\n",
    "val= pd.read_csv('val.csv')\n",
    "test= pd.read_csv('test.csv')\n",
    "\n",
    "train_oversampling= pd.read_csv('train_oversampling.csv')\n",
    "val_oversampling= pd.read_csv('val_oversampling.csv')\n",
    "test_oversampling= pd.read_csv('test_oversampling.csv')\n",
    "\n",
    "y_train= train['classe']\n",
    "y_train_oversampling= train_oversampling['classe']\n",
    "\n",
    "y_val=val['classe']\n",
    "y_val_oversampling = val_oversampling['classe']\n",
    "\n",
    "y_test= test['classe']\n",
    "y_test_oversampling = test_oversampling['classe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce488a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conjunto de treino:  classe\n",
      "-1     779\n",
      " 0    2375\n",
      " 1     312\n",
      "Name: count, dtype: int64\n",
      "\n",
      "conjunto de validação:  classe\n",
      "-1    195\n",
      " 0    594\n",
      " 1     78\n",
      "Name: count, dtype: int64\n",
      "\n",
      "conjunto de teste:  classe\n",
      "-1     417\n",
      " 0    1273\n",
      " 1     168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "conjunto de treino + exemplos:  classe\n",
      "-1    2375\n",
      " 0    2375\n",
      " 1    2375\n",
      "Name: count, dtype: int64\n",
      "\n",
      "conjunto de val + exemplos:  classe\n",
      "-1    594\n",
      " 0    594\n",
      " 1    594\n",
      "Name: count, dtype: int64\n",
      "\n",
      "conjunto de teste + exemplos:  classe\n",
      "-1    1273\n",
      " 0    1273\n",
      " 1    1273\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Contagem das classes em cada conjunto\n",
    "print (\"conjunto de treino: \",y_train.value_counts().sort_index())\n",
    "print(\"\\nconjunto de validação: \",y_val.value_counts().sort_index())\n",
    "print(\"\\nconjunto de teste: \", y_test.value_counts().sort_index())\n",
    "print(\"\\nconjunto de treino + exemplos: \", y_train_oversampling.value_counts().sort_index())\n",
    "print(\"\\nconjunto de val + exemplos: \", y_val_oversampling.value_counts().sort_index())\n",
    "print(\"\\nconjunto de teste + exemplos: \", y_test_oversampling.value_counts().sort_index())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "940b76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "def balancear_classes(conjunto):\n",
    "  # Separar cada classe\n",
    "  class_min = conjunto[conjunto['classe'] == -1]\n",
    "  class_med = conjunto[conjunto['classe'] == 1]\n",
    "  class_max = conjunto[conjunto['classe'] == 0]\n",
    "\n",
    "  # Número da classe maior\n",
    "  n_samples = len(class_max)\n",
    "\n",
    "  # Replicar as classes menores até o tamanho da maior\n",
    "  class_min_upsampled = resample(class_min, \n",
    "                              replace=True, \n",
    "                              n_samples=n_samples, \n",
    "                              random_state=42)\n",
    "\n",
    "  class_med_upsampled = resample(class_med, \n",
    "                              replace=True, \n",
    "                              n_samples=n_samples, \n",
    "                            random_state=42)\n",
    "\n",
    "  conjunto_oversampled = pd.concat([class_min_upsampled, class_med_upsampled, class_max])\n",
    "\n",
    "  # baralhar o conjunto \n",
    "  conjunto_oversampled = conjunto_oversampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "  print(conjunto_oversampled['classe'].value_counts())\n",
    "\n",
    "  return conjunto_oversampled\n",
    "\n",
    "#train_oversampling.to_csv('train_oversampling.csv', index=False)\n",
    "#val_oversampling = balancear_classes(val)\n",
    "#test_oversampling= balancear_classes(test)\n",
    "\n",
    "#val_oversampling.to_csv('val_oversampling.csv', index=False)\n",
    "#test_oversampling.to_csv('test_oversampling.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5262d8a4",
   "metadata": {},
   "source": [
    "tokeinização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d610e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\didia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [neym, ,, part, 20, novembr, vai, lider, seleç...\n",
      "1    [forç, internac, transform, quest, taiwan, ass...\n",
      "2    [alianç, mai, quant, parlament, eleit, nome, p...\n",
      "3    [ger, mort, '', ,, declar, cabr, ,, 16ª, entre...\n",
      "4    [bisp, barr, (, ba, ), ,, dom, luiz, flávi, ca...\n",
      "Name: stems, dtype: object\n",
      "0    [qued, ocorr, aliment, pux, cord, sent, contr,...\n",
      "1    [repercuss, nega, cas, fez, bolsonar, ,, candi...\n",
      "2    [brasil, lav, alm, após, decepcion, empat, col...\n",
      "3    [final, encontr, ,, sen, tucan, diss, esp, gov...\n",
      "4    [fiz, compromiss, líd, bas, hoj, líd, opos, ,,...\n",
      "Name: stems, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\didia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('portuguese'))\n",
    "def tokeinizar(train, val, test):\n",
    "\n",
    "    #criar coluna tokens\n",
    "    train['tokens'] = train['sentences'].apply(lambda x: nltk.word_tokenize(str(x)))\n",
    "    val['tokens']   = val['sentences'].apply(lambda x: nltk.word_tokenize(str(x)))\n",
    "    test['tokens'] = test['sentences'].apply(lambda x: nltk.word_tokenize(str(x)))\n",
    "    train_oversampling['tokens'] = train_oversampling['sentences'].apply(lambda x: nltk.word_tokenize(str(x)))\n",
    "\n",
    "    #remover stopwords\n",
    "    train['tokens'] = train['tokens'].apply(lambda toks: [t for t in toks if t.lower() not in stop_words])\n",
    "    val['tokens']   = val['tokens'].apply(lambda toks: [t for t in toks if t.lower() not in stop_words])\n",
    "    test['tokens']  = test['tokens'].apply(lambda toks: [t for t in toks if t.lower() not in stop_words])\n",
    "    train_oversampling['tokens'] = train_oversampling['tokens'].apply(lambda toks: [t for t in toks if t.lower() not in stop_words])\n",
    "\n",
    "    #matizar\n",
    "    #nltk . download ('rslp')\n",
    "    stemmer = nltk . stem . RSLPStemmer ()\n",
    "    train['stems'] = train['tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
    "    val['stems']   = val['tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
    "    test['stems'] = test['tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
    "    train_oversampling['stems'] = train_oversampling['tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
    "\n",
    "    nltk . download ('rslp')\n",
    "    stemmer = nltk . stem . RSLPStemmer ()\n",
    "    print(train['stems'].head())\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "train, val, test = tokeinizar(train, val, test)\n",
    "train_oversampling, val_oversampling, test_oversampling = tokeinizar(train_oversampling, val_oversampling, test_oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41608aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train= train['classe']\n",
    "y_train_oversampling= train_oversampling['classe']\n",
    "\n",
    "y_val=val['classe']\n",
    "y_val_oversampling = val_oversampling['classe']\n",
    "\n",
    "y_test= test['classe']\n",
    "y_test_oversampling = test_oversampling['classe']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50b02c9",
   "metadata": {},
   "source": [
    "# Construção dos modelos de representação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6799b632",
   "metadata": {},
   "source": [
    "## TF-IDF - fred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d13758dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_IDF(train, val, test):\n",
    "    tf_vect = TfidfVectorizer(ngram_range =(1, 2), min_df =3, max_df =0.5, max_features =500)\n",
    "\n",
    "        #juntar os tokens matizados em frases novvamente \n",
    "    text_train = train['stems'].apply(lambda tokens: ' '.join(tokens))\n",
    "    text_val   = val['stems'].apply(lambda tokens: ' '.join(tokens))\n",
    "    text_test = test['stems'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "    X_train = tf_vect.fit_transform(text_train)\n",
    "    X_val   = tf_vect.transform(text_val)\n",
    "    X_test  = tf_vect.transform(text_test)\n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b98a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf_vec, X_val_tfidf_vec, X_test_tfidf_vec = TF_IDF(train,val, test)\n",
    "X_train_oversampling_tfidf_vec, X_val_oversampling_tfidf_vec, X_test_oversampling_tfidf_vec = TF_IDF(train_oversampling, val_oversampling, test_oversampling)\n",
    "\n",
    "X_train_meta1_tfidf, X_val_meta1_tfidf, X_test_meta1_tfidf = TF_IDF(train_meta1,val_meta1, test_meta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eaac49",
   "metadata": {},
   "source": [
    "## CountVectorizer -didi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e94f9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1399bde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def objective_count_vec(trial, train, val):\n",
    "    # Parâmetros a otimizar\n",
    "    max_features = trial.suggest_int(\"max_features\", 500, 5000)\n",
    "    ngram_min = trial.suggest_int(\"ngram_min\", 1, 2)\n",
    "    ngram_max = trial.suggest_int(\"ngram_max\", ngram_min, 3)  # garante ngram_max >= ngram_min\n",
    "\n",
    "    # CountVectorizer com parâmetros sugeridos\n",
    "    c_vect = CountVectorizer(\n",
    "        token_pattern=r\"(?u)\\b\\w+\\b|[^\\w\\s]\",\n",
    "        ngram_range=(ngram_min, ngram_max),\n",
    "        strip_accents='unicode',\n",
    "        max_features=max_features\n",
    "    )\n",
    "\n",
    "    # Transformar textos\n",
    "    text_train = train['stems'].apply(lambda tokens: ' '.join(tokens))\n",
    "    text_val   = val['stems'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "    X_train = c_vect.fit_transform(text_train)\n",
    "    X_val   = c_vect.transform(text_val)\n",
    "\n",
    "    y_train = train['classe'] \n",
    "    y_val   = val['classe']\n",
    "\n",
    "    # Treinar Logistic Regression\n",
    "    model = LogisticRegression(max_iter=500)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_val)\n",
    "\n",
    "    # Métrica: F1 macro\n",
    "    score = f1_score(y_val, preds, average='macro')\n",
    "    return score \n",
    "\n",
    "def count_vec(train, val, test):\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective_count_vec(trial, train,val),\n",
    "                   n_trials=30, n_jobs=-1, show_progress_bar=False)\n",
    "    \n",
    "    best_params=study.best_params\n",
    "    print(\"Melhores parâmetros:\", study.best_params)\n",
    "    print(\"Melhor F1:\", study.best_value)\n",
    "    \n",
    "    c_vect = CountVectorizer(\n",
    "        token_pattern=r\"(?u)\\b\\w+\\b|[^\\w\\s]\",\n",
    "        ngram_range=(best_params['ngram_min'], best_params['ngram_max']),\n",
    "        strip_accents='unicode',\n",
    "        max_features=best_params['max_features']\n",
    "    )\n",
    "\n",
    "    # Transformar textos\n",
    "    text_train = train['stems'].apply(lambda tokens: ' '.join(tokens))\n",
    "    text_val = val['stems'].apply(lambda tokens: ' '.join(tokens))\n",
    "    text_test = test['stems'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "    # Fit no treino, transform nos outros conjuntos\n",
    "    X_train = c_vect.fit_transform(text_train)\n",
    "    X_val = c_vect.transform(text_val)\n",
    "    X_test = c_vect.transform(text_test)\n",
    "\n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666f038f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros: {'max_features': 3234, 'ngram_min': 1, 'ngram_max': 3}\n",
      "Melhor F1: 0.680755783275675\n",
      "Melhores parâmetros: {'max_features': 3738, 'ngram_min': 1, 'ngram_max': 1}\n",
      "Melhor F1: 0.6950529365885298\n"
     ]
    }
   ],
   "source": [
    "X_train_count_vec, X_val_count_vec, X_test_count_vec = count_vec(train, val, test)\n",
    "X_train_oversampling_count_vec, X_val_oversampling_count_vec, X_test_oversampling_count_vec= count_vec(train_oversampling, val_oversampling, test_oversampling)\n",
    "\n",
    "X_train_meta1_count_vec, X_val_meta1_count_vec, X_test_meta1_count_vec = count_vec(train_meta1,val_meta1, test_meta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a974ef",
   "metadata": {},
   "source": [
    "## Glove -fred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec3f116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def objective_tfidf(trial):\n",
    "    # Parâmetros a otimizar\n",
    "    max_features = trial.suggest_int(\"max_features\", 500, 5000)\n",
    "    ngram_min = trial.suggest_int(\"ngram_min\", 1, 2)\n",
    "    ngram_max = trial.suggest_int(\"ngram_max\", ngram_min, 3) \n",
    "    min_df = trial.suggest_int(\"min_df\", 1, 10)\n",
    "    max_df = trial.suggest_float(\"max_df\", 0.3, 0.9)\n",
    "\n",
    "    tfidf_vect = TfidfVectorizer(\n",
    "        ngram_range=(ngram_min, ngram_max),\n",
    "        min_df=min_df,\n",
    "        max_df=max_df,\n",
    "        max_features=max_features,\n",
    "        strip_accents='unicode',\n",
    "        token_pattern=r\"(?u)\\b\\w+\\b|[^\\w\\s]\"\n",
    "    )\n",
    "\n",
    "    text_train = train['stems'].apply(lambda tokens: ' '.join(tokens))\n",
    "    text_val = val['stems'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "    X_train = tfidf_vect.fit_transform(text_train)\n",
    "    X_val = tfidf_vect.transform(text_val)\n",
    "\n",
    "    y_train = train['classe']\n",
    "    y_val = val['classe']\n",
    "\n",
    "    model = LogisticRegression(max_iter=500)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_val)\n",
    "\n",
    "    score = f1_score(y_val, preds, average='macro')\n",
    "    return score\n",
    "\n",
    "def tfidf_optuna(train, val, test):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective_tfidf, n_trials=30)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    print(\"Melhores parâmetros:\", best_params)\n",
    "\n",
    "    tfidf_vect = TfidfVectorizer(\n",
    "        ngram_range=(best_params['ngram_min'], best_params['ngram_max']),\n",
    "        min_df=best_params['min_df'],\n",
    "        max_df=best_params['max_df'],\n",
    "        max_features=best_params['max_features'],\n",
    "        strip_accents='unicode',\n",
    "        token_pattern=r\"(?u)\\b\\w+\\b|[^\\w\\s]\"\n",
    "    )\n",
    "\n",
    "    text_train = train['stems'].apply(lambda tokens: ' '.join(tokens))\n",
    "    text_val = val['stems'].apply(lambda tokens: ' '.join(tokens))\n",
    "    text_test = test['stems'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "    X_train = tfidf_vect.fit_transform(text_train)\n",
    "    X_val = tfidf_vect.transform(text_val)\n",
    "    X_test = tfidf_vect.transform(text_test)\n",
    "\n",
    "    return X_train, X_val, X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdd78bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros: {'max_features': 526, 'ngram_min': 1, 'ngram_max': 1, 'min_df': 5, 'max_df': 0.553539201239897}\n",
      "Melhores parâmetros: {'max_features': 731, 'ngram_min': 1, 'ngram_max': 1, 'min_df': 4, 'max_df': 0.5697303395749046}\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf_vec, X_val_tfidf_vec, X_test_tfidf_vec = tfidf_optuna(train,val, test)\n",
    "X_train_oversampling_tfidf_vec, X_val_oversampling_tfidf_vec, X_test_oversampling_tfidf_vec = tfidf_optuna(train_oversampling, val_oversampling, test_oversampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72beca9e",
   "metadata": {},
   "source": [
    "## WORD2VEC -anaana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188d7c7d",
   "metadata": {},
   "source": [
    "fontes : \n",
    "-   https://sites.google.com/view/nilc-usp/resources-and-tools?authuser=0\n",
    "-   https://huggingface.co/nilc-nlp/word2vec-skip-gram-300d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bade9176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install huggingface_hub\n",
    "#%pip install hf_hub_download\n",
    "#%pip install safetensors\n",
    "from huggingface_hub import hf_hub_download\n",
    "from safetensors.numpy import load_file\n",
    "\n",
    "def carregar_word2vec():\n",
    "    path = hf_hub_download(repo_id=\"nilc-nlp/word2vec-skip-gram-300d\",\n",
    "                           filename=\"embeddings.safetensors\")\n",
    "\n",
    "    data = load_file(path)\n",
    "    vectors = data[\"embeddings\"]\n",
    "\n",
    "    vocab_path = hf_hub_download(repo_id=\"nilc-nlp/word2vec-skip-gram-300d\",\n",
    "                                 filename=\"vocab.txt\")\n",
    "    with open(vocab_path) as f:\n",
    "        vocab = [w.strip() for w in f]\n",
    "\n",
    "    print(vectors.shape)\n",
    "\n",
    "    model = KeyedVectors(vector_size=vectors.shape[1])\n",
    "    model.add_vectors(vocab, vectors)\n",
    "    model.fill_norms()\n",
    "\n",
    "    return model\n",
    "\n",
    "def vetor_frase(model, frase):\n",
    "    vectors = [model[w] for w in frase if w in model]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "def word2vec(train, val, test, coluna, model):\n",
    "    X_train = np.vstack([vetor_frase(model, tokens) for tokens in train[coluna]])\n",
    "    X_val   = np.vstack([vetor_frase(model, tokens) for tokens in val[coluna]])\n",
    "    X_test  = np.vstack([vetor_frase(model, tokens) for tokens in test[coluna]])\n",
    "\n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c9e729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(929606, 300)\n"
     ]
    }
   ],
   "source": [
    "model = carregar_word2vec()\n",
    "X_train_word2vec, X_val_word2vec, X_test_word2vec = word2vec(train, val, test, 'stems', model)\n",
    "X_train_oversampling_word2vec, X_val_oversampling_word2vec, X_test_oversampling_word2vec = word2vec(train_oversampling, val_oversampling, test_oversampling, 'stems', model)\n",
    "\n",
    "X_train_meta1_word2vec, X_val_meta1_word2vec, X_test_meta1_word2vec = word2vec(train_meta1, val_meta1, test_meta1, 'stems', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44e250c",
   "metadata": {},
   "source": [
    "## WORD2VEC treinado com os nossos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b6e3e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import optuna\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#vetor médio da frase\n",
    "def vetor_frase_w2v(tokens, model):\n",
    "    vecs = [model.wv[w] for w in tokens if w in model.wv]\n",
    "    return np.mean(vecs, axis=0) if vecs else np.zeros(model.vector_size)\n",
    "\n",
    "# matriz dos embeddings\n",
    "def prep_dados_w2v(dados, coluna, model):\n",
    "    return np.vstack([vetor_frase_w2v(tokens, model) for tokens in dados[coluna]])\n",
    "\n",
    "def objective_w2v(trial, train, val):\n",
    "    # Hiperparâmetros \n",
    "    vector_size = trial.suggest_int(\"vector_size\", 50, 300)\n",
    "    window = trial.suggest_int(\"window\", 3, 10)\n",
    "    min_count = trial.suggest_int(\"min_count\", 1, 5)\n",
    "    sg = trial.suggest_categorical(\"sg\", [0, 1])  # 0 = CBOW, 1 = Skip-gram\n",
    "    epochs = trial.suggest_int(\"epochs\", 5, 20)\n",
    "    C = trial.suggest_float(\"C\", 1e-3, 10, log=True)\n",
    "    use_scaler = trial.suggest_categorical(\"use_scaler\", [True, False])\n",
    "\n",
    "    # Treina Word2Vec com os parâmetros sugeridos\n",
    "    sentences = train[\"tokens\"]\n",
    "    model = Word2Vec(\n",
    "        sentences=sentences,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        sg=sg,\n",
    "        workers=4,\n",
    "        epochs=epochs\n",
    "    )\n",
    "\n",
    "    # Transformar dados nos embeddings médios\n",
    "    X_train = prep_dados_w2v(train, \"tokens\", model)\n",
    "    X_val = prep_dados_w2v(val, \"tokens\", model)\n",
    "    y_train = train[\"classe\"].values\n",
    "    y_val = val[\"classe\"].values\n",
    "\n",
    "    if use_scaler:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "\n",
    "    # Modelo de classificação (Logistic Regression)\n",
    "    clf = LogisticRegression(C=C, max_iter=500)\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_val)\n",
    "\n",
    "    return f1_score(y_val, preds, average=\"macro\")\n",
    "\n",
    "def word2vec_vec(train, val, test):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective_w2v(trial, train, val),\n",
    "                   n_trials=30, n_jobs=-1, show_progress_bar=False)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    print(\"Melhores parâmetros:\", best_params)\n",
    "    print(\"Melhor F1:\", study.best_value)\n",
    "\n",
    "    best_model = Word2Vec(\n",
    "        sentences=train[\"tokens\"],\n",
    "        vector_size=best_params[\"vector_size\"],\n",
    "        window=best_params[\"window\"],\n",
    "        min_count=best_params[\"min_count\"],\n",
    "        sg=best_params[\"sg\"],\n",
    "        workers=4,\n",
    "        epochs=best_params[\"epochs\"]\n",
    "    )\n",
    "\n",
    "    # Criar embeddings médios para todos os conjuntos\n",
    "    X_train = prep_dados_w2v(train, \"tokens\", best_model)\n",
    "    X_val = prep_dados_w2v(val, \"tokens\", best_model)\n",
    "    X_test = prep_dados_w2v(test, \"tokens\", best_model)\n",
    "\n",
    "    if best_params[\"use_scaler\"]:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, X_val, X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eab317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros: {'vector_size': 290, 'window': 9, 'min_count': 1, 'sg': 1, 'epochs': 19, 'C': 6.121006668710887, 'use_scaler': True}\n",
      "Melhor F1: 0.6036808754743623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros: {'vector_size': 250, 'window': 9, 'min_count': 4, 'sg': 1, 'epochs': 5, 'C': 8.821638367237977, 'use_scaler': False}\n",
      "Melhor F1: 0.6632855794146116\n"
     ]
    }
   ],
   "source": [
    "X_train_w2v_trained, X_val_w2v_trained, X_test_w2v_trained = word2vec_vec(train, val, test)\n",
    "X_train_over_w2v_trained, X_val_over_w2v_trained, X_test_over_w2v_trained= word2vec_vec(train_oversampling, val_oversampling, test_oversampling)\n",
    "\n",
    "X_train_meta1_w2v_trained, X_val_meta1_w2v_trained, X_test_meta1_w2v_trained = word2vec_vec(train_meta1, val_meta1, test_meta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e6afab",
   "metadata": {},
   "source": [
    "## Combinar CountVectorizer com TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7515fd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "X_train_freq = hstack([X_train_count_vec, X_train_tfidf_vec])\n",
    "X_val_freq   = hstack([X_val_count_vec, X_val_tfidf_vec])\n",
    "X_test_freq  = hstack([X_test_count_vec, X_test_tfidf_vec])\n",
    "\n",
    "X_train_freq_over = hstack([X_train_oversampling_count_vec, X_train_oversampling_tfidf_vec])\n",
    "X_val_freq_over   = hstack([X_val_oversampling_count_vec, X_val_oversampling_tfidf_vec])\n",
    "X_test_freq_over  = hstack([X_test_oversampling_count_vec, X_test_oversampling_tfidf_vec])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51db5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#com o pré-processamento da meta 1\n",
    "X_train_freq_meta1 = hstack([X_train_meta1_count_vec, X_train_meta1_tfidf_vec])\n",
    "X_val_freq_meta1   = hstack([X_val_meta1_count_vec, X_val_meta1_tfidf_vec])\n",
    "X_test_freq_meta1  = hstack([X_test_meta1_count_vec, X_test_meta1_tfidf_vec])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955eb2ec",
   "metadata": {},
   "source": [
    "# Aplicar modelos de classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0268c280",
   "metadata": {},
   "source": [
    "## Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacc51ae",
   "metadata": {},
   "source": [
    "-   Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0229cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_naive_MN(trial, X_train, X_val, y_train, y_val):\n",
    "    # Hiperparâmetros a otimizar\n",
    "    alpha = trial.suggest_float(\"alpha\", 1e-3, 2.0, log=True)\n",
    "    \n",
    "    # Modelo\n",
    "    clf = MultinomialNB(alpha=alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_val_pred = clf.predict(X_val)\n",
    "    f1 = f1_score(y_val, y_val_pred, average='weighted')  \n",
    "    \n",
    "    return f1\n",
    "\n",
    "def Naive_Bayes_MN(X_train, X_val, X_test, y_train, y_val, n_trials=30):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective_naive_MN(trial, X_train, X_val, y_train, y_val),\n",
    "                   n_trials=n_trials, n_jobs=-1, show_progress_bar=False)\n",
    "    \n",
    "    print(\"Melhores hiperparâmetros encontrados:\")\n",
    "    print(study.best_params)\n",
    "    print(f\"Melhor F1 (validação): {study.best_value:.4f}\")\n",
    "    \n",
    "    # Treinar o modelo final com o melhor alpha\n",
    "    best_model = MultinomialNB(**study.best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Previsão no conjunto de teste\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    \n",
    "    return y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "222c6a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def objective_naive_GS(trial, X_train, X_val, y_train, y_val):\n",
    "    # Definir o hiperparâmetro a otimizar\n",
    "    var_smoothing = trial.suggest_float(\"var_smoothing\", 1e-12, 1e-3, log=True)\n",
    "    # Criar o modelo\n",
    "    model = GaussianNB(var_smoothing=var_smoothing)\n",
    "    # Treinar\n",
    "    model.fit(X_train, y_train)\n",
    "    # Avaliar no conjunto de validação\n",
    "    y_pred = model.predict(X_val)\n",
    "    f1 = f1_score(y_val, y_pred, average=\"macro\")\n",
    "\n",
    "    return f1\n",
    "\n",
    "def naive_bayes_gs(X_train, X_val, X_test, y_train, y_val, n_trials=30):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective_naive_GS(trial, X_train, X_val, y_train, y_val),\n",
    "                   n_trials=n_trials, n_jobs=-1, show_progress_bar=False)\n",
    "    print(\"Melhores hiperparâmetros encontrados:\")\n",
    "    print(study.best_params)\n",
    "    # Treinar o melhor modelo com os melhores parâmetros\n",
    "    best_model = GaussianNB(**study.best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    # Avaliar no conjunto de teste\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "    return y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df142687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----COUNT VECTORIZER ----\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'alpha': 0.8068820154260331}\n",
      "Melhor F1 (validação): 0.8099\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.72      0.78      0.75       417\n",
      "           0       0.85      0.84      0.85      1273\n",
      "           1       0.32      0.30      0.31       168\n",
      "\n",
      "    accuracy                           0.78      1858\n",
      "   macro avg       0.63      0.64      0.64      1858\n",
      "weighted avg       0.78      0.78      0.78      1858\n",
      "\n",
      "[[ 327   80   10]\n",
      " [ 110 1066   97]\n",
      " [  16  101   51]]\n",
      "\n",
      "com oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'alpha': 1.7988630398124963}\n",
      "Melhor F1 (validação): 0.6957\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.77      0.79      0.78      1273\n",
      "           0       0.55      0.77      0.64      1273\n",
      "           1       0.71      0.41      0.52      1273\n",
      "\n",
      "    accuracy                           0.66      3819\n",
      "   macro avg       0.68      0.66      0.65      3819\n",
      "weighted avg       0.68      0.66      0.65      3819\n",
      "\n",
      "[[1007  201   65]\n",
      " [ 141  981  151]\n",
      " [ 159  594  520]]\n"
     ]
    }
   ],
   "source": [
    "# Count Vectorizer\n",
    "print (\"----COUNT VECTORIZER ----\")\n",
    "count_vec_predicted_NB = Naive_Bayes_MN(X_train_count_vec,X_val_count_vec,X_test_count_vec,y_train,y_val)\n",
    "print(metrics.classification_report(y_test, count_vec_predicted_NB))\n",
    "print(metrics.confusion_matrix(y_test, count_vec_predicted_NB))\n",
    "\n",
    "print(\"\\ncom oversampling\")\n",
    "count_vec_over_predicted_NB = Naive_Bayes_MN(X_train_oversampling_count_vec,X_val_oversampling_count_vec,X_test_oversampling_count_vec,y_train_oversampling,y_val_oversampling)\n",
    "print(metrics.classification_report(y_test_oversampling, count_vec_over_predicted_NB))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, count_vec_over_predicted_NB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10c2c5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----TF-IDF ----\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'alpha': 0.1533667469820008}\n",
      "Melhor F1 (validação): 0.7562\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.79      0.51      0.62       417\n",
      "           0       0.77      0.96      0.86      1273\n",
      "           1       1.00      0.02      0.04       168\n",
      "\n",
      "    accuracy                           0.78      1858\n",
      "   macro avg       0.85      0.50      0.50      1858\n",
      "weighted avg       0.80      0.78      0.73      1858\n",
      "\n",
      "[[ 212  205    0]\n",
      " [  46 1227    0]\n",
      " [  10  155    3]]\n",
      "\n",
      " com oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'alpha': 0.011582982512010613}\n",
      "Melhor F1 (validação): 0.6991\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.76      0.72      0.74      1273\n",
      "           0       0.52      0.71      0.60      1273\n",
      "           1       0.61      0.42      0.49      1273\n",
      "\n",
      "    accuracy                           0.62      3819\n",
      "   macro avg       0.63      0.62      0.61      3819\n",
      "weighted avg       0.63      0.62      0.61      3819\n",
      "\n",
      "[[920 233 120]\n",
      " [144 907 222]\n",
      " [151 591 531]]\n"
     ]
    }
   ],
   "source": [
    "print (\"----TF-IDF ----\")\n",
    "tfidf_vec_predicted_NB= Naive_Bayes_MN(X_train_tfidf_vec, X_val_tfidf_vec, X_test_tfidf_vec, y_train, y_val) \n",
    "print(metrics.classification_report(y_test, tfidf_vec_predicted_NB))\n",
    "print(metrics.confusion_matrix(y_test, tfidf_vec_predicted_NB))\n",
    "\n",
    "print(\"\\n com oversampling\")\n",
    "tfidf_vec_oversampling_predicted_NB= Naive_Bayes_MN(X_train_oversampling_tfidf_vec, X_val_oversampling_tfidf_vec, X_test_oversampling_tfidf_vec, y_train_oversampling, y_val_oversampling) \n",
    "print(metrics.classification_report(y_test_oversampling, tfidf_vec_oversampling_predicted_NB))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, tfidf_vec_oversampling_predicted_NB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d2c9102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count vec + tf idf\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'alpha': 1.1604864062521143}\n",
      "Melhor F1 (validação): 0.8078\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.70      0.78      0.74       417\n",
      "           0       0.85      0.83      0.84      1273\n",
      "           1       0.30      0.28      0.29       168\n",
      "\n",
      "    accuracy                           0.77      1858\n",
      "   macro avg       0.62      0.63      0.62      1858\n",
      "weighted avg       0.77      0.77      0.77      1858\n",
      "\n",
      "[[ 326   80   11]\n",
      " [ 120 1055   98]\n",
      " [  20  101   47]]\n",
      "oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'alpha': 1.9237907920055912}\n",
      "Melhor F1 (validação): 0.6932\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.76      0.80      0.78      1273\n",
      "           0       0.56      0.76      0.64      1273\n",
      "           1       0.69      0.40      0.51      1273\n",
      "\n",
      "    accuracy                           0.65      3819\n",
      "   macro avg       0.67      0.65      0.64      3819\n",
      "weighted avg       0.67      0.65      0.64      3819\n",
      "\n",
      "[[1016  191   66]\n",
      " [ 141  967  165]\n",
      " [ 181  579  513]]\n"
     ]
    }
   ],
   "source": [
    "print(\"count vec + tf idf\")\n",
    "freq_predicted_NB= Naive_Bayes_MN(X_train_freq, X_val_freq, X_test_freq, y_train, y_val, n_trials=30) \n",
    "print(metrics.classification_report(y_test,freq_predicted_NB ))\n",
    "print(metrics.confusion_matrix(y_test, freq_predicted_NB))\n",
    "\n",
    "print(\"oversampling\")\n",
    "freq_predicted_NB_over= Naive_Bayes_MN(X_train_freq_over, X_val_freq_over, X_test_freq_over, y_train_oversampling, y_val_oversampling, n_trials=30) \n",
    "print(metrics.classification_report(y_test_oversampling,freq_predicted_NB_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, freq_predicted_NB_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c766d86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----WORD2VEC ----\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'var_smoothing': 0.000630712599801614}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.44      0.42      0.43       417\n",
      "           0       0.84      0.32      0.47      1273\n",
      "           1       0.11      0.62      0.18       168\n",
      "\n",
      "    accuracy                           0.37      1858\n",
      "   macro avg       0.46      0.46      0.36      1858\n",
      "weighted avg       0.68      0.37      0.43      1858\n",
      "\n",
      "[[175  44 198]\n",
      " [196 409 668]\n",
      " [ 30  33 105]]\n",
      "oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'var_smoothing': 1.1308814318189501e-07}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.57      0.44      0.50      1273\n",
      "           0       0.57      0.27      0.36      1273\n",
      "           1       0.39      0.68      0.50      1273\n",
      "\n",
      "    accuracy                           0.46      3819\n",
      "   macro avg       0.51      0.46      0.45      3819\n",
      "weighted avg       0.51      0.46      0.45      3819\n",
      "\n",
      "[[561  89 623]\n",
      " [197 343 733]\n",
      " [230 175 868]]\n"
     ]
    }
   ],
   "source": [
    "#Word2vec\n",
    "print (\"----WORD2VEC ----\")\n",
    "word2vec_predicted_NB= naive_bayes_gs(X_train_word2vec, X_val_word2vec, X_test_word2vec, y_train, y_val, n_trials=30) \n",
    "print(metrics.classification_report(y_test, word2vec_predicted_NB))\n",
    "print(metrics.confusion_matrix(y_test, word2vec_predicted_NB))\n",
    "\n",
    "print (\"oversampling\")\n",
    "word2vec_over_predicted_NB= naive_bayes_gs(X_train_oversampling_word2vec, X_val_oversampling_word2vec, X_test_oversampling_word2vec, y_train_oversampling, y_val_oversampling, n_trials=30) \n",
    "print(metrics.classification_report(y_test_oversampling, word2vec_over_predicted_NB))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, word2vec_over_predicted_NB))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d45c15f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores hiperparâmetros encontrados:\n",
      "{'var_smoothing': 1.4359994095494946e-11}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.65      0.63      0.64       417\n",
      "           0       0.88      0.45      0.60      1273\n",
      "           1       0.15      0.73      0.25       168\n",
      "\n",
      "    accuracy                           0.52      1858\n",
      "   macro avg       0.56      0.60      0.50      1858\n",
      "weighted avg       0.77      0.52      0.57      1858\n",
      "\n",
      "[[263  45 109]\n",
      " [124 572 577]\n",
      " [ 15  31 122]]\n",
      "oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'var_smoothing': 2.03688686507296e-09}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.58      0.75      0.66      1273\n",
      "           0       0.48      0.66      0.55      1273\n",
      "           1       0.37      0.13      0.19      1273\n",
      "\n",
      "    accuracy                           0.51      3819\n",
      "   macro avg       0.48      0.51      0.47      3819\n",
      "weighted avg       0.48      0.51      0.47      3819\n",
      "\n",
      "[[952 199 122]\n",
      " [277 834 162]\n",
      " [399 707 167]]\n"
     ]
    }
   ],
   "source": [
    "#word2vec treinado com os nossos dados\n",
    "word2vec_trained_predicted_NB= naive_bayes_gs(X_train_w2v_trained, X_val_w2v_trained, X_test_w2v_trained, y_train, y_val, n_trials=30) \n",
    "print(metrics.classification_report(y_test, word2vec_trained_predicted_NB))\n",
    "print(metrics.confusion_matrix(y_test, word2vec_trained_predicted_NB))\n",
    "\n",
    "print(\"oversampling\")\n",
    "word2vec_oversampling_trained_predicted_NB= naive_bayes_gs(X_train_over_w2v_trained, X_val_over_w2v_trained, X_test_over_w2v_trained, y_train_oversampling, y_val_oversampling, n_trials=30) \n",
    "print(metrics.classification_report(y_test_oversampling, word2vec_oversampling_trained_predicted_NB))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, word2vec_oversampling_trained_predicted_NB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d8da10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----GLOVE ----\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train_glove' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Glove\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----GLOVE ----\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m glove_predicted_NB\u001b[38;5;241m=\u001b[39m naive_bayes_gs(X_train_glove, X_val_glove, X_test_glove, y_train, y_val, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m) \n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(metrics\u001b[38;5;241m.\u001b[39mclassification_report(y_test, glove_predicted_NB))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(metrics\u001b[38;5;241m.\u001b[39mconfusion_matrix(y_test, glove_predicted_NB))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_glove' is not defined"
     ]
    }
   ],
   "source": [
    "#Glove\n",
    "print (\"----GLOVE ----\")\n",
    "glove_predicted_NB= naive_bayes_gs(X_train_glove, X_val_glove, X_test_glove, y_train, y_val, n_trials=30) \n",
    "print(metrics.classification_report(y_test, glove_predicted_NB))\n",
    "print(metrics.confusion_matrix(y_test, glove_predicted_NB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db44bdd3",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab39b613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X_train, X_val, y_train, y_val):\n",
    "\n",
    "    #Definir hiperparâmetros a testar \n",
    "    C = trial.suggest_float('C', 0.01, 10.0, log=True)\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly'])\n",
    "    gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "    degree = trial.suggest_int('degree', 2, 5) if kernel == 'poly' else 3\n",
    "\n",
    "    #Criar modelo SVM \n",
    "    model = SVC(C=C, kernel=kernel, gamma=gamma, degree=degree)\n",
    "    model = SVC(C=C, kernel=kernel, gamma=gamma, degree=degree)\n",
    "\n",
    "    #treinar modelo com conjunto de treino\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    #prever no conjunto de validação\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    #calcular f1\n",
    "    f1 = f1_score(y_val, y_pred, average='macro')\n",
    "\n",
    "    return f1\n",
    "\n",
    "def svm(X_train, X_val, X_test, y_train, y_val, n_trials=30):\n",
    "    # Chamar o Optuna para otimizar os parâmetros\n",
    "    objeto_para_otimizar = optuna.create_study(direction='maximize')\n",
    "    objeto_para_otimizar.optimize(lambda trial: objective(trial, X_train, X_val, y_train, y_val),\n",
    "                   n_trials=n_trials, n_jobs=-1, show_progress_bar=False )\n",
    "\n",
    "    print(\"Melhores hiperparâmetros encontrados:\")\n",
    "    print(objeto_para_otimizar.best_params)\n",
    "\n",
    "    # Treinar o melhor modelo\n",
    "    best_model = SVC(**objeto_para_otimizar.best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    #  Avaliar no conjunto de teste\n",
    "    y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "    return y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8e61fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----COUNT VECTORIZER ----\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'C': 1.155407962843028, 'kernel': 'linear', 'gamma': 'scale'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.70      0.72      0.71       417\n",
      "           0       0.83      0.84      0.84      1273\n",
      "           1       0.25      0.21      0.23       168\n",
      "\n",
      "    accuracy                           0.76      1858\n",
      "   macro avg       0.59      0.59      0.59      1858\n",
      "weighted avg       0.75      0.76      0.75      1858\n",
      "\n",
      "[[ 299  100   18]\n",
      " [ 113 1070   90]\n",
      " [  13  119   36]]\n",
      "\n",
      "com oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'C': 0.03408939664161886, 'kernel': 'linear', 'gamma': 'auto'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.84      0.73      0.78      1273\n",
      "           0       0.52      0.83      0.64      1273\n",
      "           1       0.72      0.38      0.50      1273\n",
      "\n",
      "    accuracy                           0.65      3819\n",
      "   macro avg       0.69      0.65      0.64      3819\n",
      "weighted avg       0.69      0.65      0.64      3819\n",
      "\n",
      "[[ 935  271   67]\n",
      " [ 101 1055  117]\n",
      " [  78  711  484]]\n"
     ]
    }
   ],
   "source": [
    "print (\"----COUNT VECTORIZER ----\")\n",
    "count_vec_predicted_SVM= svm(X_train_count_vec, X_val_count_vec, X_test_count_vec, y_train, y_val, n_trials=30) \n",
    "print(metrics.classification_report(\n",
    "y_test, count_vec_predicted_SVM))\n",
    "print(metrics.confusion_matrix(y_test, count_vec_predicted_SVM))\n",
    "\n",
    "print(\"\\ncom oversampling\")\n",
    "count_vec_over_predicted_SVM = svm(X_train_oversampling_count_vec, X_val_oversampling_count_vec, X_test_oversampling_count_vec, y_train_oversampling, y_val_oversampling, n_trials=30)\n",
    "print(metrics.classification_report(y_test_oversampling, count_vec_over_predicted_SVM))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, count_vec_over_predicted_SVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e5930db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- TF-IDF ----\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'C': 8.904928075038061, 'kernel': 'linear', 'gamma': 'auto'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.67      0.63      0.65       417\n",
      "           0       0.80      0.87      0.84      1273\n",
      "           1       0.27      0.12      0.17       168\n",
      "\n",
      "    accuracy                           0.75      1858\n",
      "   macro avg       0.58      0.54      0.55      1858\n",
      "weighted avg       0.72      0.75      0.73      1858\n",
      "\n",
      "[[ 262  150    5]\n",
      " [ 108 1113   52]\n",
      " [  22  125   21]]\n",
      "---- TF-IDF ----\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'C': 0.22823930939231468, 'kernel': 'linear', 'gamma': 'scale'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.81      0.74      0.78      1273\n",
      "           0       0.54      0.72      0.62      1273\n",
      "           1       0.63      0.48      0.54      1273\n",
      "\n",
      "    accuracy                           0.65      3819\n",
      "   macro avg       0.66      0.65      0.65      3819\n",
      "weighted avg       0.66      0.65      0.65      3819\n",
      "\n",
      "[[947 215 111]\n",
      " [113 914 246]\n",
      " [102 566 605]]\n"
     ]
    }
   ],
   "source": [
    "print (\"---- TF-IDF ----\")\n",
    "tfidf_vec_predicted_SVM= svm(X_train_tfidf_vec, X_val_tfidf_vec, X_test_tfidf_vec, y_train, y_val, n_trials=30) \n",
    "print(metrics.classification_report(y_test, tfidf_vec_predicted_SVM))\n",
    "print(metrics.confusion_matrix(y_test, tfidf_vec_predicted_SVM))\n",
    "\n",
    "print (\"---- TF-IDF ----\")\n",
    "tfidf_vec_predicted_SVM_oversampling= svm(X_train_oversampling_tfidf_vec, X_val_oversampling_tfidf_vec, X_test_oversampling_tfidf_vec, y_train_oversampling, y_val_oversampling, n_trials=30) \n",
    "print(metrics.classification_report(y_test_oversampling, tfidf_vec_predicted_SVM_oversampling))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, tfidf_vec_predicted_SVM_oversampling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d469dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count vec + tf idf\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'C': 0.2667411350495317, 'kernel': 'linear', 'gamma': 'auto'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.81      0.67      0.73       417\n",
      "           0       0.82      0.91      0.86      1273\n",
      "           1       0.25      0.14      0.18       168\n",
      "\n",
      "    accuracy                           0.79      1858\n",
      "   macro avg       0.62      0.57      0.59      1858\n",
      "weighted avg       0.76      0.79      0.77      1858\n",
      "\n",
      "[[ 278  129   10]\n",
      " [  55 1158   60]\n",
      " [  12  133   23]]\n",
      "oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'C': 0.016681982809073317, 'kernel': 'linear', 'gamma': 'scale'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.82      0.69      0.75      1273\n",
      "           0       0.52      0.81      0.63      1273\n",
      "           1       0.69      0.42      0.52      1273\n",
      "\n",
      "    accuracy                           0.64      3819\n",
      "   macro avg       0.68      0.64      0.63      3819\n",
      "weighted avg       0.68      0.64      0.63      3819\n",
      "\n",
      "[[ 875  303   95]\n",
      " [  94 1034  145]\n",
      " [  92  650  531]]\n"
     ]
    }
   ],
   "source": [
    "print(\"count vec + tf idf\")\n",
    "freq_predicted_svm= svm(X_train_freq, X_val_freq, X_test_freq, y_train, y_val, n_trials=30) \n",
    "print(metrics.classification_report(y_test,freq_predicted_svm))\n",
    "print(metrics.confusion_matrix(y_test, freq_predicted_svm))\n",
    "\n",
    "print(\"oversampling\")\n",
    "freq_predicted_svm_over= svm(X_train_freq_over, X_val_freq_over, X_test_freq_over, y_train_oversampling, y_val_oversampling, n_trials=30) \n",
    "print(metrics.classification_report(y_test_oversampling,freq_predicted_svm_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, freq_predicted_svm_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9f769d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----WORD2VEC ----\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'C': 9.836846123151167, 'kernel': 'rbf', 'gamma': 'scale'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.58      0.45      0.51       417\n",
      "           0       0.77      0.89      0.83      1273\n",
      "           1       0.33      0.10      0.15       168\n",
      "\n",
      "    accuracy                           0.72      1858\n",
      "   macro avg       0.56      0.48      0.49      1858\n",
      "weighted avg       0.69      0.72      0.69      1858\n",
      "\n",
      "[[ 188  222    7]\n",
      " [ 109 1139   25]\n",
      " [  25  127   16]]\n",
      "C/ oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'C': 0.6138352787200456, 'kernel': 'poly', 'gamma': 'scale', 'degree': 2}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.62      0.56      0.59      1273\n",
      "           0       0.47      0.72      0.57      1273\n",
      "           1       0.55      0.31      0.40      1273\n",
      "\n",
      "    accuracy                           0.53      3819\n",
      "   macro avg       0.55      0.53      0.52      3819\n",
      "weighted avg       0.55      0.53      0.52      3819\n",
      "\n",
      "[[715 405 153]\n",
      " [176 920 177]\n",
      " [254 620 399]]\n"
     ]
    }
   ],
   "source": [
    "print (\"----WORD2VEC ----\")\n",
    "word2vec_predicted_SVM= svm(X_train_word2vec, X_val_word2vec, X_test_word2vec, y_train, y_val, n_trials=30) \n",
    "print(metrics.classification_report(y_test, word2vec_predicted_SVM))\n",
    "print(metrics.confusion_matrix(y_test, word2vec_predicted_SVM))\n",
    "\n",
    "print (\"C/ oversampling\")\n",
    "word2vec_predicted_SVM_oversampling= svm(X_train_oversampling_word2vec, X_val_oversampling_word2vec, X_test_oversampling_word2vec, y_train_oversampling, y_val_oversampling, n_trials=30) \n",
    "print(metrics.classification_report(y_test_oversampling, word2vec_predicted_SVM_oversampling))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, word2vec_predicted_SVM_oversampling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13e589a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores hiperparâmetros encontrados:\n",
      "{'C': 8.9025480634146, 'kernel': 'rbf', 'gamma': 'auto'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.72      0.72      0.72       417\n",
      "           0       0.83      0.90      0.86      1273\n",
      "           1       0.25      0.07      0.10       168\n",
      "\n",
      "    accuracy                           0.79      1858\n",
      "   macro avg       0.60      0.56      0.56      1858\n",
      "weighted avg       0.75      0.79      0.76      1858\n",
      "\n",
      "[[ 302  110    5]\n",
      " [  94 1151   28]\n",
      " [  23  134   11]]\n",
      "oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'C': 0.17975185927753537, 'kernel': 'linear', 'gamma': 'auto'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.73      0.73      0.73      1273\n",
      "           0       0.50      0.72      0.59      1273\n",
      "           1       0.61      0.35      0.44      1273\n",
      "\n",
      "    accuracy                           0.60      3819\n",
      "   macro avg       0.61      0.60      0.59      3819\n",
      "weighted avg       0.61      0.60      0.59      3819\n",
      "\n",
      "[[924 259  90]\n",
      " [159 913 201]\n",
      " [187 640 446]]\n"
     ]
    }
   ],
   "source": [
    "#word2vec treinado com os nossos dados\n",
    "word2vec_trained_predicted_svm= svm(X_train_w2v_trained, X_val_w2v_trained, X_test_w2v_trained, y_train, y_val, n_trials=30) \n",
    "print(metrics.classification_report(y_test, word2vec_trained_predicted_svm))\n",
    "print(metrics.confusion_matrix(y_test, word2vec_trained_predicted_svm))\n",
    "\n",
    "print(\"oversampling\")\n",
    "word2vec_trained_predicted_svm_over= svm(X_train_over_w2v_trained, X_val_over_w2v_trained, X_test_over_w2v_trained, y_train_oversampling, y_val_oversampling, n_trials=30) \n",
    "print(metrics.classification_report(y_test_oversampling, word2vec_trained_predicted_svm_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, word2vec_trained_predicted_svm_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f1488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"----GLOVE ----\")\n",
    "glove_predicted_SVM= svm(X_train_glove, X_val_glove, X_test_glove, y_train, y_val, n_trials=30) \n",
    "print(metrics.classification_report(y_test, glove_predicted_SVM))\n",
    "print(metrics.confusion_matrix(y_test, glove_predicted_SVM))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6791a4e1",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21c7661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def KNN(X_train, y_train, X_val, y_val, X_test):\n",
    "    k_values = range(1, 200,10)  # Determinar  intervalo de valores a testar para k\n",
    "    best_k = 1\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    k_list = []\n",
    "    accuracy_list = [] \n",
    "\n",
    "    for k in k_values:\n",
    "\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)  \n",
    "        knn.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_KNN = knn.predict(X_val) \n",
    "        accuracy = accuracy_score(y_val, y_pred_KNN)  \n",
    "\n",
    "        k_list.append(k)\n",
    "        accuracy_list.append(accuracy)  \n",
    "\n",
    "        if accuracy > best_accuracy: \n",
    "            best_accuracy = accuracy\n",
    "            best_k = k\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=best_k)  \n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_KNN= knn.predict(X_test) \n",
    "    print(f\"Best k: {best_k}\")\n",
    "\n",
    "    return y_pred_KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6fead510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----COUNT VECTORIZER ----\n",
      "Best k: 51\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.74      0.60      0.66       417\n",
      "           0       0.79      0.94      0.86      1273\n",
      "           1       0.00      0.00      0.00       168\n",
      "\n",
      "    accuracy                           0.78      1858\n",
      "   macro avg       0.51      0.51      0.51      1858\n",
      "weighted avg       0.71      0.78      0.74      1858\n",
      "\n",
      "[[ 251  166    0]\n",
      " [  78 1195    0]\n",
      " [  10  158    0]]\n",
      "c/ oversampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k: 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.65      0.72      0.68      1273\n",
      "           0       0.45      0.78      0.57      1273\n",
      "           1       0.65      0.11      0.18      1273\n",
      "\n",
      "    accuracy                           0.53      3819\n",
      "   macro avg       0.58      0.53      0.48      3819\n",
      "weighted avg       0.58      0.53      0.48      3819\n",
      "\n",
      "[[915 325  33]\n",
      " [240 991  42]\n",
      " [244 892 137]]\n"
     ]
    }
   ],
   "source": [
    "# Count Vectorizer\n",
    "print (\"----COUNT VECTORIZER ----\")\n",
    "count_vec_predicted_KNN= KNN(X_train_count_vec, y_train, X_val_count_vec, y_val, X_test_count_vec) \n",
    "print(metrics.classification_report(y_test, count_vec_predicted_KNN))\n",
    "print(metrics.confusion_matrix(y_test, count_vec_predicted_KNN))\n",
    "\n",
    "print (\"c/ oversampling\")\n",
    "count_vec_predicted_KNN_oversampling= KNN(X_train_oversampling_count_vec, y_train_oversampling, X_val_oversampling_count_vec, y_val_oversampling, X_test_oversampling_count_vec) \n",
    "print(metrics.classification_report(y_test_oversampling, count_vec_predicted_KNN_oversampling))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, count_vec_predicted_KNN_oversampling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cef2dc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----TF-IDF ----\n",
      "Best k: 181\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.74      0.60      0.67       417\n",
      "           0       0.79      0.94      0.86      1273\n",
      "           1       0.00      0.00      0.00       168\n",
      "\n",
      "    accuracy                           0.78      1858\n",
      "   macro avg       0.51      0.51      0.51      1858\n",
      "weighted avg       0.71      0.78      0.74      1858\n",
      "\n",
      "[[ 251  166    0]\n",
      " [  78 1195    0]\n",
      " [   8  160    0]]\n",
      "c/oversampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k: 171\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.66      0.88      0.75      1273\n",
      "           0       0.58      0.67      0.62      1273\n",
      "           1       0.67      0.34      0.45      1273\n",
      "\n",
      "    accuracy                           0.63      3819\n",
      "   macro avg       0.64      0.63      0.61      3819\n",
      "weighted avg       0.64      0.63      0.61      3819\n",
      "\n",
      "[[1116  100   57]\n",
      " [ 259  859  155]\n",
      " [ 318  525  430]]\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF\n",
    "print (\"----TF-IDF ----\")\n",
    "tfidf_vec_predicted_KNN= KNN(X_train_tfidf_vec, y_train, X_val_tfidf_vec, y_val, X_test_tfidf_vec) \n",
    "print(metrics.classification_report(y_test, tfidf_vec_predicted_KNN))\n",
    "print(metrics.confusion_matrix(y_test, tfidf_vec_predicted_KNN))\n",
    "\n",
    "print (\"c/oversampling\")\n",
    "tfidf_vec_predicted_KNN_oversampling= KNN(X_train_oversampling_tfidf_vec, y_train_oversampling, X_val_oversampling_tfidf_vec, y_val_oversampling, X_test_oversampling_tfidf_vec) \n",
    "print(metrics.classification_report(y_test_oversampling, tfidf_vec_predicted_KNN_oversampling))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, tfidf_vec_predicted_KNN_oversampling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7123e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count vec + tf idf\n",
      "Best k: 41\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.74      0.60      0.66       417\n",
      "           0       0.79      0.94      0.86      1273\n",
      "           1       0.00      0.00      0.00       168\n",
      "\n",
      "    accuracy                           0.78      1858\n",
      "   macro avg       0.51      0.51      0.51      1858\n",
      "weighted avg       0.71      0.78      0.74      1858\n",
      "\n",
      "[[ 250  167    0]\n",
      " [  77 1196    0]\n",
      " [   9  159    0]]\n",
      "oversampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k: 11\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.58      0.80      0.67      1273\n",
      "           0       0.46      0.64      0.54      1273\n",
      "           1       0.48      0.11      0.18      1273\n",
      "\n",
      "    accuracy                           0.52      3819\n",
      "   macro avg       0.51      0.52      0.46      3819\n",
      "weighted avg       0.51      0.52      0.46      3819\n",
      "\n",
      "[[1024  212   37]\n",
      " [ 352  811  110]\n",
      " [ 401  734  138]]\n"
     ]
    }
   ],
   "source": [
    "print(\"count vec + tf idf\")\n",
    "freq_predicted_KNN= KNN(X_train_freq, y_train,X_val_freq,y_val, X_test_freq) \n",
    "print(metrics.classification_report(y_test,freq_predicted_KNN ))\n",
    "print(metrics.confusion_matrix(y_test, freq_predicted_KNN))\n",
    "\n",
    "print(\"oversampling\")\n",
    "freq_predicted_KNN_over= KNN(X_train_freq_over, y_train_oversampling, X_val_freq_over, y_val_oversampling, X_test_freq_over) \n",
    "print(metrics.classification_report(y_test_oversampling,freq_predicted_KNN_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, freq_predicted_KNN_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4c3c7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----WORD2VEC ----\n",
      "Best k: 11\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.62      0.27      0.37       417\n",
      "           0       0.73      0.95      0.82      1273\n",
      "           1       0.20      0.01      0.02       168\n",
      "\n",
      "    accuracy                           0.71      1858\n",
      "   macro avg       0.51      0.41      0.41      1858\n",
      "weighted avg       0.65      0.71      0.65      1858\n",
      "\n",
      "[[ 112  304    1]\n",
      " [  56 1210    7]\n",
      " [  13  153    2]]\n",
      "c/oversampling\n",
      "Best k: 31\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.54      0.47      0.50      1273\n",
      "           0       0.48      0.39      0.43      1273\n",
      "           1       0.41      0.53      0.46      1273\n",
      "\n",
      "    accuracy                           0.46      3819\n",
      "   macro avg       0.47      0.46      0.46      3819\n",
      "weighted avg       0.47      0.46      0.46      3819\n",
      "\n",
      "[[597 202 474]\n",
      " [252 496 525]\n",
      " [258 335 680]]\n"
     ]
    }
   ],
   "source": [
    "#Word2vec\n",
    "print (\"----WORD2VEC ----\")\n",
    "word2vec_predicted_KNN= KNN(X_train_word2vec, y_train, X_val_word2vec, y_val, X_test_word2vec) \n",
    "print(metrics.classification_report(y_test, word2vec_predicted_KNN))\n",
    "print(metrics.confusion_matrix(y_test, word2vec_predicted_KNN))\n",
    "\n",
    "print (\"c/oversampling\")\n",
    "word2vec_predicted_KNN_oversampling= KNN(X_train_oversampling_word2vec, y_train_oversampling, X_val_oversampling_word2vec, y_val_oversampling, X_test_oversampling_word2vec) \n",
    "print(metrics.classification_report(y_test_oversampling, word2vec_predicted_KNN_oversampling))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, word2vec_predicted_KNN_oversampling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9214a6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k: 21\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.74      0.65      0.69       417\n",
      "           0       0.81      0.93      0.87      1273\n",
      "           1       0.36      0.02      0.04       168\n",
      "\n",
      "    accuracy                           0.79      1858\n",
      "   macro avg       0.64      0.54      0.53      1858\n",
      "weighted avg       0.75      0.79      0.75      1858\n",
      "\n",
      "[[ 273  144    0]\n",
      " [  77 1189    7]\n",
      " [  21  143    4]]\n",
      "oversampling\n",
      "Best k: 51\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.69      0.67      0.68      1273\n",
      "           0       0.46      0.83      0.59      1273\n",
      "           1       0.55      0.12      0.20      1273\n",
      "\n",
      "    accuracy                           0.54      3819\n",
      "   macro avg       0.57      0.54      0.49      3819\n",
      "weighted avg       0.57      0.54      0.49      3819\n",
      "\n",
      "[[ 848  369   56]\n",
      " [ 144 1059   70]\n",
      " [ 236  883  154]]\n"
     ]
    }
   ],
   "source": [
    "#word2vec treinado com os nossos dados\n",
    "word2vec_trained_predicted_KNN= KNN(X_train_w2v_trained, y_train, X_val_w2v_trained, y_val, X_test_w2v_trained) \n",
    "print(metrics.classification_report(y_test, word2vec_trained_predicted_KNN))\n",
    "print(metrics.confusion_matrix(y_test, word2vec_trained_predicted_KNN))\n",
    "\n",
    "print(\"oversampling\")\n",
    "word2vec_trained_predicted_KNN_over= KNN(X_train_over_w2v_trained, y_train_oversampling, X_val_over_w2v_trained, y_val_oversampling,X_test_over_w2v_trained) \n",
    "print(metrics.classification_report(y_test_oversampling, word2vec_trained_predicted_KNN_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, word2vec_trained_predicted_KNN_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e26ff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Glove\n",
    "print (\"----GLOVE ----\")\n",
    "glove_predicted_KNN= KNN(X_train_glove, y_train, X_val_glove, y_val, X_test_glove) \n",
    "print(metrics.classification_report(y_test, glove_predicted_KNN))\n",
    "print(metrics.confusion_matrix(y_test, glove_predicted_KNN))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49589df2",
   "metadata": {},
   "source": [
    "## Decison Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5d53d6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def objective_decision_tree(trial, X_train, X_val, y_train, y_val):\n",
    "    # Hiperparâmetros a otimizar\n",
    "    criterion = trial.suggest_categorical(\"criterion\", [\"entropy\", \"gini\"])\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 50)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "    ccp_alpha = trial.suggest_float(\"ccp_alpha\", 0.0, 0.01)\n",
    "    \n",
    "    # Criar o modelo com os parâmetros sugeridos\n",
    "    model = DecisionTreeClassifier(\n",
    "        criterion=criterion,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        ccp_alpha=ccp_alpha,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    f1 = f1_score(y_val, y_pred, average=\"macro\")\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "def decision_tree(X_train, y_train, X_val, y_val, X_test,n_trials=30):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective_decision_tree(trial, X_train, X_val, y_train, y_val),\n",
    "                   n_trials=n_trials, n_jobs=-1, show_progress_bar=False)\n",
    "\n",
    "    print(\"Melhores hiperparâmetros encontrados:\")\n",
    "    print(study.best_params)\n",
    "    print(\"Melhor F1 obtido:\", study.best_value)\n",
    "\n",
    "    # Treinar o modelo final com os melhores parâmetros\n",
    "    best_model = DecisionTreeClassifier(**study.best_params, random_state=42)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Prever no conjunto de teste\n",
    "    y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "    return y_pred_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7b899214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----COUNT VECTORIZER ----\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'criterion': 'entropy', 'max_depth': 25, 'min_samples_split': 2, 'min_samples_leaf': 3, 'ccp_alpha': 0.0014559206837436035}\n",
      "Melhor F1 obtido: 0.5507680953501684\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.77      0.62      0.69       417\n",
      "           0       0.79      0.93      0.86      1273\n",
      "           1       0.17      0.04      0.06       168\n",
      "\n",
      "    accuracy                           0.78      1858\n",
      "   macro avg       0.58      0.53      0.53      1858\n",
      "weighted avg       0.73      0.78      0.75      1858\n",
      "\n",
      "[[ 260  154    3]\n",
      " [  66 1180   27]\n",
      " [  10  152    6]]\n",
      "c/oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'criterion': 'entropy', 'max_depth': 19, 'min_samples_split': 9, 'min_samples_leaf': 2, 'ccp_alpha': 0.00474403157282603}\n",
      "Melhor F1 obtido: 0.610198235699106\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.82      0.68      0.74      1273\n",
      "           0       0.49      0.51      0.50      1273\n",
      "           1       0.52      0.60      0.56      1273\n",
      "\n",
      "    accuracy                           0.59      3819\n",
      "   macro avg       0.61      0.59      0.60      3819\n",
      "weighted avg       0.61      0.59      0.60      3819\n",
      "\n",
      "[[865 233 175]\n",
      " [103 644 526]\n",
      " [ 82 428 763]]\n"
     ]
    }
   ],
   "source": [
    "# Count Vectorizer\n",
    "print (\"----COUNT VECTORIZER ----\")\n",
    "count_vec_predicted_DT= decision_tree(X_train_count_vec, y_train, X_val_count_vec, y_val, X_test_count_vec) \n",
    "print(metrics.classification_report(y_test, count_vec_predicted_DT))\n",
    "print(metrics.confusion_matrix(y_test, count_vec_predicted_DT))\n",
    "\n",
    "print (\"c/oversampling\")\n",
    "count_vec_predicted_DT_oversampling= decision_tree(X_train_oversampling_count_vec, y_train_oversampling, X_val_oversampling_count_vec, y_val_oversampling, X_test_oversampling_count_vec) \n",
    "print(metrics.classification_report(y_test_oversampling, count_vec_predicted_DT_oversampling))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, count_vec_predicted_DT_oversampling))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8b119b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----TF-IDF ----\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'criterion': 'gini', 'max_depth': 33, 'min_samples_split': 14, 'min_samples_leaf': 7, 'ccp_alpha': 2.3592523720135416e-05}\n",
      "Melhor F1 obtido: 0.5458773225026033\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.72      0.60      0.66       417\n",
      "           0       0.78      0.90      0.84      1273\n",
      "           1       0.16      0.04      0.07       168\n",
      "\n",
      "    accuracy                           0.76      1858\n",
      "   macro avg       0.55      0.52      0.52      1858\n",
      "weighted avg       0.71      0.76      0.73      1858\n",
      "\n",
      "[[ 250  163    4]\n",
      " [  88 1151   34]\n",
      " [   8  153    7]]\n",
      "c/oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'criterion': 'entropy', 'max_depth': 37, 'min_samples_split': 13, 'min_samples_leaf': 7, 'ccp_alpha': 0.006429921761582082}\n",
      "Melhor F1 obtido: 0.5984361369248031\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.81      0.68      0.74      1273\n",
      "           0       0.65      0.29      0.40      1273\n",
      "           1       0.47      0.80      0.59      1273\n",
      "\n",
      "    accuracy                           0.59      3819\n",
      "   macro avg       0.64      0.59      0.58      3819\n",
      "weighted avg       0.64      0.59      0.58      3819\n",
      "\n",
      "[[ 866   32  375]\n",
      " [ 115  371  787]\n",
      " [  92  167 1014]]\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF\n",
    "print (\"----TF-IDF ----\")\n",
    "tfidf_vec_predicted_DT= decision_tree(X_train_tfidf_vec, y_train, X_val_tfidf_vec, y_val, X_test_tfidf_vec) \n",
    "print(metrics.classification_report(y_test, tfidf_vec_predicted_DT))\n",
    "print(metrics.confusion_matrix(y_test, tfidf_vec_predicted_DT))\n",
    "\n",
    "print (\"c/oversampling\")\n",
    "tfidf_vec_predicted_DT_oversampling= decision_tree(X_train_oversampling_tfidf_vec, y_train_oversampling, X_val_oversampling_tfidf_vec, y_val_oversampling, X_test_oversampling_tfidf_vec) \n",
    "print(metrics.classification_report(y_test_oversampling, tfidf_vec_predicted_DT_oversampling))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, tfidf_vec_predicted_DT_oversampling))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2026acb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count vec + tf idf\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'criterion': 'gini', 'max_depth': 13, 'min_samples_split': 16, 'min_samples_leaf': 6, 'ccp_alpha': 4.2733906337860976e-05}\n",
      "Melhor F1 obtido: 0.5536996336996337\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.78      0.63      0.69       417\n",
      "           0       0.80      0.94      0.86      1273\n",
      "           1       0.32      0.04      0.06       168\n",
      "\n",
      "    accuracy                           0.79      1858\n",
      "   macro avg       0.63      0.53      0.54      1858\n",
      "weighted avg       0.75      0.79      0.75      1858\n",
      "\n",
      "[[ 261  151    5]\n",
      " [  67 1198    8]\n",
      " [   7  155    6]]\n",
      "oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'criterion': 'entropy', 'max_depth': 28, 'min_samples_split': 14, 'min_samples_leaf': 1, 'ccp_alpha': 0.0031623735520961652}\n",
      "Melhor F1 obtido: 0.5946687279923211\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.79      0.69      0.73      1273\n",
      "           0       0.51      0.55      0.53      1273\n",
      "           1       0.53      0.56      0.55      1273\n",
      "\n",
      "    accuracy                           0.60      3819\n",
      "   macro avg       0.61      0.60      0.60      3819\n",
      "weighted avg       0.61      0.60      0.60      3819\n",
      "\n",
      "[[873 217 183]\n",
      " [123 700 450]\n",
      " [110 449 714]]\n"
     ]
    }
   ],
   "source": [
    "print(\"count vec + tf idf\")\n",
    "freq_predicted_DT= decision_tree(X_train_freq, y_train,X_val_freq,y_val, X_test_freq, n_trials=30) \n",
    "print(metrics.classification_report(y_test,freq_predicted_DT ))\n",
    "print(metrics.confusion_matrix(y_test, freq_predicted_DT))\n",
    "\n",
    "print(\"oversampling\")\n",
    "freq_predicted_DT_over= decision_tree(X_train_freq_over, y_train_oversampling, X_val_freq_over, y_val_oversampling, X_test_freq_over, n_trials=30) \n",
    "print(metrics.classification_report(y_test_oversampling,freq_predicted_DT_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, freq_predicted_DT_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d288c09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----WORD2VEC ----\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'criterion': 'entropy', 'max_depth': 47, 'min_samples_split': 11, 'min_samples_leaf': 1, 'ccp_alpha': 0.0036997943352353414}\n",
      "Melhor F1 obtido: 0.44793490802052477\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.39      0.31      0.34       417\n",
      "           0       0.72      0.85      0.78      1273\n",
      "           1       0.18      0.02      0.04       168\n",
      "\n",
      "    accuracy                           0.66      1858\n",
      "   macro avg       0.43      0.40      0.39      1858\n",
      "weighted avg       0.60      0.66      0.62      1858\n",
      "\n",
      "[[ 129  285    3]\n",
      " [ 173 1085   15]\n",
      " [  29  135    4]]\n",
      "c/oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'criterion': 'entropy', 'max_depth': 12, 'min_samples_split': 11, 'min_samples_leaf': 5, 'ccp_alpha': 0.003806690003661194}\n",
      "Melhor F1 obtido: 0.4585068642215681\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.43      0.49      0.46      1273\n",
      "           0       0.39      0.48      0.43      1273\n",
      "           1       0.41      0.25      0.31      1273\n",
      "\n",
      "    accuracy                           0.41      3819\n",
      "   macro avg       0.41      0.41      0.40      3819\n",
      "weighted avg       0.41      0.41      0.40      3819\n",
      "\n",
      "[[627 445 201]\n",
      " [402 613 258]\n",
      " [444 511 318]]\n"
     ]
    }
   ],
   "source": [
    "#Word2vec\n",
    "print (\"----WORD2VEC ----\")\n",
    "word2vec_predicted_DT= decision_tree(X_train_word2vec, y_train, X_val_word2vec, y_val, X_test_word2vec) \n",
    "print(metrics.classification_report(y_test, word2vec_predicted_DT))\n",
    "print(metrics.confusion_matrix(y_test, word2vec_predicted_DT))\n",
    "\n",
    "print (\"c/oversampling\")\n",
    "word2vec_predicted_DT_over= decision_tree(X_train_oversampling_word2vec, y_train_oversampling, X_val_oversampling_word2vec, y_val_oversampling, X_test_oversampling_word2vec) \n",
    "print(metrics.classification_report(y_test_oversampling, word2vec_predicted_DT_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, word2vec_predicted_DT_over))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5aed8bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores hiperparâmetros encontrados:\n",
      "{'criterion': 'gini', 'max_depth': 34, 'min_samples_split': 14, 'min_samples_leaf': 3, 'ccp_alpha': 0.0001381633030137809}\n",
      "Melhor F1 obtido: 0.5721000851597866\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.61      0.62      0.62       417\n",
      "           0       0.81      0.81      0.81      1273\n",
      "           1       0.19      0.18      0.18       168\n",
      "\n",
      "    accuracy                           0.71      1858\n",
      "   macro avg       0.54      0.54      0.54      1858\n",
      "weighted avg       0.71      0.71      0.71      1858\n",
      "\n",
      "[[ 260  137   20]\n",
      " [ 136 1029  108]\n",
      " [  31  107   30]]\n",
      "oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'criterion': 'gini', 'max_depth': 23, 'min_samples_split': 16, 'min_samples_leaf': 10, 'ccp_alpha': 0.0027320940944740267}\n",
      "Melhor F1 obtido: 0.5971706309811452\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.71      0.74      0.73      1273\n",
      "           0       0.50      0.73      0.60      1273\n",
      "           1       0.65      0.33      0.43      1273\n",
      "\n",
      "    accuracy                           0.60      3819\n",
      "   macro avg       0.62      0.60      0.58      3819\n",
      "weighted avg       0.62      0.60      0.58      3819\n",
      "\n",
      "[[942 253  78]\n",
      " [192 931 150]\n",
      " [187 671 415]]\n"
     ]
    }
   ],
   "source": [
    "#word2vec treinado com os nossos dados\n",
    "word2vec_trained_predicted_DT= decision_tree(X_train_w2v_trained, y_train, X_val_w2v_trained, y_val, X_test_w2v_trained, n_trials=30) \n",
    "print(metrics.classification_report(y_test, word2vec_trained_predicted_DT))\n",
    "print(metrics.confusion_matrix(y_test, word2vec_trained_predicted_DT))\n",
    "\n",
    "print(\"oversampling\")\n",
    "word2vec_trained_predicted_DT_over= decision_tree(X_train_over_w2v_trained, y_train_oversampling, X_val_over_w2v_trained, y_val_oversampling,X_test_over_w2v_trained, n_trials=30) \n",
    "print(metrics.classification_report(y_test_oversampling, word2vec_trained_predicted_DT_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, word2vec_trained_predicted_DT_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77bced6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Glove\n",
    "print (\"---- GLOVE ----\")\n",
    "glove_predicted_DT= decision_tree(X_train_glove, y_train, X_val_glove, y_val, X_test_glove) \n",
    "print(metrics.classification_report(y_test, glove_predicted_DT))\n",
    "print(metrics.confusion_matrix(y_test, glove_predicted_DT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033b484c",
   "metadata": {},
   "source": [
    "## Random Florest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ba3a7257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def objective_random_forest(trial, X_train, X_val, y_train, y_val):\n",
    "    # Hiperparâmetros a otimizar\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 300, step=50)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 5, 50)\n",
    "    max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None])\n",
    "    criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"])\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "\n",
    "    # Criar o modelo\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        max_features=max_features,\n",
    "        criterion=criterion,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    f1 = f1_score(y_val, y_pred, average=\"macro\")\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "def random_florest(X_train, y_train, X_val, y_val, X_test, n_trials=50):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective_random_forest(trial, X_train, X_val, y_train, y_val),\n",
    "                   n_trials=n_trials, n_jobs=-1, show_progress_bar=False)\n",
    "\n",
    "    print(\"Melhores hiperparâmetros Random Forest:\")\n",
    "    print(study.best_params)\n",
    "    print(\"Melhor F1 obtido:\", study.best_value)\n",
    "\n",
    "    best_model = RandomForestClassifier(**study.best_params, random_state=42, n_jobs=-1)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "    return y_pred_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a1030113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----COUNT VECTORIZER ----\n",
      "Melhores hiperparâmetros Random Forest:\n",
      "{'n_estimators': 200, 'max_depth': 36, 'max_features': None, 'criterion': 'gini', 'min_samples_split': 4, 'min_samples_leaf': 2}\n",
      "Melhor F1 obtido: 0.5710808944215292\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.80      0.66      0.72       417\n",
      "           0       0.80      0.95      0.87      1273\n",
      "           1       0.55      0.04      0.07       168\n",
      "\n",
      "    accuracy                           0.80      1858\n",
      "   macro avg       0.72      0.55      0.55      1858\n",
      "weighted avg       0.78      0.80      0.76      1858\n",
      "\n",
      "[[ 274  142    1]\n",
      " [  61 1208    4]\n",
      " [   7  155    6]]\n",
      "c/oversampling\n",
      "Melhores hiperparâmetros Random Forest:\n",
      "{'n_estimators': 200, 'max_depth': 43, 'max_features': 'log2', 'criterion': 'gini', 'min_samples_split': 13, 'min_samples_leaf': 5}\n",
      "Melhor F1 obtido: 0.6905909568779465\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.76      0.79      0.78      1273\n",
      "           0       0.55      0.80      0.65      1273\n",
      "           1       0.77      0.37      0.50      1273\n",
      "\n",
      "    accuracy                           0.66      3819\n",
      "   macro avg       0.69      0.66      0.64      3819\n",
      "weighted avg       0.69      0.66      0.64      3819\n",
      "\n",
      "[[1010  231   32]\n",
      " [ 135 1024  114]\n",
      " [ 179  617  477]]\n"
     ]
    }
   ],
   "source": [
    "# Count Vectorizer\n",
    "print (\"----COUNT VECTORIZER ----\")\n",
    "count_vec_predicted_RF= random_florest(X_train_count_vec, y_train, X_val_count_vec, y_val, X_test_count_vec) \n",
    "print(metrics.classification_report(y_test, count_vec_predicted_RF))\n",
    "print(metrics.confusion_matrix(y_test, count_vec_predicted_RF))\n",
    "\n",
    "print (\"c/oversampling\")\n",
    "count_vec_predicted_RF_over= random_florest(X_train_oversampling_count_vec, y_train_oversampling, X_val_oversampling_count_vec, y_val_oversampling, X_test_oversampling_count_vec) \n",
    "print(metrics.classification_report(y_test_oversampling, count_vec_predicted_RF_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, count_vec_predicted_RF_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f80463ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----TF-IDF ----\n",
      "Melhores hiperparâmetros Random Forest:\n",
      "{'n_estimators': 150, 'max_depth': 43, 'max_features': 'sqrt', 'criterion': 'entropy', 'min_samples_split': 20, 'min_samples_leaf': 1}\n",
      "Melhor F1 obtido: 0.5385620509495447\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.78      0.64      0.70       417\n",
      "           0       0.80      0.95      0.87      1273\n",
      "           1       0.00      0.00      0.00       168\n",
      "\n",
      "    accuracy                           0.79      1858\n",
      "   macro avg       0.53      0.53      0.52      1858\n",
      "weighted avg       0.72      0.79      0.75      1858\n",
      "\n",
      "[[ 265  152    0]\n",
      " [  63 1209    1]\n",
      " [  10  158    0]]\n",
      "c/oversampling\n",
      "Melhores hiperparâmetros Random Forest:\n",
      "{'n_estimators': 200, 'max_depth': 34, 'max_features': 'log2', 'criterion': 'entropy', 'min_samples_split': 17, 'min_samples_leaf': 4}\n",
      "Melhor F1 obtido: 0.6906231760667535\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.77      0.76      0.76      1273\n",
      "           0       0.52      0.78      0.62      1273\n",
      "           1       0.66      0.34      0.45      1273\n",
      "\n",
      "    accuracy                           0.63      3819\n",
      "   macro avg       0.65      0.63      0.61      3819\n",
      "weighted avg       0.65      0.63      0.61      3819\n",
      "\n",
      "[[968 238  67]\n",
      " [124 987 162]\n",
      " [171 667 435]]\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF\n",
    "print (\"----TF-IDF ----\")\n",
    "tfidf_vec_predicted_RF= random_florest(X_train_tfidf_vec, y_train, X_val_tfidf_vec, y_val, X_test_tfidf_vec) \n",
    "print(metrics.classification_report(y_test, tfidf_vec_predicted_RF))\n",
    "print(metrics.confusion_matrix(y_test, tfidf_vec_predicted_RF))\n",
    "\n",
    "#TF-IDF\n",
    "print (\"c/oversampling\")\n",
    "tfidf_vec_predicted_RF_over= random_florest(X_train_oversampling_tfidf_vec, y_train_oversampling, X_val_oversampling_tfidf_vec, y_val_oversampling, X_test_oversampling_tfidf_vec) \n",
    "print(metrics.classification_report(y_test_oversampling, tfidf_vec_predicted_RF_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, tfidf_vec_predicted_RF_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2542f850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count vec + tf idf\n",
      "Melhores hiperparâmetros Random Forest:\n",
      "{'n_estimators': 150, 'max_depth': 37, 'max_features': None, 'criterion': 'entropy', 'min_samples_split': 6, 'min_samples_leaf': 1}\n",
      "Melhor F1 obtido: 0.560136606322754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.80      0.64      0.71       417\n",
      "           0       0.80      0.95      0.87      1273\n",
      "           1       0.00      0.00      0.00       168\n",
      "\n",
      "    accuracy                           0.80      1858\n",
      "   macro avg       0.53      0.53      0.53      1858\n",
      "weighted avg       0.72      0.80      0.75      1858\n",
      "\n",
      "[[ 268  149    0]\n",
      " [  62 1211    0]\n",
      " [   6  162    0]]\n",
      "oversampling\n",
      "Melhores hiperparâmetros Random Forest:\n",
      "{'n_estimators': 250, 'max_depth': 40, 'max_features': 'log2', 'criterion': 'gini', 'min_samples_split': 7, 'min_samples_leaf': 6}\n",
      "Melhor F1 obtido: 0.6986316573908139\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.78      0.80      0.79      1273\n",
      "           0       0.54      0.79      0.64      1273\n",
      "           1       0.73      0.36      0.49      1273\n",
      "\n",
      "    accuracy                           0.65      3819\n",
      "   macro avg       0.68      0.65      0.64      3819\n",
      "weighted avg       0.68      0.65      0.64      3819\n",
      "\n",
      "[[1017  217   39]\n",
      " [ 133 1011  129]\n",
      " [ 150  661  462]]\n"
     ]
    }
   ],
   "source": [
    "print(\"count vec + tf idf\")\n",
    "freq_predicted_RF= random_florest(X_train_freq, y_train,X_val_freq,y_val, X_test_freq, n_trials=30) \n",
    "print(metrics.classification_report(y_test,freq_predicted_RF))\n",
    "print(metrics.confusion_matrix(y_test, freq_predicted_RF))\n",
    "\n",
    "print(\"oversampling\")\n",
    "freq_predicted_RF_over= random_florest(X_train_freq_over, y_train_oversampling, X_val_freq_over, y_val_oversampling, X_test_freq_over, n_trials=30) \n",
    "print(metrics.classification_report(y_test_oversampling,freq_predicted_RF_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, freq_predicted_RF_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6516285a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----WORD2VEC ----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Word2vec\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----WORD2VEC ----\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m word2vec_predicted_RF\u001b[38;5;241m=\u001b[39m random_florest(X_train_word2vec, y_train, X_val_word2vec, y_val, X_test_word2vec) \n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(metrics\u001b[38;5;241m.\u001b[39mclassification_report(y_test, word2vec_predicted_RF))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(metrics\u001b[38;5;241m.\u001b[39mconfusion_matrix(y_test, word2vec_predicted_RF))\n",
      "Cell \u001b[1;32mIn[51], line 36\u001b[0m, in \u001b[0;36mrandom_florest\u001b[1;34m(X_train, y_train, X_val, y_val, X_test, n_trials)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrandom_florest\u001b[39m(X_train, y_train, X_val, y_val, X_test, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):\n\u001b[0;32m     35\u001b[0m     study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m     study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: objective_random_forest(trial, X_train, X_val, y_train, y_val),\n\u001b[0;32m     37\u001b[0m                    n_trials\u001b[38;5;241m=\u001b[39mn_trials, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMelhores hiperparâmetros Random Forest:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(study\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "File \u001b[1;32mc:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    390\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    397\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    398\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    399\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 490\u001b[0m     _optimize(\n\u001b[0;32m    491\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    492\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m    493\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[0;32m    494\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    495\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    496\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[0;32m    497\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    498\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[0;32m    499\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[0;32m    500\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:97\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(futures) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m n_jobs:\n\u001b[1;32m---> 97\u001b[0m     completed, futures \u001b[38;5;241m=\u001b[39m wait(futures, return_when\u001b[38;5;241m=\u001b[39mFIRST_COMPLETED)\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;66;03m# Raise if exception occurred in executing the completed futures.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m completed:\n",
      "File \u001b[1;32mc:\\Users\\didia\\anaconda3\\Lib\\concurrent\\futures\\_base.py:305\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(fs, timeout, return_when)\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m DoneAndNotDoneFutures(done, not_done)\n\u001b[0;32m    303\u001b[0m     waiter \u001b[38;5;241m=\u001b[39m _create_and_install_waiters(fs, return_when)\n\u001b[1;32m--> 305\u001b[0m waiter\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fs:\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m f\u001b[38;5;241m.\u001b[39m_condition:\n",
      "File \u001b[1;32mc:\\Users\\didia\\anaconda3\\Lib\\threading.py:622\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    620\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 622\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\Users\\didia\\anaconda3\\Lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Word2vec\n",
    "print (\"----WORD2VEC ----\")\n",
    "word2vec_predicted_RF= random_florest(X_train_word2vec, y_train, X_val_word2vec, y_val, X_test_word2vec) \n",
    "print(metrics.classification_report(y_test, word2vec_predicted_RF))\n",
    "print(metrics.confusion_matrix(y_test, word2vec_predicted_RF))\n",
    "\n",
    "print (\"c/oversampling\")\n",
    "word2vec_predicted_RF_over= random_florest(X_train_oversampling_word2vec, y_train_oversampling, X_val_oversampling_word2vec, y_val_oversampling, X_test_oversampling_word2vec) \n",
    "print(metrics.classification_report(y_test_oversampling, word2vec_predicted_RF_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, word2vec_predicted_RF_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa17c8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec treinado com os nossos dados\n",
    "word2vec_trained_predicted_RF= random_florest(X_train_w2v_trained, y_train, X_val_w2v_trained, y_val, X_test_w2v_trained, n_trials=30) \n",
    "print(metrics.classification_report(y_test, word2vec_trained_predicted_RF))\n",
    "print(metrics.confusion_matrix(y_test, word2vec_trained_predicted_RF))\n",
    "\n",
    "print(\"oversampling\")\n",
    "word2vec_trained_predicted_RF_over= svm(X_train_over_w2v_trained, y_train_oversampling, X_val_over_w2v_trained, y_val_oversampling,X_test_over_w2v_trained, n_trials=30) \n",
    "print(metrics.classification_report(y_test_oversampling, word2vec_trained_predicted_RF_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, word2vec_trained_predicted_RF_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7c1907",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Glove\n",
    "print (\"---- GLOVE ----\")\n",
    "glove_predicted_DT= random_florest(X_train_glove, y_train, X_val_glove, y_val, X_test_glove, y_test) \n",
    "print(metrics.classification_report(y_test, glove_predicted_DT))\n",
    "print(metrics.confusion_matrix(y_test, glove_predicted_DT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01765274",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a144959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import optuna\n",
    "\n",
    "def objective(X_train, y_train, X_val, y_val, trial):\n",
    "    hidden_layer_sizes = trial.suggest_categorical('hidden_layer_sizes', [50, 100, 200])\n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'tanh'])\n",
    "    learning_rate_init = trial.suggest_float('learning_rate_init', 1e-4, 1e-2, log=True)\n",
    "\n",
    "    clf = MLPClassifier(\n",
    "        hidden_layer_sizes=hidden_layer_sizes,\n",
    "        activation=activation,\n",
    "        learning_rate_init=learning_rate_init,\n",
    "        max_iter=300,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_val)\n",
    "    f1 = f1_score(y_val, preds, average='macro')\n",
    "    return f1\n",
    "\n",
    "def neural_network(X_train, y_train, X_val, y_val, X_test):\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(X_train, y_train, X_val, y_val, trial), n_trials=30)\n",
    "    print(\"Melhores parâmetros:\", study.best_params)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_model = MLPClassifier(\n",
    "        hidden_layer_sizes=best_params['hidden_layer_sizes'],\n",
    "        activation=best_params['activation'],\n",
    "        learning_rate_init=best_params['learning_rate_init'],\n",
    "        max_iter=300,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    best_model.fit(X_train, y_train)\n",
    "    previsoes = best_model.predict(X_test)\n",
    "    \n",
    "    return previsoes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0898740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----COUNT VECTORIZER ----\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train_count_vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Count Vectorizer\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----COUNT VECTORIZER ----\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m count_vec_predicted_NN\u001b[38;5;241m=\u001b[39m neural_network(X_train_count_vec, y_train, X_val_count_vec, y_val, X_test_count_vec) \n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(metrics\u001b[38;5;241m.\u001b[39mclassification_report(y_test, count_vec_predicted_NN))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(metrics\u001b[38;5;241m.\u001b[39mconfusion_matrix(y_test, count_vec_predicted_NN))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_count_vec' is not defined"
     ]
    }
   ],
   "source": [
    "# Count Vectorizer\n",
    "print (\"----COUNT VECTORIZER ----\")\n",
    "count_vec_predicted_NN= neural_network(X_train_count_vec, y_train, X_val_count_vec, y_val, X_test_count_vec) \n",
    "print(metrics.classification_report(y_test, count_vec_predicted_NN))\n",
    "print(metrics.confusion_matrix(y_test, count_vec_predicted_NN))\n",
    "\n",
    "print (\"c/oversampling\")\n",
    "count_vec_predicted_NN_over= neural_network(X_train_oversampling_count_vec, y_train_oversampling, X_val_oversampling_count_vec, y_val_oversampling, X_test_oversampling_count_vec) \n",
    "print(metrics.classification_report(y_test_oversampling, count_vec_predicted_NN_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, count_vec_predicted_NN_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b4947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "print (\"----TF-IDF ----\")\n",
    "tfidf_vec_predicted_NN= neural_network(X_train_tfidf_vec, y_train, X_val_tfidf_vec, y_val, X_test_tfidf_vec) \n",
    "print(metrics.classification_report(y_test, tfidf_vec_predicted_NN))\n",
    "print(metrics.confusion_matrix(y_test, tfidf_vec_predicted_NN))\n",
    "\n",
    "#TF-IDF\n",
    "print (\"c/oversampling\")\n",
    "tfidf_vec_predicted_NN_over= neural_network(X_train_oversampling_tfidf_vec, y_train_oversampling, X_val_oversampling_tfidf_vec, y_val_oversampling, X_test_oversampling_tfidf_vec) \n",
    "print(metrics.classification_report(y_test_oversampling, tfidf_vec_predicted_NN_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, tfidf_vec_predicted_NN_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9242ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count vec + tf idf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros: {'hidden_layer_sizes': (100, 50), 'activation': 'relu', 'learning_rate_init': 0.005324643450656601}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.77      0.74      0.75       417\n",
      "           0       0.84      0.88      0.86      1273\n",
      "           1       0.25      0.17      0.20       168\n",
      "\n",
      "    accuracy                           0.78      1858\n",
      "   macro avg       0.62      0.60      0.60      1858\n",
      "weighted avg       0.77      0.78      0.77      1858\n",
      "\n",
      "[[ 308   97   12]\n",
      " [  77 1120   76]\n",
      " [  15  124   29]]\n",
      "oversampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:788: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "print(\"count vec + tf idf\")\n",
    "freq_predicted_NN= neural_network(X_train_freq, y_train,X_val_freq,y_val, X_test_freq) \n",
    "print(metrics.classification_report(y_test,freq_predicted_NN))\n",
    "print(metrics.confusion_matrix(y_test, freq_predicted_NN))\n",
    "\n",
    "print(\"oversampling\")\n",
    "freq_predicted_NN_over= neural_network(X_train_freq_over, y_train_oversampling, X_val_freq_over, y_val_oversampling, X_test_freq_over) \n",
    "print(metrics.classification_report(y_test_oversampling,freq_predicted_NN_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, freq_predicted_NN_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03af9815",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2vec\n",
    "print (\"----WORD2VEC ----\")\n",
    "word2vec_predicted_NN= neural_network(X_train_word2vec, y_train, X_val_word2vec, y_val, X_test_word2vec) \n",
    "print(metrics.classification_report(y_test, word2vec_predicted_NN))\n",
    "print(metrics.confusion_matrix(y_test, word2vec_predicted_NN))\n",
    "\n",
    "print (\"c/oversampling\")\n",
    "word2vec_predicted_NN_over= neural_network(X_train_oversampling_word2vec, y_train_oversampling, X_val_oversampling_word2vec, y_val_oversampling, X_test_oversampling_word2vec) \n",
    "print(metrics.classification_report(y_test_oversampling, word2vec_predicted_NN_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, word2vec_predicted_NN_over))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2cd2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec treinado com os nossos dados\n",
    "word2vec_trained_predicted_NN= neural_network(X_train_w2v_trained, y_train, X_val_w2v_trained, y_val, X_test_w2v_trained, n_trials=30) \n",
    "print(metrics.classification_report(y_test, word2vec_trained_predicted_NN))\n",
    "print(metrics.confusion_matrix(y_test, word2vec_trained_predicted_NN))\n",
    "\n",
    "print(\"oversampling\")\n",
    "word2vec_trained_predicted_NN_over= svm(X_train_over_w2v_trained, y_train_oversampling, X_val_over_w2v_trained, y_val_oversampling,X_test_over_w2v_trained, n_trials=30) \n",
    "print(metrics.classification_report(y_test_oversampling, word2vec_trained_predicted_NN_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, word2vec_trained_predicted_NN_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3c3b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Glove\n",
    "print (\"----GLOVE ----\")\n",
    "glove_predicted_NN= neural_network(X_train_glove, y_train, X_val_glove, y_val, X_test_glove) \n",
    "print(metrics.classification_report(y_test, glove_predicted_NN))\n",
    "print(metrics.confusion_matrix(y_test, glove_predicted_NN))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f827bcd",
   "metadata": {},
   "source": [
    "## Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d051225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "import optuna\n",
    "\n",
    "def objective(X_train, y_train, X_val, y_val, trial):\n",
    "    # Hiperparâmetros a otimizar\n",
    "    C = trial.suggest_float('C', 1e-3, 1e3, log=True)\n",
    "    penalty = trial.suggest_categorical('penalty', ['l1', 'l2'])\n",
    "    solver = trial.suggest_categorical('solver', ['liblinear', 'saga'])  # compatíveis com L1 e L2\n",
    "    max_iter = trial.suggest_int('max_iter', 100, 1000)\n",
    "\n",
    "    model = LogisticRegression(\n",
    "        C=C,\n",
    "        penalty=penalty,\n",
    "        solver=solver,\n",
    "        max_iter=max_iter,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_val)\n",
    "\n",
    "    f1 = f1_score(y_val, preds, average='macro')\n",
    "    return f1\n",
    "\n",
    "\n",
    "def logistic_regression(X_train, y_train, X_val, y_val, X_test):\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(X_train, y_train, X_val, y_val, trial), n_trials=30)\n",
    "\n",
    "    print(\"Melhores parâmetros encontrados:\")\n",
    "    print(study.best_params)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_model = LogisticRegression(\n",
    "        **best_params,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    previsoes = best_model.predict(X_test)\n",
    "\n",
    "    return previsoes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e03c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-13 01:38:47,774] A new study created in memory with name: no-name-f804a6c1-a9ec-4956-92ee-a72dd1f88037\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:38:47,815] Trial 0 finished with value: 0.5759647633129582 and parameters: {'C': 0.5702975431962373, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 434}. Best is trial 0 with value: 0.5759647633129582.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:38:47,838] Trial 1 finished with value: 0.4817582369487045 and parameters: {'C': 0.024178449537411693, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 958}. Best is trial 0 with value: 0.5759647633129582.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----COUNT VECTORIZER ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:38:48,033] Trial 2 finished with value: 0.6556576016190188 and parameters: {'C': 100.62436324687397, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 135}. Best is trial 2 with value: 0.6556576016190188.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:38:48,752] Trial 3 finished with value: 0.6504041189506938 and parameters: {'C': 162.82595003814305, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 567}. Best is trial 2 with value: 0.6556576016190188.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:38:49,360] Trial 4 finished with value: 0.48280955931962644 and parameters: {'C': 0.020068665251334997, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 452}. Best is trial 2 with value: 0.6556576016190188.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:38:49,472] Trial 5 finished with value: 0.6133551317639342 and parameters: {'C': 39.57151379943558, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 615}. Best is trial 2 with value: 0.6556576016190188.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:38:49,508] Trial 6 finished with value: 0.5098039215686274 and parameters: {'C': 0.294397046750876, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 992}. Best is trial 2 with value: 0.6556576016190188.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:38:49,547] Trial 7 finished with value: 0.5705119006104282 and parameters: {'C': 0.20205435998066762, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 776}. Best is trial 2 with value: 0.6556576016190188.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:38:49,559] Trial 8 finished with value: 0.4234293621241545 and parameters: {'C': 0.003557600040672028, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 275}. Best is trial 2 with value: 0.6556576016190188.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:38:49,743] Trial 9 finished with value: 0.6077744553829092 and parameters: {'C': 553.2979230436213, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 537}. Best is trial 2 with value: 0.6556576016190188.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:38:59,219] Trial 10 finished with value: 0.6552789886034468 and parameters: {'C': 9.916768695437854, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 127}. Best is trial 2 with value: 0.6556576016190188.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:39:07,423] Trial 11 finished with value: 0.6647095405734628 and parameters: {'C': 15.282658311116817, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 104}. Best is trial 11 with value: 0.6647095405734628.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:39:16,389] Trial 12 finished with value: 0.6637222489559013 and parameters: {'C': 9.85712118634492, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 114}. Best is trial 11 with value: 0.6647095405734628.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:39:36,151] Trial 13 finished with value: 0.6622131951818232 and parameters: {'C': 14.088989037426721, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 252}. Best is trial 11 with value: 0.6647095405734628.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:39:48,662] Trial 14 finished with value: 0.6770186152390382 and parameters: {'C': 3.4457245926565587, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 277}. Best is trial 14 with value: 0.6770186152390382.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:39:58,891] Trial 15 finished with value: 0.6650737387526919 and parameters: {'C': 2.361814418410887, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 277}. Best is trial 14 with value: 0.6770186152390382.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:40:10,975] Trial 16 finished with value: 0.6650737387526919 and parameters: {'C': 2.3236036584634028, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 308}. Best is trial 14 with value: 0.6770186152390382.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:40:22,669] Trial 17 finished with value: 0.6594394079862055 and parameters: {'C': 1.9281679823370776, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 380}. Best is trial 14 with value: 0.6770186152390382.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:40:23,784] Trial 18 finished with value: 0.4211064817524936 and parameters: {'C': 0.048462891612876466, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 664}. Best is trial 14 with value: 0.6770186152390382.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:40:34,035] Trial 19 finished with value: 0.6677610475806594 and parameters: {'C': 3.068460582723694, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 220}. Best is trial 14 with value: 0.6770186152390382.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:41:07,142] Trial 20 finished with value: 0.6496410997208824 and parameters: {'C': 722.5038133378972, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 366}. Best is trial 14 with value: 0.6770186152390382.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:41:16,031] Trial 21 finished with value: 0.6647007988963143 and parameters: {'C': 2.2792048244156153, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 234}. Best is trial 14 with value: 0.6770186152390382.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:41:16,399] Trial 22 finished with value: 0.47707987986016515 and parameters: {'C': 0.10833353334598418, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 174}. Best is trial 14 with value: 0.6770186152390382.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:41:20,878] Trial 23 finished with value: 0.6345127108226912 and parameters: {'C': 1.121590028412787, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 207}. Best is trial 14 with value: 0.6770186152390382.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:41:37,844] Trial 24 finished with value: 0.6659089482536446 and parameters: {'C': 3.941867775587999, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 334}. Best is trial 14 with value: 0.6770186152390382.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:42:01,263] Trial 25 finished with value: 0.662745322386999 and parameters: {'C': 5.689394883043907, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 374}. Best is trial 14 with value: 0.6770186152390382.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:42:42,737] Trial 26 finished with value: 0.6481329413472314 and parameters: {'C': 56.76384691306071, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 475}. Best is trial 14 with value: 0.6770186152390382.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:42:45,786] Trial 27 finished with value: 0.5909131725514046 and parameters: {'C': 0.5544105718153584, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 322}. Best is trial 14 with value: 0.6770186152390382.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:00,598] Trial 28 finished with value: 0.6622350604364993 and parameters: {'C': 26.528181254690583, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 193}. Best is trial 14 with value: 0.6770186152390382.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:21,725] Trial 29 finished with value: 0.6580010632642211 and parameters: {'C': 4.299736531448158, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 402}. Best is trial 14 with value: 0.6770186152390382.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros encontrados:\n",
      "{'C': 3.4457245926565587, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 277}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:34,711] A new study created in memory with name: no-name-8b78f7ef-34bf-4cd4-ae02-5e635fa2a503\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:34,881] Trial 0 finished with value: 0.677824372328581 and parameters: {'C': 1.6684911551814994, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 894}. Best is trial 0 with value: 0.677824372328581.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.77      0.64      0.70       417\n",
      "           0       0.81      0.91      0.86      1273\n",
      "           1       0.34      0.18      0.24       168\n",
      "\n",
      "    accuracy                           0.78      1858\n",
      "   macro avg       0.64      0.58      0.60      1858\n",
      "weighted avg       0.76      0.78      0.77      1858\n",
      "\n",
      "[[ 268  139   10]\n",
      " [  67 1159   47]\n",
      " [  13  125   30]]\n",
      "c/oversampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-13 01:43:36,823] Trial 1 finished with value: 0.7002086643251745 and parameters: {'C': 0.01636145823939285, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 969}. Best is trial 1 with value: 0.7002086643251745.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:37,006] Trial 2 finished with value: 0.6204439095503163 and parameters: {'C': 41.268624677841466, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 568}. Best is trial 1 with value: 0.7002086643251745.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:37,040] Trial 3 finished with value: 0.6221880187008261 and parameters: {'C': 0.0012823722959344008, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 808}. Best is trial 1 with value: 0.7002086643251745.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:39,220] Trial 4 finished with value: 0.6401537638542519 and parameters: {'C': 122.8421119866404, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 649}. Best is trial 1 with value: 0.7002086643251745.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:39,356] Trial 5 finished with value: 0.6570220088060158 and parameters: {'C': 2.294571552068283, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 665}. Best is trial 1 with value: 0.7002086643251745.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:39,867] Trial 6 finished with value: 0.6667492440822625 and parameters: {'C': 0.0038026738684157663, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 169}. Best is trial 1 with value: 0.7002086643251745.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:40,007] Trial 7 finished with value: 0.7006355543663503 and parameters: {'C': 1.3062076911105878, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 870}. Best is trial 7 with value: 0.7006355543663503.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:40,085] Trial 8 finished with value: 0.7168878035705196 and parameters: {'C': 0.1084583216913827, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 263}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:41,991] Trial 9 finished with value: 0.6401537638542519 and parameters: {'C': 837.6202799019968, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 623}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:42,055] Trial 10 finished with value: 0.6193004947549531 and parameters: {'C': 0.05931491306642158, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 286}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:42,122] Trial 11 finished with value: 0.6989057396281192 and parameters: {'C': 0.13776121438497146, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 394}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:42,207] Trial 12 finished with value: 0.6854682321308969 and parameters: {'C': 0.3379765746526792, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 415}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:42,483] Trial 13 finished with value: 0.6720197336223274 and parameters: {'C': 5.657299044318926, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 102}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:42,754] Trial 14 finished with value: 0.6537956850376825 and parameters: {'C': 10.997328673340208, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 798}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:46,933] Trial 15 finished with value: 0.6862934565285325 and parameters: {'C': 0.5011213472790349, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 448}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:46,999] Trial 16 finished with value: 0.5647387556533549 and parameters: {'C': 0.02273326883194768, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 274}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:47,089] Trial 17 finished with value: 0.6855694753307763 and parameters: {'C': 0.11777143518743549, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 756}. Best is trial 8 with value: 0.7168878035705196.\n",
      "[I 2025-11-13 01:43:47,362] Trial 18 finished with value: 0.473367223901145 and parameters: {'C': 0.011088350124596877, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 995}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:47,539] Trial 19 finished with value: 0.6350408393593395 and parameters: {'C': 16.868490702339628, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 517}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:47,731] Trial 20 finished with value: 0.7082776325765678 and parameters: {'C': 0.7359353818334339, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 338}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:47,938] Trial 21 finished with value: 0.7104264295013126 and parameters: {'C': 0.9945873004294815, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 284}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:48,059] Trial 22 finished with value: 0.6905250105198041 and parameters: {'C': 0.36801697067916017, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 280}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:48,132] Trial 23 finished with value: 0.6276268943316103 and parameters: {'C': 0.060651641125637805, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 214}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:48,541] Trial 24 finished with value: 0.6704927005304944 and parameters: {'C': 4.4153442890075265, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 349}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:48,726] Trial 25 finished with value: 0.6924120556187446 and parameters: {'C': 0.563047911345828, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 193}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:50,030] Trial 26 finished with value: 0.706282382316446 and parameters: {'C': 0.1922270598084053, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 345}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:50,107] Trial 27 finished with value: 0.5882229614576487 and parameters: {'C': 0.04035278692823199, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 497}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:50,172] Trial 28 finished with value: 0.670307613404951 and parameters: {'C': 0.006543860084360959, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 102}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:50,303] Trial 29 finished with value: 0.6835157979056753 and parameters: {'C': 1.625244943582272, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 361}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros encontrados:\n",
      "{'C': 0.1084583216913827, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 263}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.84      0.77      0.80      1273\n",
      "           0       0.54      0.83      0.65      1273\n",
      "           1       0.72      0.40      0.51      1273\n",
      "\n",
      "    accuracy                           0.67      3819\n",
      "   macro avg       0.70      0.67      0.66      3819\n",
      "weighted avg       0.70      0.67      0.66      3819\n",
      "\n",
      "[[ 979  219   75]\n",
      " [  96 1058  119]\n",
      " [  89  681  503]]\n"
     ]
    }
   ],
   "source": [
    "# Count Vectorizer\n",
    "print (\"----COUNT VECTORIZER ----\")\n",
    "count_vec_predicted_LR= logistic_regression(X_train_count_vec, y_train, X_val_count_vec, y_val, X_test_count_vec) \n",
    "print(metrics.classification_report(y_test, count_vec_predicted_LR))\n",
    "print(metrics.confusion_matrix(y_test, count_vec_predicted_LR))\n",
    "\n",
    "print (\"c/oversampling\")\n",
    "count_vec_predicted_LR_over= logistic_regression(X_train_oversampling_count_vec, y_train_oversampling, X_val_oversampling_count_vec, y_val_oversampling, X_test_oversampling_count_vec) \n",
    "print(metrics.classification_report(y_test_oversampling, count_vec_predicted_LR_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, count_vec_predicted_LR_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d55ab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "print (\"----TF-IDF ----\")\n",
    "tfidf_vec_predicted_LR= logistic_regression(X_train_tfidf_vec, y_train, X_val_tfidf_vec, y_val, X_test_tfidf_vec) \n",
    "print(metrics.classification_report(y_test, tfidf_vec_predicted_LR))\n",
    "print(metrics.confusion_matrix(y_test, tfidf_vec_predicted_LR))\n",
    "\n",
    "#TF-IDF\n",
    "print (\"c/oversampling\")\n",
    "tfidf_vec_predicted_LR_over= logistic_regression(X_train_oversampling_tfidf_vec, y_train_oversampling, X_val_oversampling_tfidf_vec, y_val_oversampling, X_test_oversampling_tfidf_vec) \n",
    "print(metrics.classification_report(y_test_oversampling, tfidf_vec_predicted_LR_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, tfidf_vec_predicted_LR_over))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc322f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"count vec + tf idf\")\n",
    "freq_predicted_LR= logistic_regression(X_train_freq, y_train,X_val_freq,y_val, X_test_freq) \n",
    "print(metrics.classification_report(y_test,freq_predicted_RF))\n",
    "print(metrics.confusion_matrix(y_test, freq_predicted_RF))\n",
    "\n",
    "print(\"oversampling\")\n",
    "freq_predicted_LR_over= logistic_regression(X_train_freq_over, y_train_oversampling, X_val_freq_over, y_val_oversampling, X_test_freq_over) \n",
    "print(metrics.classification_report(y_test_oversampling,freq_predicted_LR_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, freq_predicted_LR_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d486c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2vec\n",
    "print (\"----WORD2VEC ----\")\n",
    "word2vec_predicted_LR= logistic_regression(X_train_word2vec, y_train, X_val_word2vec, y_val, X_test_word2vec) \n",
    "print(metrics.classification_report(y_test, word2vec_predicted_LR))\n",
    "print(metrics.confusion_matrix(y_test, word2vec_predicted_LR))\n",
    "\n",
    "print (\"c/oversampling\")\n",
    "word2vec_predicted_LR_over= logistic_regression(X_train_oversampling_word2vec, y_train_oversampling, X_val_oversampling_word2vec, y_val_oversampling, X_test_oversampling_word2vec) \n",
    "print(metrics.classification_report(y_test_oversampling, word2vec_predicted_LR_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, word2vec_predicted_LR_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dabf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec treinado com os nossos dados\n",
    "word2vec_trained_predicted_LR= logistic_regression(X_train_w2v_trained, y_train, X_val_w2v_trained, y_val, X_test_w2v_trained) \n",
    "print(metrics.classification_report(y_test, word2vec_trained_predicted_LR))\n",
    "print(metrics.confusion_matrix(y_test, word2vec_trained_predicted_LR))\n",
    "\n",
    "print(\"oversampling\")\n",
    "word2vec_trained_predicted_LR_over= logistic_regression(X_train_over_w2v_trained, y_train_oversampling, X_val_over_w2v_trained, y_val_oversampling,X_test_over_w2v_trained) \n",
    "print(metrics.classification_report(y_test_oversampling, word2vec_trained_predicted_LR_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, word2vec_trained_predicted_LR_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c19df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Glove\n",
    "print (\"----GLOVE ----\")\n",
    "glove_predicted_LR= logistic_regression(X_train_glove, y_train, X_val_glove, y_val, X_test_glove, y_test) \n",
    "print(metrics.classification_report(y_test, glove_predicted_LR))\n",
    "print(metrics.confusion_matrix(y_test, glove_predicted_LR))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
