{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5874fce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 711, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 729, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 411, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 531, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3006, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3061, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3266, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3445, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\didia\\AppData\\Local\\Temp\\ipykernel_10916\\3441494382.py\", line 1, in <module>\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 61, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 53, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\__init__.py\", line 8, in <module>\n",
      "    from pandas.core.ops.array_ops import (\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py\", line 56, in <module>\n",
      "    from pandas.core.computation import expressions\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\pandas\\core\\computation\\expressions.py\", line 21, in <module>\n",
      "    from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\pandas\\core\\computation\\check.py\", line 5, in <module>\n",
      "    ne = import_optional_dependency(\"numexpr\", errors=\"warn\")\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\numexpr\\__init__.py\", line 24, in <module>\n",
      "    from numexpr.interpreter import MAX_THREADS, use_vml, __BLOCK_SIZE1__\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 711, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 729, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 411, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 531, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3006, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3061, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3266, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3445, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\didia\\AppData\\Local\\Temp\\ipykernel_10916\\3441494382.py\", line 1, in <module>\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 61, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 67, in <module>\n",
      "    from pandas.core.arrays.masked import BaseMaskedArray\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py\", line 61, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py\", line 52, in <module>\n",
      "    bn = import_optional_dependency(\"bottleneck\", errors=\"warn\")\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\bottleneck\\__init__.py\", line 7, in <module>\n",
      "    from .move import (move_argmax, move_argmin, move_max, move_mean, move_median,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import optuna\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.utils import resample\n",
    "import optuna\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "import optuna\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from huggingface_hub import hf_hub_download\n",
    "from safetensors.numpy import load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fddcf97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file          0\n",
       "id_sente      0\n",
       "id_article    0\n",
       "domain        0\n",
       "year          0\n",
       "sentences     0\n",
       "classe        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#carregar dataset\n",
    "df = pd.read_csv('factnews_dataset.csv')\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa80a44",
   "metadata": {},
   "source": [
    "# Pré-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b710fb",
   "metadata": {},
   "source": [
    "divisão dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16d457cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#criar grupo treino, validação e teste\\ntrain_df, test_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df[\\'classe\\'])\\ntrain_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df[\\'classe\\'])\\n\\n#guardar conjuntos em memória\\ntrain_df.to_csv(\"train.csv\", index=False) \\nval_df.to_csv(\"val.csv\", index=False)\\ntest_df.to_csv(\"test.csv\", index=False)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#criar grupo treino, validação e teste\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['classe'])\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['classe'])\n",
    "\n",
    "#guardar conjuntos em memória\n",
    "train_df.to_csv(\"train.csv\", index=False) \n",
    "val_df.to_csv(\"val.csv\", index=False)\n",
    "test_df.to_csv(\"test.csv\", index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9319ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.read_csv('train.csv')\n",
    "val= pd.read_csv('val.csv')\n",
    "test= pd.read_csv('test.csv')\n",
    "\n",
    "train_oversampling= pd.read_csv('train_oversampling.csv')\n",
    "val_oversampling= pd.read_csv('val_oversampling.csv')\n",
    "test_oversampling= pd.read_csv('test_oversampling.csv')\n",
    "\n",
    "y_train= train['classe']\n",
    "y_train_oversampling= train_oversampling['classe']\n",
    "\n",
    "y_val=val['classe']\n",
    "y_val_oversampling = val_oversampling['classe']\n",
    "\n",
    "y_test= test['classe']\n",
    "y_test_oversampling = test_oversampling['classe']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767f6d97",
   "metadata": {},
   "source": [
    "Pré-processamento da primeira meta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b071f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('factnews_dataset.csv')\n",
    "def pre_processamento_meta1(df):\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    #criar coluna tokens\n",
    "    df['tokens'] = df['sentences'].apply(lambda x: nltk.word_tokenize(str(x).lower()))\n",
    "    # Tokenizar sem stopwords\n",
    "    df['tokens'] = df['tokens'].apply(lambda toks: [t for t in toks if t not in stop_words])\n",
    "    return df\n",
    "\n",
    "train_meta1 = pre_processamento_meta1(train)\n",
    "val_meta1 = pre_processamento_meta1(val)\n",
    "test_meta1= pre_processamento_meta1(test)\n",
    "\n",
    "y_train_meta1=train_meta1['classe']\n",
    "y_val_meta1=val_meta1['classe']\n",
    "y_test_meta1=test_meta1['classe']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1708b8",
   "metadata": {},
   "source": [
    "balanceamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f65b63e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.read_csv('train.csv')\n",
    "val= pd.read_csv('val.csv')\n",
    "test= pd.read_csv('test.csv')\n",
    "\n",
    "train_oversampling= pd.read_csv('train_oversampling.csv')\n",
    "val_oversampling= pd.read_csv('val_oversampling.csv')\n",
    "test_oversampling= pd.read_csv('test_oversampling.csv')\n",
    "\n",
    "y_train= train['classe']\n",
    "y_train_oversampling= train_oversampling['classe']\n",
    "\n",
    "y_val=val['classe']\n",
    "y_val_oversampling = val_oversampling['classe']\n",
    "\n",
    "y_test= test['classe']\n",
    "y_test_oversampling = test_oversampling['classe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce488a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conjunto de treino:  classe\n",
      "-1     779\n",
      " 0    2375\n",
      " 1     312\n",
      "Name: count, dtype: int64\n",
      "\n",
      "conjunto de validação:  classe\n",
      "-1    195\n",
      " 0    594\n",
      " 1     78\n",
      "Name: count, dtype: int64\n",
      "\n",
      "conjunto de teste:  classe\n",
      "-1     417\n",
      " 0    1273\n",
      " 1     168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "conjunto de treino + exemplos:  classe\n",
      "-1    2375\n",
      " 0    2375\n",
      " 1    2375\n",
      "Name: count, dtype: int64\n",
      "\n",
      "conjunto de val + exemplos:  classe\n",
      "-1    594\n",
      " 0    594\n",
      " 1    594\n",
      "Name: count, dtype: int64\n",
      "\n",
      "conjunto de teste + exemplos:  classe\n",
      "-1    1273\n",
      " 0    1273\n",
      " 1    1273\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Contagem das classes em cada conjunto\n",
    "print (\"conjunto de treino: \",y_train.value_counts().sort_index())\n",
    "print(\"\\nconjunto de validação: \",y_val.value_counts().sort_index())\n",
    "print(\"\\nconjunto de teste: \", y_test.value_counts().sort_index())\n",
    "print(\"\\nconjunto de treino + exemplos: \", y_train_oversampling.value_counts().sort_index())\n",
    "print(\"\\nconjunto de val + exemplos: \", y_val_oversampling.value_counts().sort_index())\n",
    "print(\"\\nconjunto de teste + exemplos: \", y_test_oversampling.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "940b76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balancear_classes(conjunto):\n",
    "  # Separar cada classe\n",
    "  class_min = conjunto[conjunto['classe'] == -1]\n",
    "  class_med = conjunto[conjunto['classe'] == 1]\n",
    "  class_max = conjunto[conjunto['classe'] == 0]\n",
    "\n",
    "  # Número da classe maior\n",
    "  n_samples = len(class_max)\n",
    "\n",
    "  # Replicar as classes menores até o tamanho da maior\n",
    "  class_min_upsampled = resample(class_min, \n",
    "                              replace=True, \n",
    "                              n_samples=n_samples, \n",
    "                              random_state=42)\n",
    "\n",
    "  class_med_upsampled = resample(class_med, \n",
    "                              replace=True, \n",
    "                              n_samples=n_samples, \n",
    "                            random_state=42)\n",
    "\n",
    "  conjunto_oversampled = pd.concat([class_min_upsampled, class_med_upsampled, class_max])\n",
    "\n",
    "  # baralhar o conjunto \n",
    "  conjunto_oversampled = conjunto_oversampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "  print(conjunto_oversampled['classe'].value_counts())\n",
    "\n",
    "  return conjunto_oversampled\n",
    "\n",
    "#train_oversampling.to_csv('train_oversampling.csv', index=False)\n",
    "#val_oversampling = balancear_classes(val)\n",
    "#test_oversampling= balancear_classes(test)\n",
    "\n",
    "#val_oversampling.to_csv('val_oversampling.csv', index=False)\n",
    "#test_oversampling.to_csv('test_oversampling.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5262d8a4",
   "metadata": {},
   "source": [
    "tokeinização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d610e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\didia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [neym, ,, part, 20, novembr, vai, lider, seleç...\n",
      "1    [forç, internac, transform, quest, taiwan, ass...\n",
      "2    [alianç, mai, quant, parlament, eleit, nome, p...\n",
      "3    [ger, mort, '', ,, declar, cabr, ,, 16ª, entre...\n",
      "4    [bisp, barr, (, ba, ), ,, dom, luiz, flávi, ca...\n",
      "Name: stems, dtype: object\n",
      "0    [qued, ocorr, aliment, pux, cord, sent, contr,...\n",
      "1    [repercuss, nega, cas, fez, bolsonar, ,, candi...\n",
      "2    [brasil, lav, alm, após, decepcion, empat, col...\n",
      "3    [final, encontr, ,, sen, tucan, diss, esp, gov...\n",
      "4    [fiz, compromiss, líd, bas, hoj, líd, opos, ,,...\n",
      "Name: stems, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\didia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('portuguese'))\n",
    "def tokeinizar(train, val, test):\n",
    "\n",
    "    #criar coluna tokens\n",
    "    train['tokens'] = train['sentences'].apply(lambda x: nltk.word_tokenize(str(x)))\n",
    "    val['tokens']   = val['sentences'].apply(lambda x: nltk.word_tokenize(str(x)))\n",
    "    test['tokens'] = test['sentences'].apply(lambda x: nltk.word_tokenize(str(x)))\n",
    "    train_oversampling['tokens'] = train_oversampling['sentences'].apply(lambda x: nltk.word_tokenize(str(x)))\n",
    "\n",
    "    #remover stopwords\n",
    "    train['tokens'] = train['tokens'].apply(lambda toks: [t for t in toks if t.lower() not in stop_words])\n",
    "    val['tokens']   = val['tokens'].apply(lambda toks: [t for t in toks if t.lower() not in stop_words])\n",
    "    test['tokens']  = test['tokens'].apply(lambda toks: [t for t in toks if t.lower() not in stop_words])\n",
    "    train_oversampling['tokens'] = train_oversampling['tokens'].apply(lambda toks: [t for t in toks if t.lower() not in stop_words])\n",
    "\n",
    "    #matizar\n",
    "    #nltk . download ('rslp')\n",
    "    stemmer = nltk . stem . RSLPStemmer ()\n",
    "    train['stems'] = train['tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
    "    val['stems']   = val['tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
    "    test['stems'] = test['tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
    "    train_oversampling['stems'] = train_oversampling['tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
    "\n",
    "    nltk . download ('rslp')\n",
    "    stemmer = nltk . stem . RSLPStemmer ()\n",
    "    print(train['stems'].head())\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "train, val, test = tokeinizar(train, val, test)\n",
    "train_oversampling, val_oversampling, test_oversampling = tokeinizar(train_oversampling, val_oversampling, test_oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41608aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train= train['classe']\n",
    "y_train_oversampling= train_oversampling['classe']\n",
    "\n",
    "y_val=val['classe']\n",
    "y_val_oversampling = val_oversampling['classe']\n",
    "\n",
    "y_test= test['classe']\n",
    "y_test_oversampling = test_oversampling['classe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6cf346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50b02c9",
   "metadata": {},
   "source": [
    "# Construção dos modelos de representação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6799b632",
   "metadata": {},
   "source": [
    "## TF-IDF - fred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d13758dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_tfidf(trial):\n",
    "    # Parâmetros a otimizar\n",
    "    max_features = trial.suggest_int(\"max_features\", 500, 5000)\n",
    "    ngram_min = trial.suggest_int(\"ngram_min\", 1, 2)\n",
    "    ngram_max = trial.suggest_int(\"ngram_max\", ngram_min, 3) \n",
    "    min_df = trial.suggest_int(\"min_df\", 1, 10)\n",
    "    max_df = trial.suggest_float(\"max_df\", 0.3, 0.9)\n",
    "\n",
    "    tfidf_vect = TfidfVectorizer(\n",
    "        ngram_range=(ngram_min, ngram_max),\n",
    "        min_df=min_df,\n",
    "        max_df=max_df,\n",
    "        max_features=max_features,\n",
    "        strip_accents='unicode',\n",
    "        token_pattern=r\"(?u)\\b\\w+\\b|[^\\w\\s]\"\n",
    "    )\n",
    "\n",
    "    text_train = train['stems'].apply(lambda tokens: ' '.join(tokens))\n",
    "    text_val = val['stems'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "    X_train = tfidf_vect.fit_transform(text_train)\n",
    "    X_val = tfidf_vect.transform(text_val)\n",
    "\n",
    "    y_train = train['classe']\n",
    "    y_val = val['classe']\n",
    "\n",
    "    model = LogisticRegression(max_iter=500)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_val)\n",
    "\n",
    "    score = f1_score(y_val, preds, average='macro')\n",
    "    return score\n",
    "\n",
    "def tfidf_optuna(train, val, test):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective_tfidf, n_trials=30)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    print(\"Melhores parâmetros:\", best_params)\n",
    "\n",
    "    tfidf_vect = TfidfVectorizer(\n",
    "        ngram_range=(best_params['ngram_min'], best_params['ngram_max']),\n",
    "        min_df=best_params['min_df'],\n",
    "        max_df=best_params['max_df'],\n",
    "        max_features=best_params['max_features'],\n",
    "        strip_accents='unicode',\n",
    "        token_pattern=r\"(?u)\\b\\w+\\b|[^\\w\\s]\"\n",
    "    )\n",
    "\n",
    "    text_train = train['stems'].apply(lambda tokens: ' '.join(tokens))\n",
    "    text_val = val['stems'].apply(lambda tokens: ' '.join(tokens))\n",
    "    text_test = test['stems'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "    X_train = tfidf_vect.fit_transform(text_train)\n",
    "    X_val = tfidf_vect.transform(text_val)\n",
    "    X_test = tfidf_vect.transform(text_test)\n",
    "\n",
    "    return X_train, X_val, X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b98a9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros: {'max_features': 1049, 'ngram_min': 1, 'ngram_max': 1, 'min_df': 4, 'max_df': 0.4816199424117911}\n",
      "Melhores parâmetros: {'max_features': 2024, 'ngram_min': 1, 'ngram_max': 2, 'min_df': 7, 'max_df': 0.5043171462648697}\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf_vec, X_val_tfidf_vec, X_test_tfidf_vec = tfidf_optuna(train,val, test)\n",
    "X_train_oversampling_tfidf_vec, X_val_oversampling_tfidf_vec, X_test_oversampling_tfidf_vec = tfidf_optuna(train_oversampling, val_oversampling, test_oversampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eaac49",
   "metadata": {},
   "source": [
    "## CountVectorizer -didi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1399bde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_count_vec(trial, train, val):\n",
    "    # Parâmetros a otimizar\n",
    "    max_features = trial.suggest_int(\"max_features\", 500, 5000)\n",
    "    ngram_min = trial.suggest_int(\"ngram_min\", 1, 2)\n",
    "    ngram_max = trial.suggest_int(\"ngram_max\", ngram_min, 3)  # garante ngram_max >= ngram_min\n",
    "\n",
    "    # CountVectorizer com parâmetros sugeridos\n",
    "    c_vect = CountVectorizer(\n",
    "        token_pattern=r\"(?u)\\b\\w+\\b|[^\\w\\s]\",\n",
    "        ngram_range=(ngram_min, ngram_max),\n",
    "        strip_accents='unicode',\n",
    "        max_features=max_features\n",
    "    )\n",
    "\n",
    "    # Transformar textos\n",
    "    text_train = train['stems'].apply(lambda tokens: ' '.join(tokens))\n",
    "    text_val   = val['stems'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "    X_train = c_vect.fit_transform(text_train)\n",
    "    X_val   = c_vect.transform(text_val)\n",
    "\n",
    "    y_train = train['classe'] \n",
    "    y_val   = val['classe']\n",
    "\n",
    "    # Treinar Logistic Regression\n",
    "    model = LogisticRegression(max_iter=500)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_val)\n",
    "\n",
    "    # Métrica: F1 macro\n",
    "    score = f1_score(y_val, preds, average='macro')\n",
    "    return score \n",
    "\n",
    "def count_vec(train, val, test):\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective_count_vec(trial, train,val),\n",
    "                   n_trials=30, n_jobs=-1, show_progress_bar=False)\n",
    "    \n",
    "    best_params=study.best_params\n",
    "    print(\"Melhores parâmetros:\", study.best_params)\n",
    "    print(\"Melhor F1:\", study.best_value)\n",
    "    \n",
    "    c_vect = CountVectorizer(\n",
    "        token_pattern=r\"(?u)\\b\\w+\\b|[^\\w\\s]\",\n",
    "        ngram_range=(best_params['ngram_min'], best_params['ngram_max']),\n",
    "        strip_accents='unicode',\n",
    "        max_features=best_params['max_features']\n",
    "    )\n",
    "\n",
    "    # Transformar textos\n",
    "    text_train = train['stems'].apply(lambda tokens: ' '.join(tokens))\n",
    "    text_val = val['stems'].apply(lambda tokens: ' '.join(tokens))\n",
    "    text_test = test['stems'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "    # Fit no treino, transform nos outros conjuntos\n",
    "    X_train = c_vect.fit_transform(text_train)\n",
    "    X_val = c_vect.transform(text_val)\n",
    "    X_test = c_vect.transform(text_test)\n",
    "\n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "666f038f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros: {'max_features': 2960, 'ngram_min': 1, 'ngram_max': 3}\n",
      "Melhor F1: 0.6717121522910342\n",
      "Melhores parâmetros: {'max_features': 3552, 'ngram_min': 1, 'ngram_max': 1}\n",
      "Melhor F1: 0.6932399038941085\n"
     ]
    }
   ],
   "source": [
    "X_train_count_vec, X_val_count_vec, X_test_count_vec = count_vec(train, val, test)\n",
    "X_train_oversampling_count_vec, X_val_oversampling_count_vec, X_test_oversampling_count_vec= count_vec(train_oversampling, val_oversampling, test_oversampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a974ef",
   "metadata": {},
   "source": [
    "## Glove -fred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec3f116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = KeyedVectors.load_word2vec_format('glove_s100.txt')\n",
    "\n",
    "def vetor_frase_glove(tokens):\n",
    "    vecs = [glove[w] for w in tokens if w in glove]\n",
    "    return np.mean(vecs, axis=0) if vecs else np.zeros(glove.vector_size)\n",
    "\n",
    "def prep_dados(dados, coluna):\n",
    "    return np.vstack([vetor_frase_glove(tokens) for tokens in dados[coluna]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e22a43f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove = prep_dados(train, \"stems\")\n",
    "X_val_glove = prep_dados(val, \"stems\")\n",
    "X_test_glove = prep_dados(test, \"stems\")\n",
    "\n",
    "X_train_glove_oversampling = prep_dados(train_oversampling, \"stems\")\n",
    "X_val_glove_oversampling = prep_dados(val_oversampling, \"stems\")\n",
    "X_test_glove_oversampling = prep_dados(test_oversampling, \"stems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72beca9e",
   "metadata": {},
   "source": [
    "## WORD2VEC -anaana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188d7c7d",
   "metadata": {},
   "source": [
    "fontes : \n",
    "-   https://sites.google.com/view/nilc-usp/resources-and-tools?authuser=0\n",
    "-   https://huggingface.co/nilc-nlp/word2vec-skip-gram-300d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bade9176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_word2vec():\n",
    "    path = hf_hub_download(repo_id=\"nilc-nlp/word2vec-skip-gram-300d\",\n",
    "                           filename=\"embeddings.safetensors\")\n",
    "\n",
    "    data = load_file(path)\n",
    "    vectors = data[\"embeddings\"]\n",
    "\n",
    "    vocab_path = hf_hub_download(repo_id=\"nilc-nlp/word2vec-skip-gram-300d\",\n",
    "                                 filename=\"vocab.txt\")\n",
    "    with open(vocab_path) as f:\n",
    "        vocab = [w.strip() for w in f]\n",
    "\n",
    "    print(vectors.shape)\n",
    "\n",
    "    model = KeyedVectors(vector_size=vectors.shape[1])\n",
    "    model.add_vectors(vocab, vectors)\n",
    "    model.fill_norms()\n",
    "\n",
    "    return model\n",
    "\n",
    "def vetor_frase(model, frase):\n",
    "    vectors = [model[w] for w in frase if w in model]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "def word2vec(train, val, test, coluna, model):\n",
    "    X_train = np.vstack([vetor_frase(model, tokens) for tokens in train[coluna]])\n",
    "    X_val   = np.vstack([vetor_frase(model, tokens) for tokens in val[coluna]])\n",
    "    X_test  = np.vstack([vetor_frase(model, tokens) for tokens in test[coluna]])\n",
    "\n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07c9e729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(929606, 300)\n"
     ]
    }
   ],
   "source": [
    "model = carregar_word2vec()\n",
    "X_train_word2vec, X_val_word2vec, X_test_word2vec = word2vec(train, val, test, 'stems', model)\n",
    "X_train_oversampling_word2vec, X_val_oversampling_word2vec, X_test_oversampling_word2vec = word2vec(train_oversampling, val_oversampling, test_oversampling, 'stems', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44e250c",
   "metadata": {},
   "source": [
    "## WORD2VEC treinado com os nossos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b6e3e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import optuna\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#vetor médio da frase\n",
    "def vetor_frase_w2v(tokens, model):\n",
    "    vecs = [model.wv[w] for w in tokens if w in model.wv]\n",
    "    return np.mean(vecs, axis=0) if vecs else np.zeros(model.vector_size)\n",
    "\n",
    "# matriz dos embeddings\n",
    "def prep_dados_w2v(dados, coluna, model):\n",
    "    return np.vstack([vetor_frase_w2v(tokens, model) for tokens in dados[coluna]])\n",
    "\n",
    "def objective_w2v(trial, train, val):\n",
    "    # Hiperparâmetros \n",
    "    vector_size = trial.suggest_int(\"vector_size\", 50, 300)\n",
    "    window = trial.suggest_int(\"window\", 3, 10)\n",
    "    min_count = trial.suggest_int(\"min_count\", 1, 5)\n",
    "    sg = trial.suggest_categorical(\"sg\", [0, 1])  # 0 = CBOW, 1 = Skip-gram\n",
    "    epochs = trial.suggest_int(\"epochs\", 5, 20)\n",
    "    C = trial.suggest_float(\"C\", 1e-3, 10, log=True)\n",
    "    use_scaler = trial.suggest_categorical(\"use_scaler\", [True, False])\n",
    "\n",
    "    # Treina Word2Vec com os parâmetros sugeridos\n",
    "    sentences = train[\"tokens\"]\n",
    "    model = Word2Vec(\n",
    "        sentences=sentences,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        sg=sg,\n",
    "        workers=4,\n",
    "        epochs=epochs\n",
    "    )\n",
    "\n",
    "    # Transformar dados nos embeddings médios\n",
    "    X_train = prep_dados_w2v(train, \"tokens\", model)\n",
    "    X_val = prep_dados_w2v(val, \"tokens\", model)\n",
    "    y_train = train[\"classe\"].values\n",
    "    y_val = val[\"classe\"].values\n",
    "\n",
    "    if use_scaler:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "\n",
    "    # Modelo de classificação (Logistic Regression)\n",
    "    clf = LogisticRegression(C=C, max_iter=500)\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_val)\n",
    "\n",
    "    return f1_score(y_val, preds, average=\"macro\")\n",
    "\n",
    "def word2vec_vec(train, val, test):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective_w2v(trial, train, val),\n",
    "                   n_trials=30, n_jobs=-1, show_progress_bar=False)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    print(\"Melhores parâmetros:\", best_params)\n",
    "    print(\"Melhor F1:\", study.best_value)\n",
    "\n",
    "    best_model = Word2Vec(\n",
    "        sentences=train[\"tokens\"],\n",
    "        vector_size=best_params[\"vector_size\"],\n",
    "        window=best_params[\"window\"],\n",
    "        min_count=best_params[\"min_count\"],\n",
    "        sg=best_params[\"sg\"],\n",
    "        workers=4,\n",
    "        epochs=best_params[\"epochs\"]\n",
    "    )\n",
    "\n",
    "    # Criar embeddings médios para todos os conjuntos\n",
    "    X_train = prep_dados_w2v(train, \"tokens\", best_model)\n",
    "    X_val = prep_dados_w2v(val, \"tokens\", best_model)\n",
    "    X_test = prep_dados_w2v(test, \"tokens\", best_model)\n",
    "\n",
    "    if best_params[\"use_scaler\"]:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, X_val, X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83eab317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros: {'vector_size': 300, 'window': 10, 'min_count': 2, 'sg': 1, 'epochs': 13, 'C': 0.731370704222962, 'use_scaler': True}\n",
      "Melhor F1: 0.5435659081071725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros: {'vector_size': 248, 'window': 5, 'min_count': 4, 'sg': 1, 'epochs': 6, 'C': 1.7490498564923553, 'use_scaler': True}\n",
      "Melhor F1: 0.6472373543862201\n"
     ]
    }
   ],
   "source": [
    "X_train_w2v_trained, X_val_w2v_trained, X_test_w2v_trained = word2vec_vec(train, val, test)\n",
    "X_train_over_w2v_trained, X_val_over_w2v_trained, X_test_over_w2v_trained= word2vec_vec(train_oversampling, val_oversampling, test_oversampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e6afab",
   "metadata": {},
   "source": [
    "## Combinar CountVectorizer com TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7515fd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "X_train_freq = hstack([X_train_count_vec, X_train_tfidf_vec])\n",
    "X_val_freq   = hstack([X_val_count_vec, X_val_tfidf_vec])\n",
    "X_test_freq  = hstack([X_test_count_vec, X_test_tfidf_vec])\n",
    "\n",
    "X_train_freq_over = hstack([X_train_oversampling_count_vec, X_train_oversampling_tfidf_vec])\n",
    "X_val_freq_over   = hstack([X_val_oversampling_count_vec, X_val_oversampling_tfidf_vec])\n",
    "X_test_freq_over  = hstack([X_test_oversampling_count_vec, X_test_oversampling_tfidf_vec])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955eb2ec",
   "metadata": {},
   "source": [
    "# Aplicar modelos de classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0268c280",
   "metadata": {},
   "source": [
    "## Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacc51ae",
   "metadata": {},
   "source": [
    "-   Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0229cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_naive_MN(trial, X_train, X_val, y_train, y_val):\n",
    "    # Hiperparâmetros a otimizar\n",
    "    alpha = trial.suggest_float(\"alpha\", 1e-3, 2.0, log=True)\n",
    "    \n",
    "    # Modelo\n",
    "    clf = MultinomialNB(alpha=alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_val_pred = clf.predict(X_val)\n",
    "    f1 = f1_score(y_val, y_val_pred, average='weighted')  \n",
    "    \n",
    "    return f1\n",
    "\n",
    "def Naive_Bayes_MN(X_train, X_val, X_test, y_train, y_val, n_trials=30):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective_naive_MN(trial, X_train, X_val, y_train, y_val),\n",
    "                   n_trials=n_trials, n_jobs=-1, show_progress_bar=False)\n",
    "    \n",
    "    print(\"Melhores hiperparâmetros encontrados:\")\n",
    "    print(study.best_params)\n",
    "    print(f\"Melhor F1 (validação): {study.best_value:.4f}\")\n",
    "    \n",
    "    # Treinar o modelo final com o melhor alpha\n",
    "    best_model = MultinomialNB(**study.best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Previsão no conjunto de teste\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    \n",
    "    return y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "222c6a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def objective_naive_GS(trial, X_train, X_val, y_train, y_val):\n",
    "    # Definir o hiperparâmetro a otimizar\n",
    "    var_smoothing = trial.suggest_float(\"var_smoothing\", 1e-12, 1e-3, log=True)\n",
    "    # Criar o modelo\n",
    "    model = GaussianNB(var_smoothing=var_smoothing)\n",
    "    # Treinar\n",
    "    model.fit(X_train, y_train)\n",
    "    # Avaliar no conjunto de validação\n",
    "    y_pred = model.predict(X_val)\n",
    "    f1 = f1_score(y_val, y_pred, average=\"macro\")\n",
    "\n",
    "    return f1\n",
    "\n",
    "def naive_bayes_gs(X_train, X_val, X_test, y_train, y_val, n_trials=30):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective_naive_GS(trial, X_train, X_val, y_train, y_val),\n",
    "                   n_trials=n_trials, n_jobs=-1, show_progress_bar=False)\n",
    "    print(\"Melhores hiperparâmetros encontrados:\")\n",
    "    print(study.best_params)\n",
    "    # Treinar o melhor modelo com os melhores parâmetros\n",
    "    best_model = GaussianNB(**study.best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    # Avaliar no conjunto de teste\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "    return y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df142687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----COUNT VECTORIZER ----\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'alpha': 0.6525842312042156}\n",
      "Melhor F1 (validação): 0.8048\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.72      0.78      0.75       417\n",
      "           0       0.86      0.82      0.84      1273\n",
      "           1       0.31      0.35      0.33       168\n",
      "\n",
      "    accuracy                           0.77      1858\n",
      "   macro avg       0.63      0.65      0.64      1858\n",
      "weighted avg       0.78      0.77      0.77      1858\n",
      "\n",
      "[[ 324   79   14]\n",
      " [ 111 1049  113]\n",
      " [  15   95   58]]\n",
      "\n",
      "com oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'alpha': 1.9022469287980377}\n",
      "Melhor F1 (validação): 0.6949\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.77      0.80      0.78      1273\n",
      "           0       0.56      0.77      0.65      1273\n",
      "           1       0.72      0.42      0.54      1273\n",
      "\n",
      "    accuracy                           0.67      3819\n",
      "   macro avg       0.68      0.67      0.66      3819\n",
      "weighted avg       0.68      0.67      0.66      3819\n",
      "\n",
      "[[1017  198   58]\n",
      " [ 140  983  150]\n",
      " [ 170  562  541]]\n"
     ]
    }
   ],
   "source": [
    "# Count Vectorizer\n",
    "print (\"----COUNT VECTORIZER ----\")\n",
    "count_vec_predicted_NB = Naive_Bayes_MN(X_train_count_vec,X_val_count_vec,X_test_count_vec,y_train,y_val)\n",
    "print(metrics.classification_report(y_test, count_vec_predicted_NB))\n",
    "print(metrics.confusion_matrix(y_test, count_vec_predicted_NB))\n",
    "\n",
    "print(\"\\ncom oversampling\")\n",
    "count_vec_over_predicted_NB = Naive_Bayes_MN(X_train_oversampling_count_vec,X_val_oversampling_count_vec,X_test_oversampling_count_vec,y_train_oversampling,y_val_oversampling)\n",
    "print(metrics.classification_report(y_test_oversampling, count_vec_over_predicted_NB))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, count_vec_over_predicted_NB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10c2c5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----TF-IDF ----\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'alpha': 0.05111554759693943}\n",
      "Melhor F1 (validação): 0.7583\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.80      0.54      0.65       417\n",
      "           0       0.78      0.96      0.86      1273\n",
      "           1       0.70      0.04      0.08       168\n",
      "\n",
      "    accuracy                           0.79      1858\n",
      "   macro avg       0.76      0.52      0.53      1858\n",
      "weighted avg       0.78      0.79      0.74      1858\n",
      "\n",
      "[[ 227  189    1]\n",
      " [  45 1226    2]\n",
      " [  10  151    7]]\n",
      "\n",
      " com oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'alpha': 0.33377206020038025}\n",
      "Melhor F1 (validação): 0.6925\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.74      0.74      0.74      1273\n",
      "           0       0.52      0.78      0.62      1273\n",
      "           1       0.65      0.32      0.43      1273\n",
      "\n",
      "    accuracy                           0.62      3819\n",
      "   macro avg       0.64      0.62      0.60      3819\n",
      "weighted avg       0.64      0.62      0.60      3819\n",
      "\n",
      "[[947 246  80]\n",
      " [145 991 137]\n",
      " [189 673 411]]\n"
     ]
    }
   ],
   "source": [
    "print (\"----TF-IDF ----\")\n",
    "tfidf_vec_predicted_NB= Naive_Bayes_MN(X_train_tfidf_vec, X_val_tfidf_vec, X_test_tfidf_vec, y_train, y_val) \n",
    "print(metrics.classification_report(y_test, tfidf_vec_predicted_NB))\n",
    "print(metrics.confusion_matrix(y_test, tfidf_vec_predicted_NB))\n",
    "\n",
    "print(\"\\n com oversampling\")\n",
    "tfidf_vec_oversampling_predicted_NB= Naive_Bayes_MN(X_train_oversampling_tfidf_vec, X_val_oversampling_tfidf_vec, X_test_oversampling_tfidf_vec, y_train_oversampling, y_val_oversampling) \n",
    "print(metrics.classification_report(y_test_oversampling, tfidf_vec_oversampling_predicted_NB))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, tfidf_vec_oversampling_predicted_NB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d2c9102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count vec + tf idf\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'alpha': 0.8027533154982965}\n",
      "Melhor F1 (validação): 0.8054\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.69      0.79      0.74       417\n",
      "           0       0.86      0.80      0.83      1273\n",
      "           1       0.28      0.33      0.31       168\n",
      "\n",
      "    accuracy                           0.75      1858\n",
      "   macro avg       0.61      0.64      0.62      1858\n",
      "weighted avg       0.77      0.75      0.76      1858\n",
      "\n",
      "[[ 329   74   14]\n",
      " [ 128 1017  128]\n",
      " [  19   93   56]]\n",
      "oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'alpha': 1.7579063716823469}\n",
      "Melhor F1 (validação): 0.6951\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.75      0.79      0.77      1273\n",
      "           0       0.55      0.78      0.65      1273\n",
      "           1       0.71      0.38      0.49      1273\n",
      "\n",
      "    accuracy                           0.65      3819\n",
      "   macro avg       0.67      0.65      0.64      3819\n",
      "weighted avg       0.67      0.65      0.64      3819\n",
      "\n",
      "[[1010  209   54]\n",
      " [ 138  995  140]\n",
      " [ 196  599  478]]\n"
     ]
    }
   ],
   "source": [
    "print(\"count vec + tf idf\")\n",
    "freq_predicted_NB= Naive_Bayes_MN(X_train_freq, X_val_freq, X_test_freq, y_train, y_val, n_trials=30) \n",
    "print(metrics.classification_report(y_test,freq_predicted_NB ))\n",
    "print(metrics.confusion_matrix(y_test, freq_predicted_NB))\n",
    "\n",
    "print(\"oversampling\")\n",
    "freq_predicted_NB_over= Naive_Bayes_MN(X_train_freq_over, X_val_freq_over, X_test_freq_over, y_train_oversampling, y_val_oversampling, n_trials=30) \n",
    "print(metrics.classification_report(y_test_oversampling,freq_predicted_NB_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, freq_predicted_NB_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c766d86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----WORD2VEC ----\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'var_smoothing': 1.1263207666310125e-11}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.44      0.42      0.43       417\n",
      "           0       0.84      0.32      0.47      1273\n",
      "           1       0.11      0.62      0.18       168\n",
      "\n",
      "    accuracy                           0.37      1858\n",
      "   macro avg       0.46      0.46      0.36      1858\n",
      "weighted avg       0.68      0.37      0.43      1858\n",
      "\n",
      "[[175  44 198]\n",
      " [196 410 667]\n",
      " [ 30  33 105]]\n",
      "oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'var_smoothing': 1.0523527258838917e-10}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.57      0.44      0.50      1273\n",
      "           0       0.57      0.27      0.36      1273\n",
      "           1       0.39      0.68      0.50      1273\n",
      "\n",
      "    accuracy                           0.46      3819\n",
      "   macro avg       0.51      0.46      0.45      3819\n",
      "weighted avg       0.51      0.46      0.45      3819\n",
      "\n",
      "[[561  89 623]\n",
      " [197 343 733]\n",
      " [230 175 868]]\n"
     ]
    }
   ],
   "source": [
    "#Word2vec\n",
    "print (\"----WORD2VEC ----\")\n",
    "word2vec_predicted_NB= naive_bayes_gs(X_train_word2vec, X_val_word2vec, X_test_word2vec, y_train, y_val, n_trials=30) \n",
    "print(metrics.classification_report(y_test, word2vec_predicted_NB))\n",
    "print(metrics.confusion_matrix(y_test, word2vec_predicted_NB))\n",
    "\n",
    "print (\"oversampling\")\n",
    "word2vec_over_predicted_NB= naive_bayes_gs(X_train_oversampling_word2vec, X_val_oversampling_word2vec, X_test_oversampling_word2vec, y_train_oversampling, y_val_oversampling, n_trials=30) \n",
    "print(metrics.classification_report(y_test_oversampling, word2vec_over_predicted_NB))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, word2vec_over_predicted_NB))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d45c15f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores hiperparâmetros encontrados:\n",
      "{'var_smoothing': 1.2301068833012321e-12}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.64      0.62      0.63       417\n",
      "           0       0.88      0.43      0.58      1273\n",
      "           1       0.14      0.69      0.23       168\n",
      "\n",
      "    accuracy                           0.50      1858\n",
      "   macro avg       0.55      0.58      0.48      1858\n",
      "weighted avg       0.76      0.50      0.56      1858\n",
      "\n",
      "[[260  45 112]\n",
      " [126 552 595]\n",
      " [ 21  31 116]]\n",
      "oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'var_smoothing': 2.628842682605908e-12}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.56      0.76      0.65      1273\n",
      "           0       0.50      0.63      0.56      1273\n",
      "           1       0.45      0.18      0.26      1273\n",
      "\n",
      "    accuracy                           0.52      3819\n",
      "   macro avg       0.51      0.52      0.49      3819\n",
      "weighted avg       0.51      0.52      0.49      3819\n",
      "\n",
      "[[968 181 124]\n",
      " [325 800 148]\n",
      " [429 617 227]]\n"
     ]
    }
   ],
   "source": [
    "#word2vec treinado com os nossos dados\n",
    "word2vec_trained_predicted_NB= naive_bayes_gs(X_train_w2v_trained, X_val_w2v_trained, X_test_w2v_trained, y_train, y_val, n_trials=30) \n",
    "print(metrics.classification_report(y_test, word2vec_trained_predicted_NB))\n",
    "print(metrics.confusion_matrix(y_test, word2vec_trained_predicted_NB))\n",
    "\n",
    "print(\"oversampling\")\n",
    "word2vec_oversampling_trained_predicted_NB= naive_bayes_gs(X_train_over_w2v_trained, X_val_over_w2v_trained, X_test_over_w2v_trained, y_train_oversampling, y_val_oversampling, n_trials=30) \n",
    "print(metrics.classification_report(y_test_oversampling, word2vec_oversampling_trained_predicted_NB))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, word2vec_oversampling_trained_predicted_NB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d8da10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----GLOVE ----\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'var_smoothing': 0.0004661108003621178}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.41      0.36      0.38       417\n",
      "           0       0.79      0.41      0.54      1273\n",
      "           1       0.12      0.59      0.20       168\n",
      "\n",
      "    accuracy                           0.41      1858\n",
      "   macro avg       0.44      0.45      0.37      1858\n",
      "weighted avg       0.64      0.41      0.47      1858\n",
      "\n",
      "[[152  98 167]\n",
      " [192 519 562]\n",
      " [ 29  40  99]]\n",
      "c/oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'var_smoothing': 1.4841405035508754e-09}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.55      0.39      0.46      1273\n",
      "           0       0.49      0.30      0.37      1273\n",
      "           1       0.38      0.64      0.47      1273\n",
      "\n",
      "    accuracy                           0.44      3819\n",
      "   macro avg       0.47      0.44      0.43      3819\n",
      "weighted avg       0.47      0.44      0.43      3819\n",
      "\n",
      "[[493 153 627]\n",
      " [187 384 702]\n",
      " [213 251 809]]\n"
     ]
    }
   ],
   "source": [
    "#Glove\n",
    "print (\"----GLOVE ----\")\n",
    "glove_predicted_NB= naive_bayes_gs(X_train_glove, X_val_glove, X_test_glove, y_train, y_val, n_trials=30) \n",
    "print(metrics.classification_report(y_test, glove_predicted_NB))\n",
    "print(metrics.confusion_matrix(y_test, glove_predicted_NB))\n",
    "\n",
    "print (\"c/oversampling\")\n",
    "glove_predicted_NB_over= naive_bayes_gs(X_train_glove_oversampling, X_val_glove_oversampling, X_test_glove_oversampling, y_train_oversampling, y_val_oversampling, n_trials=30) \n",
    "print(metrics.classification_report(y_test_oversampling, glove_predicted_NB_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, glove_predicted_NB_over))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db44bdd3",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab39b613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X_train, X_val, y_train, y_val):\n",
    "\n",
    "    #Definir hiperparâmetros a testar \n",
    "    C = trial.suggest_float('C', 0.01, 10.0, log=True)\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly'])\n",
    "    gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "    degree = trial.suggest_int('degree', 2, 5) if kernel == 'poly' else 3\n",
    "\n",
    "    #Criar modelo SVM \n",
    "    model = SVC(C=C, kernel=kernel, gamma=gamma, degree=degree)\n",
    "    model = SVC(C=C, kernel=kernel, gamma=gamma, degree=degree)\n",
    "\n",
    "    #treinar modelo com conjunto de treino\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    #prever no conjunto de validação\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    #calcular f1\n",
    "    f1 = f1_score(y_val, y_pred, average='macro')\n",
    "\n",
    "    return f1\n",
    "\n",
    "def svm(X_train, X_val, X_test, y_train, y_val, n_trials=30):\n",
    "    # Chamar o Optuna para otimizar os parâmetros\n",
    "    objeto_para_otimizar = optuna.create_study(direction='maximize')\n",
    "    objeto_para_otimizar.optimize(lambda trial: objective(trial, X_train, X_val, y_train, y_val),\n",
    "                   n_trials=n_trials, n_jobs=-1, show_progress_bar=False )\n",
    "\n",
    "    print(\"Melhores hiperparâmetros encontrados:\")\n",
    "    print(objeto_para_otimizar.best_params)\n",
    "\n",
    "    # Treinar o melhor modelo\n",
    "    best_model = SVC(**objeto_para_otimizar.best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    #  Avaliar no conjunto de teste\n",
    "    y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "    return y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8e61fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----COUNT VECTORIZER ----\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'C': 0.8704277091071306, 'kernel': 'linear', 'gamma': 'scale'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.72      0.71      0.72       417\n",
      "           0       0.83      0.85      0.84      1273\n",
      "           1       0.24      0.20      0.22       168\n",
      "\n",
      "    accuracy                           0.76      1858\n",
      "   macro avg       0.60      0.59      0.59      1858\n",
      "weighted avg       0.75      0.76      0.76      1858\n",
      "\n",
      "[[ 297  104   16]\n",
      " [  99 1087   87]\n",
      " [  14  121   33]]\n",
      "\n",
      "com oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'C': 0.038015534193093364, 'kernel': 'linear', 'gamma': 'auto'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.84      0.73      0.78      1273\n",
      "           0       0.51      0.83      0.63      1273\n",
      "           1       0.71      0.35      0.47      1273\n",
      "\n",
      "    accuracy                           0.64      3819\n",
      "   macro avg       0.69      0.64      0.63      3819\n",
      "weighted avg       0.69      0.64      0.63      3819\n",
      "\n",
      "[[ 934  271   68]\n",
      " [ 100 1060  113]\n",
      " [  78  752  443]]\n"
     ]
    }
   ],
   "source": [
    "print (\"----COUNT VECTORIZER ----\")\n",
    "count_vec_predicted_SVM= svm(X_train_count_vec, X_val_count_vec, X_test_count_vec, y_train, y_val, n_trials=30) \n",
    "print(metrics.classification_report(\n",
    "y_test, count_vec_predicted_SVM))\n",
    "print(metrics.confusion_matrix(y_test, count_vec_predicted_SVM))\n",
    "\n",
    "print(\"\\ncom oversampling\")\n",
    "count_vec_over_predicted_SVM = svm(X_train_oversampling_count_vec, X_val_oversampling_count_vec, X_test_oversampling_count_vec, y_train_oversampling, y_val_oversampling, n_trials=30)\n",
    "print(metrics.classification_report(y_test_oversampling, count_vec_over_predicted_SVM))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, count_vec_over_predicted_SVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e5930db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- TF-IDF ----\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'C': 3.5727520241153576, 'kernel': 'linear', 'gamma': 'scale'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.73      0.67      0.70       417\n",
      "           0       0.81      0.89      0.85      1273\n",
      "           1       0.29      0.14      0.19       168\n",
      "\n",
      "    accuracy                           0.77      1858\n",
      "   macro avg       0.61      0.57      0.58      1858\n",
      "weighted avg       0.75      0.77      0.76      1858\n",
      "\n",
      "[[ 278  130    9]\n",
      " [  88 1135   50]\n",
      " [  15  129   24]]\n",
      "---- TF-IDF ----\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'C': 0.18205863238046194, 'kernel': 'linear', 'gamma': 'scale'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.84      0.72      0.77      1273\n",
      "           0       0.54      0.78      0.63      1273\n",
      "           1       0.66      0.46      0.54      1273\n",
      "\n",
      "    accuracy                           0.65      3819\n",
      "   macro avg       0.68      0.65      0.65      3819\n",
      "weighted avg       0.68      0.65      0.65      3819\n",
      "\n",
      "[[911 243 119]\n",
      " [ 97 989 187]\n",
      " [ 71 613 589]]\n"
     ]
    }
   ],
   "source": [
    "print (\"---- TF-IDF ----\")\n",
    "tfidf_vec_predicted_SVM= svm(X_train_tfidf_vec, X_val_tfidf_vec, X_test_tfidf_vec, y_train, y_val, n_trials=30) \n",
    "print(metrics.classification_report(y_test, tfidf_vec_predicted_SVM))\n",
    "print(metrics.confusion_matrix(y_test, tfidf_vec_predicted_SVM))\n",
    "\n",
    "print (\"---- TF-IDF ----\")\n",
    "tfidf_vec_predicted_SVM_oversampling= svm(X_train_oversampling_tfidf_vec, X_val_oversampling_tfidf_vec, X_test_oversampling_tfidf_vec, y_train_oversampling, y_val_oversampling, n_trials=30) \n",
    "print(metrics.classification_report(y_test_oversampling, tfidf_vec_predicted_SVM_oversampling))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, tfidf_vec_predicted_SVM_oversampling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d469dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count vec + tf idf\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'C': 0.4770284146530357, 'kernel': 'linear', 'gamma': 'auto'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.76      0.70      0.73       417\n",
      "           0       0.82      0.88      0.85      1273\n",
      "           1       0.25      0.17      0.20       168\n",
      "\n",
      "    accuracy                           0.78      1858\n",
      "   macro avg       0.61      0.58      0.60      1858\n",
      "weighted avg       0.76      0.78      0.77      1858\n",
      "\n",
      "[[ 293  113   11]\n",
      " [  78 1118   77]\n",
      " [  13  126   29]]\n",
      "oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'C': 0.05811348382378595, 'kernel': 'linear', 'gamma': 'auto'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.84      0.74      0.79      1273\n",
      "           0       0.50      0.85      0.63      1273\n",
      "           1       0.72      0.31      0.44      1273\n",
      "\n",
      "    accuracy                           0.63      3819\n",
      "   macro avg       0.69      0.63      0.62      3819\n",
      "weighted avg       0.69      0.63      0.62      3819\n",
      "\n",
      "[[ 945  275   53]\n",
      " [  93 1081   99]\n",
      " [  84  790  399]]\n"
     ]
    }
   ],
   "source": [
    "print(\"count vec + tf idf\")\n",
    "freq_predicted_svm= svm(X_train_freq, X_val_freq, X_test_freq, y_train, y_val, n_trials=30) \n",
    "print(metrics.classification_report(y_test,freq_predicted_svm))\n",
    "print(metrics.confusion_matrix(y_test, freq_predicted_svm))\n",
    "\n",
    "print(\"oversampling\")\n",
    "freq_predicted_svm_over= svm(X_train_freq_over, X_val_freq_over, X_test_freq_over, y_train_oversampling, y_val_oversampling, n_trials=30) \n",
    "print(metrics.classification_report(y_test_oversampling,freq_predicted_svm_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, freq_predicted_svm_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9f769d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----WORD2VEC ----\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'C': 6.889905900465077, 'kernel': 'poly', 'gamma': 'scale', 'degree': 2}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.62      0.40      0.49       417\n",
      "           0       0.75      0.92      0.83      1273\n",
      "           1       0.22      0.05      0.08       168\n",
      "\n",
      "    accuracy                           0.72      1858\n",
      "   macro avg       0.53      0.46      0.46      1858\n",
      "weighted avg       0.67      0.72      0.68      1858\n",
      "\n",
      "[[ 167  240   10]\n",
      " [  85 1169   19]\n",
      " [  19  141    8]]\n",
      "C/ oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'C': 0.6115086154857086, 'kernel': 'poly', 'gamma': 'scale', 'degree': 2}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.63      0.57      0.59      1273\n",
      "           0       0.47      0.72      0.57      1273\n",
      "           1       0.55      0.31      0.40      1273\n",
      "\n",
      "    accuracy                           0.53      3819\n",
      "   macro avg       0.55      0.53      0.52      3819\n",
      "weighted avg       0.55      0.53      0.52      3819\n",
      "\n",
      "[[720 400 153]\n",
      " [176 919 178]\n",
      " [254 620 399]]\n"
     ]
    }
   ],
   "source": [
    "print (\"----WORD2VEC ----\")\n",
    "word2vec_predicted_SVM= svm(X_train_word2vec, X_val_word2vec, X_test_word2vec, y_train, y_val, n_trials=30) \n",
    "print(metrics.classification_report(y_test, word2vec_predicted_SVM))\n",
    "print(metrics.confusion_matrix(y_test, word2vec_predicted_SVM))\n",
    "\n",
    "print (\"C/ oversampling\")\n",
    "word2vec_predicted_SVM_oversampling= svm(X_train_oversampling_word2vec, X_val_oversampling_word2vec, X_test_oversampling_word2vec, y_train_oversampling, y_val_oversampling, n_trials=30) \n",
    "print(metrics.classification_report(y_test_oversampling, word2vec_predicted_SVM_oversampling))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, word2vec_predicted_SVM_oversampling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13e589a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores hiperparâmetros encontrados:\n",
      "{'C': 0.3571064836427892, 'kernel': 'linear', 'gamma': 'scale'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.75      0.67      0.71       417\n",
      "           0       0.81      0.94      0.87      1273\n",
      "           1       0.00      0.00      0.00       168\n",
      "\n",
      "    accuracy                           0.79      1858\n",
      "   macro avg       0.52      0.54      0.53      1858\n",
      "weighted avg       0.72      0.79      0.75      1858\n",
      "\n",
      "[[ 278  139    0]\n",
      " [  74 1199    0]\n",
      " [  17  151    0]]\n",
      "oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'C': 0.23477370582968596, 'kernel': 'linear', 'gamma': 'auto'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.76      0.73      0.75      1273\n",
      "           0       0.50      0.74      0.60      1273\n",
      "           1       0.66      0.37      0.47      1273\n",
      "\n",
      "    accuracy                           0.61      3819\n",
      "   macro avg       0.64      0.61      0.60      3819\n",
      "weighted avg       0.64      0.61      0.60      3819\n",
      "\n",
      "[[935 297  41]\n",
      " [138 941 194]\n",
      " [157 651 465]]\n"
     ]
    }
   ],
   "source": [
    "#word2vec treinado com os nossos dados\n",
    "word2vec_trained_predicted_svm= svm(X_train_w2v_trained, X_val_w2v_trained, X_test_w2v_trained, y_train, y_val, n_trials=30) \n",
    "print(metrics.classification_report(y_test, word2vec_trained_predicted_svm))\n",
    "print(metrics.confusion_matrix(y_test, word2vec_trained_predicted_svm))\n",
    "\n",
    "print(\"oversampling\")\n",
    "word2vec_trained_predicted_svm_over= svm(X_train_over_w2v_trained, X_val_over_w2v_trained, X_test_over_w2v_trained, y_train_oversampling, y_val_oversampling, n_trials=30) \n",
    "print(metrics.classification_report(y_test_oversampling, word2vec_trained_predicted_svm_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, word2vec_trained_predicted_svm_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "01f1488d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----GLOVE ----\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'C': 3.5379295458946056, 'kernel': 'linear', 'gamma': 'auto'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.66      0.20      0.31       417\n",
      "           0       0.72      0.97      0.82      1273\n",
      "           1       0.00      0.00      0.00       168\n",
      "\n",
      "    accuracy                           0.71      1858\n",
      "   macro avg       0.46      0.39      0.38      1858\n",
      "weighted avg       0.64      0.71      0.63      1858\n",
      "\n",
      "[[  84  333    0]\n",
      " [  35 1238    0]\n",
      " [   8  160    0]]\n",
      "c/oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'C': 0.4299252422906833, 'kernel': 'rbf', 'gamma': 'scale'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.60      0.50      0.55      1273\n",
      "           0       0.50      0.57      0.53      1273\n",
      "           1       0.48      0.49      0.49      1273\n",
      "\n",
      "    accuracy                           0.52      3819\n",
      "   macro avg       0.53      0.52      0.52      3819\n",
      "weighted avg       0.53      0.52      0.52      3819\n",
      "\n",
      "[[638 307 328]\n",
      " [199 727 347]\n",
      " [223 423 627]]\n"
     ]
    }
   ],
   "source": [
    "print (\"----GLOVE ----\")\n",
    "glove_predicted_SVM= svm(X_train_glove, X_val_glove, X_test_glove, y_train, y_val, n_trials=30) \n",
    "print(metrics.classification_report(y_test, glove_predicted_SVM))\n",
    "print(metrics.confusion_matrix(y_test, glove_predicted_SVM))\n",
    "\n",
    "print (\"c/oversampling\")\n",
    "glove_predicted_SVM_over= svm(X_train_glove_oversampling, X_val_glove_oversampling, X_test_glove_oversampling, y_train_oversampling, y_val_oversampling, n_trials=30) \n",
    "print(metrics.classification_report(y_test_oversampling, glove_predicted_SVM_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, glove_predicted_SVM_over))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6791a4e1",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "21c7661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def KNN(X_train, y_train, X_val, y_val, X_test):\n",
    "    k_values = range(1, 200,10)  # Determinar  intervalo de valores a testar para k\n",
    "    best_k = 1\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    k_list = []\n",
    "    accuracy_list = [] \n",
    "\n",
    "    for k in k_values:\n",
    "\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)  \n",
    "        knn.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_KNN = knn.predict(X_val) \n",
    "        accuracy = accuracy_score(y_val, y_pred_KNN)  \n",
    "\n",
    "        k_list.append(k)\n",
    "        accuracy_list.append(accuracy)  \n",
    "\n",
    "        if accuracy > best_accuracy: \n",
    "            best_accuracy = accuracy\n",
    "            best_k = k\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=best_k)  \n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_KNN= knn.predict(X_test) \n",
    "    print(f\"Best k: {best_k}\")\n",
    "\n",
    "    return y_pred_KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6fead510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----COUNT VECTORIZER ----\n",
      "Best k: 11\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.76      0.64      0.69       417\n",
      "           0       0.79      0.94      0.86      1273\n",
      "           1       0.00      0.00      0.00       168\n",
      "\n",
      "    accuracy                           0.79      1858\n",
      "   macro avg       0.52      0.53      0.52      1858\n",
      "weighted avg       0.71      0.79      0.74      1858\n",
      "\n",
      "[[ 266  151    0]\n",
      " [  78 1194    1]\n",
      " [   8  160    0]]\n",
      "c/ oversampling\n",
      "Best k: 11\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.51      0.88      0.65      1273\n",
      "           0       0.51      0.56      0.53      1273\n",
      "           1       0.52      0.10      0.17      1273\n",
      "\n",
      "    accuracy                           0.51      3819\n",
      "   macro avg       0.51      0.51      0.45      3819\n",
      "weighted avg       0.51      0.51      0.45      3819\n",
      "\n",
      "[[1115  134   24]\n",
      " [ 471  708   94]\n",
      " [ 587  557  129]]\n"
     ]
    }
   ],
   "source": [
    "# Count Vectorizer\n",
    "print (\"----COUNT VECTORIZER ----\")\n",
    "count_vec_predicted_KNN= KNN(X_train_count_vec, y_train, X_val_count_vec, y_val, X_test_count_vec) \n",
    "print(metrics.classification_report(y_test, count_vec_predicted_KNN))\n",
    "print(metrics.confusion_matrix(y_test, count_vec_predicted_KNN))\n",
    "\n",
    "print (\"c/ oversampling\")\n",
    "count_vec_predicted_KNN_oversampling= KNN(X_train_oversampling_count_vec, y_train_oversampling, X_val_oversampling_count_vec, y_val_oversampling, X_test_oversampling_count_vec) \n",
    "print(metrics.classification_report(y_test_oversampling, count_vec_predicted_KNN_oversampling))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, count_vec_predicted_KNN_oversampling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cef2dc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----TF-IDF ----\n",
      "Best k: 51\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.77      0.62      0.69       417\n",
      "           0       0.79      0.95      0.86      1273\n",
      "           1       0.00      0.00      0.00       168\n",
      "\n",
      "    accuracy                           0.79      1858\n",
      "   macro avg       0.52      0.52      0.52      1858\n",
      "weighted avg       0.71      0.79      0.74      1858\n",
      "\n",
      "[[ 259  158    0]\n",
      " [  70 1203    0]\n",
      " [   9  159    0]]\n",
      "c/oversampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k: 191\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.74      0.81      0.77      1273\n",
      "           0       0.54      0.79      0.64      1273\n",
      "           1       0.72      0.31      0.43      1273\n",
      "\n",
      "    accuracy                           0.64      3819\n",
      "   macro avg       0.66      0.64      0.62      3819\n",
      "weighted avg       0.66      0.64      0.62      3819\n",
      "\n",
      "[[1034  168   71]\n",
      " [ 178 1009   86]\n",
      " [ 190  687  396]]\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF\n",
    "print (\"----TF-IDF ----\")\n",
    "tfidf_vec_predicted_KNN= KNN(X_train_tfidf_vec, y_train, X_val_tfidf_vec, y_val, X_test_tfidf_vec) \n",
    "print(metrics.classification_report(y_test, tfidf_vec_predicted_KNN))\n",
    "print(metrics.confusion_matrix(y_test, tfidf_vec_predicted_KNN))\n",
    "\n",
    "print (\"c/oversampling\")\n",
    "tfidf_vec_predicted_KNN_oversampling= KNN(X_train_oversampling_tfidf_vec, y_train_oversampling, X_val_oversampling_tfidf_vec, y_val_oversampling, X_test_oversampling_tfidf_vec) \n",
    "print(metrics.classification_report(y_test_oversampling, tfidf_vec_predicted_KNN_oversampling))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, tfidf_vec_predicted_KNN_oversampling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b7123e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count vec + tf idf\n",
      "Best k: 41\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.75      0.61      0.67       417\n",
      "           0       0.79      0.94      0.86      1273\n",
      "           1       0.00      0.00      0.00       168\n",
      "\n",
      "    accuracy                           0.78      1858\n",
      "   macro avg       0.51      0.52      0.51      1858\n",
      "weighted avg       0.71      0.78      0.74      1858\n",
      "\n",
      "[[ 255  162    0]\n",
      " [  78 1195    0]\n",
      " [   9  159    0]]\n",
      "oversampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k: 21\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.59      0.77      0.67      1273\n",
      "           0       0.45      0.70      0.55      1273\n",
      "           1       0.49      0.08      0.14      1273\n",
      "\n",
      "    accuracy                           0.51      3819\n",
      "   macro avg       0.51      0.51      0.45      3819\n",
      "weighted avg       0.51      0.51      0.45      3819\n",
      "\n",
      "[[977 267  29]\n",
      " [311 886  76]\n",
      " [355 816 102]]\n"
     ]
    }
   ],
   "source": [
    "print(\"count vec + tf idf\")\n",
    "freq_predicted_KNN= KNN(X_train_freq, y_train,X_val_freq,y_val, X_test_freq) \n",
    "print(metrics.classification_report(y_test,freq_predicted_KNN ))\n",
    "print(metrics.confusion_matrix(y_test, freq_predicted_KNN))\n",
    "\n",
    "print(\"oversampling\")\n",
    "freq_predicted_KNN_over= KNN(X_train_freq_over, y_train_oversampling, X_val_freq_over, y_val_oversampling, X_test_freq_over) \n",
    "print(metrics.classification_report(y_test_oversampling,freq_predicted_KNN_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, freq_predicted_KNN_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c4c3c7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----WORD2VEC ----\n",
      "Best k: 11\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.62      0.27      0.37       417\n",
      "           0       0.73      0.95      0.82      1273\n",
      "           1       0.20      0.01      0.02       168\n",
      "\n",
      "    accuracy                           0.71      1858\n",
      "   macro avg       0.51      0.41      0.41      1858\n",
      "weighted avg       0.65      0.71      0.65      1858\n",
      "\n",
      "[[ 112  304    1]\n",
      " [  56 1210    7]\n",
      " [  13  153    2]]\n",
      "c/oversampling\n",
      "Best k: 31\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.54      0.47      0.50      1273\n",
      "           0       0.48      0.39      0.43      1273\n",
      "           1       0.41      0.53      0.46      1273\n",
      "\n",
      "    accuracy                           0.46      3819\n",
      "   macro avg       0.47      0.46      0.46      3819\n",
      "weighted avg       0.47      0.46      0.46      3819\n",
      "\n",
      "[[597 202 474]\n",
      " [252 496 525]\n",
      " [258 335 680]]\n"
     ]
    }
   ],
   "source": [
    "#Word2vec\n",
    "print (\"----WORD2VEC ----\")\n",
    "word2vec_predicted_KNN= KNN(X_train_word2vec, y_train, X_val_word2vec, y_val, X_test_word2vec) \n",
    "print(metrics.classification_report(y_test, word2vec_predicted_KNN))\n",
    "print(metrics.confusion_matrix(y_test, word2vec_predicted_KNN))\n",
    "\n",
    "print (\"c/oversampling\")\n",
    "word2vec_predicted_KNN_oversampling= KNN(X_train_oversampling_word2vec, y_train_oversampling, X_val_oversampling_word2vec, y_val_oversampling, X_test_oversampling_word2vec) \n",
    "print(metrics.classification_report(y_test_oversampling, word2vec_predicted_KNN_oversampling))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, word2vec_predicted_KNN_oversampling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9214a6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k: 31\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.73      0.59      0.65       417\n",
      "           0       0.79      0.94      0.86      1273\n",
      "           1       0.67      0.01      0.02       168\n",
      "\n",
      "    accuracy                           0.78      1858\n",
      "   macro avg       0.73      0.51      0.51      1858\n",
      "weighted avg       0.77      0.78      0.74      1858\n",
      "\n",
      "[[ 246  171    0]\n",
      " [  72 1200    1]\n",
      " [  18  148    2]]\n",
      "oversampling\n",
      "Best k: 81\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.69      0.66      0.68      1273\n",
      "           0       0.46      0.79      0.58      1273\n",
      "           1       0.58      0.18      0.27      1273\n",
      "\n",
      "    accuracy                           0.55      3819\n",
      "   macro avg       0.58      0.55      0.51      3819\n",
      "weighted avg       0.58      0.55      0.51      3819\n",
      "\n",
      "[[ 845  359   69]\n",
      " [ 164 1012   97]\n",
      " [ 219  825  229]]\n"
     ]
    }
   ],
   "source": [
    "#word2vec treinado com os nossos dados\n",
    "word2vec_trained_predicted_KNN= KNN(X_train_w2v_trained, y_train, X_val_w2v_trained, y_val, X_test_w2v_trained) \n",
    "print(metrics.classification_report(y_test, word2vec_trained_predicted_KNN))\n",
    "print(metrics.confusion_matrix(y_test, word2vec_trained_predicted_KNN))\n",
    "\n",
    "print(\"oversampling\")\n",
    "word2vec_trained_predicted_KNN_over= KNN(X_train_over_w2v_trained, y_train_oversampling, X_val_over_w2v_trained, y_val_oversampling,X_test_over_w2v_trained) \n",
    "print(metrics.classification_report(y_test_oversampling, word2vec_trained_predicted_KNN_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, word2vec_trained_predicted_KNN_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5e26ff80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----GLOVE ----\n",
      "Best k: 11\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.60      0.27      0.37       417\n",
      "           0       0.73      0.95      0.82      1273\n",
      "           1       0.38      0.02      0.03       168\n",
      "\n",
      "    accuracy                           0.71      1858\n",
      "   macro avg       0.57      0.41      0.41      1858\n",
      "weighted avg       0.67      0.71      0.65      1858\n",
      "\n",
      "[[ 113  303    1]\n",
      " [  63 1206    4]\n",
      " [  13  152    3]]\n",
      "----GLOVE ----\n",
      "Best k: 181\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.57      0.43      0.49      1273\n",
      "           0       0.51      0.37      0.43      1273\n",
      "           1       0.42      0.64      0.51      1273\n",
      "\n",
      "    accuracy                           0.48      3819\n",
      "   macro avg       0.50      0.48      0.48      3819\n",
      "weighted avg       0.50      0.48      0.48      3819\n",
      "\n",
      "[[550 201 522]\n",
      " [203 467 603]\n",
      " [209 249 815]]\n"
     ]
    }
   ],
   "source": [
    "#Glove\n",
    "print (\"----GLOVE ----\")\n",
    "glove_predicted_KNN= KNN(X_train_glove, y_train, X_val_glove, y_val, X_test_glove) \n",
    "print(metrics.classification_report(y_test, glove_predicted_KNN))\n",
    "print(metrics.confusion_matrix(y_test, glove_predicted_KNN))\n",
    "\n",
    "print (\"----GLOVE ----\")\n",
    "glove_predicted_KNN_over= KNN(X_train_glove_oversampling, y_train_oversampling, X_val_glove_oversampling, y_val_oversampling, X_test_glove_oversampling) \n",
    "print(metrics.classification_report(y_test_oversampling, glove_predicted_KNN_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, glove_predicted_KNN_over))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49589df2",
   "metadata": {},
   "source": [
    "## Decison Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d53d6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def objective_decision_tree(trial, X_train, X_val, y_train, y_val):\n",
    "    # Hiperparâmetros a otimizar\n",
    "    criterion = trial.suggest_categorical(\"criterion\", [\"entropy\", \"gini\"])\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 50)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "    ccp_alpha = trial.suggest_float(\"ccp_alpha\", 0.0, 0.01)\n",
    "    \n",
    "    # Criar o modelo com os parâmetros sugeridos\n",
    "    model = DecisionTreeClassifier(\n",
    "        criterion=criterion,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        ccp_alpha=ccp_alpha,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    f1 = f1_score(y_val, y_pred, average=\"macro\")\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "def decision_tree(X_train, y_train, X_val, y_val, X_test,n_trials=30):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective_decision_tree(trial, X_train, X_val, y_train, y_val),\n",
    "                   n_trials=n_trials, n_jobs=-1, show_progress_bar=False)\n",
    "\n",
    "    print(\"Melhores hiperparâmetros encontrados:\")\n",
    "    print(study.best_params)\n",
    "    print(\"Melhor F1 obtido:\", study.best_value)\n",
    "\n",
    "    # Treinar o modelo final com os melhores parâmetros\n",
    "    best_model = DecisionTreeClassifier(**study.best_params, random_state=42)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Prever no conjunto de teste\n",
    "    y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "    return y_pred_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7b899214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----COUNT VECTORIZER ----\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'criterion': 'gini', 'max_depth': 39, 'min_samples_split': 9, 'min_samples_leaf': 3, 'ccp_alpha': 8.261402875363937e-05}\n",
      "Melhor F1 obtido: 0.5814815683445257\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.73      0.63      0.67       417\n",
      "           0       0.80      0.91      0.85      1273\n",
      "           1       0.22      0.07      0.10       168\n",
      "\n",
      "    accuracy                           0.77      1858\n",
      "   macro avg       0.59      0.53      0.54      1858\n",
      "weighted avg       0.73      0.77      0.74      1858\n",
      "\n",
      "[[ 261  149    7]\n",
      " [  81 1161   31]\n",
      " [  15  142   11]]\n",
      "c/oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'criterion': 'entropy', 'max_depth': 46, 'min_samples_split': 12, 'min_samples_leaf': 5, 'ccp_alpha': 0.0031308605576877567}\n",
      "Melhor F1 obtido: 0.6334687194362459\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.79      0.69      0.74      1273\n",
      "           0       0.50      0.52      0.51      1273\n",
      "           1       0.52      0.56      0.54      1273\n",
      "\n",
      "    accuracy                           0.59      3819\n",
      "   macro avg       0.60      0.59      0.60      3819\n",
      "weighted avg       0.60      0.59      0.60      3819\n",
      "\n",
      "[[882 230 161]\n",
      " [121 666 486]\n",
      " [113 447 713]]\n"
     ]
    }
   ],
   "source": [
    "# Count Vectorizer\n",
    "print (\"----COUNT VECTORIZER ----\")\n",
    "count_vec_predicted_DT= decision_tree(X_train_count_vec, y_train, X_val_count_vec, y_val, X_test_count_vec) \n",
    "print(metrics.classification_report(y_test, count_vec_predicted_DT))\n",
    "print(metrics.confusion_matrix(y_test, count_vec_predicted_DT))\n",
    "\n",
    "print (\"c/oversampling\")\n",
    "count_vec_predicted_DT_oversampling= decision_tree(X_train_oversampling_count_vec, y_train_oversampling, X_val_oversampling_count_vec, y_val_oversampling, X_test_oversampling_count_vec) \n",
    "print(metrics.classification_report(y_test_oversampling, count_vec_predicted_DT_oversampling))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, count_vec_predicted_DT_oversampling))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b119b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----TF-IDF ----\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'criterion': 'gini', 'max_depth': 35, 'min_samples_split': 14, 'min_samples_leaf': 1, 'ccp_alpha': 9.782648265672296e-05}\n",
      "Melhor F1 obtido: 0.5694544343259154\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.75      0.59      0.66       417\n",
      "           0       0.79      0.91      0.85      1273\n",
      "           1       0.27      0.11      0.16       168\n",
      "\n",
      "    accuracy                           0.77      1858\n",
      "   macro avg       0.60      0.54      0.56      1858\n",
      "weighted avg       0.74      0.77      0.74      1858\n",
      "\n",
      "[[ 247  159   11]\n",
      " [  75 1157   41]\n",
      " [   9  140   19]]\n",
      "c/oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'criterion': 'entropy', 'max_depth': 33, 'min_samples_split': 3, 'min_samples_leaf': 8, 'ccp_alpha': 0.002283031618215434}\n",
      "Melhor F1 obtido: 0.611706342369522\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.84      0.66      0.74      1273\n",
      "           0       0.58      0.38      0.46      1273\n",
      "           1       0.47      0.73      0.57      1273\n",
      "\n",
      "    accuracy                           0.59      3819\n",
      "   macro avg       0.63      0.59      0.59      3819\n",
      "weighted avg       0.63      0.59      0.59      3819\n",
      "\n",
      "[[841  75 357]\n",
      " [ 91 490 692]\n",
      " [ 74 273 926]]\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF\n",
    "print (\"----TF-IDF ----\")\n",
    "tfidf_vec_predicted_DT= decision_tree(X_train_tfidf_vec, y_train, X_val_tfidf_vec, y_val, X_test_tfidf_vec) \n",
    "print(metrics.classification_report(y_test, tfidf_vec_predicted_DT))\n",
    "print(metrics.confusion_matrix(y_test, tfidf_vec_predicted_DT))\n",
    "\n",
    "print (\"c/oversampling\")\n",
    "tfidf_vec_predicted_DT_oversampling= decision_tree(X_train_oversampling_tfidf_vec, y_train_oversampling, X_val_oversampling_tfidf_vec, y_val_oversampling, X_test_oversampling_tfidf_vec) \n",
    "print(metrics.classification_report(y_test_oversampling, tfidf_vec_predicted_DT_oversampling))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, tfidf_vec_predicted_DT_oversampling))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2026acb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count vec + tf idf\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'criterion': 'gini', 'max_depth': 45, 'min_samples_split': 12, 'min_samples_leaf': 7, 'ccp_alpha': 0.0007415517020819152}\n",
      "Melhor F1 obtido: 0.584560111125576\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.70      0.68      0.69       417\n",
      "           0       0.81      0.88      0.85      1273\n",
      "           1       0.21      0.09      0.13       168\n",
      "\n",
      "    accuracy                           0.76      1858\n",
      "   macro avg       0.57      0.55      0.55      1858\n",
      "weighted avg       0.73      0.76      0.74      1858\n",
      "\n",
      "[[ 282  123   12]\n",
      " [ 107 1122   44]\n",
      " [  16  137   15]]\n",
      "oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'criterion': 'entropy', 'max_depth': 34, 'min_samples_split': 3, 'min_samples_leaf': 4, 'ccp_alpha': 0.0020992530445319074}\n",
      "Melhor F1 obtido: 0.598337283052171\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.81      0.65      0.72      1273\n",
      "           0       0.46      0.55      0.50      1273\n",
      "           1       0.53      0.53      0.53      1273\n",
      "\n",
      "    accuracy                           0.58      3819\n",
      "   macro avg       0.60      0.58      0.59      3819\n",
      "weighted avg       0.60      0.58      0.59      3819\n",
      "\n",
      "[[830 298 145]\n",
      " [108 701 464]\n",
      " [ 83 509 681]]\n"
     ]
    }
   ],
   "source": [
    "print(\"count vec + tf idf\")\n",
    "freq_predicted_DT= decision_tree(X_train_freq, y_train,X_val_freq,y_val, X_test_freq, n_trials=30) \n",
    "print(metrics.classification_report(y_test,freq_predicted_DT ))\n",
    "print(metrics.confusion_matrix(y_test, freq_predicted_DT))\n",
    "\n",
    "print(\"oversampling\")\n",
    "freq_predicted_DT_over= decision_tree(X_train_freq_over, y_train_oversampling, X_val_freq_over, y_val_oversampling, X_test_freq_over, n_trials=30) \n",
    "print(metrics.classification_report(y_test_oversampling,freq_predicted_DT_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, freq_predicted_DT_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d288c09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----WORD2VEC ----\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'criterion': 'entropy', 'max_depth': 17, 'min_samples_split': 7, 'min_samples_leaf': 8, 'ccp_alpha': 0.00038722651780731754}\n",
      "Melhor F1 obtido: 0.4334001660962159\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.36      0.37      0.37       417\n",
      "           0       0.73      0.75      0.74      1273\n",
      "           1       0.14      0.11      0.12       168\n",
      "\n",
      "    accuracy                           0.61      1858\n",
      "   macro avg       0.41      0.41      0.41      1858\n",
      "weighted avg       0.60      0.61      0.60      1858\n",
      "\n",
      "[[156 235  26]\n",
      " [235 952  86]\n",
      " [ 37 113  18]]\n",
      "c/oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'criterion': 'gini', 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 2, 'ccp_alpha': 0.005998249214750752}\n",
      "Melhor F1 obtido: 0.45154675474940226\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.46      0.44      0.45      1273\n",
      "           0       0.58      0.16      0.25      1273\n",
      "           1       0.37      0.66      0.48      1273\n",
      "\n",
      "    accuracy                           0.42      3819\n",
      "   macro avg       0.47      0.42      0.39      3819\n",
      "weighted avg       0.47      0.42      0.39      3819\n",
      "\n",
      "[[557  80 636]\n",
      " [286 201 786]\n",
      " [362  65 846]]\n"
     ]
    }
   ],
   "source": [
    "#Word2vec\n",
    "print (\"----WORD2VEC ----\")\n",
    "word2vec_predicted_DT= decision_tree(X_train_word2vec, y_train, X_val_word2vec, y_val, X_test_word2vec) \n",
    "print(metrics.classification_report(y_test, word2vec_predicted_DT))\n",
    "print(metrics.confusion_matrix(y_test, word2vec_predicted_DT))\n",
    "\n",
    "print (\"c/oversampling\")\n",
    "word2vec_predicted_DT_over= decision_tree(X_train_oversampling_word2vec, y_train_oversampling, X_val_oversampling_word2vec, y_val_oversampling, X_test_oversampling_word2vec) \n",
    "print(metrics.classification_report(y_test_oversampling, word2vec_predicted_DT_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, word2vec_predicted_DT_over))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5aed8bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores hiperparâmetros encontrados:\n",
      "{'criterion': 'gini', 'max_depth': 45, 'min_samples_split': 7, 'min_samples_leaf': 7, 'ccp_alpha': 0.0014702145931244676}\n",
      "Melhor F1 obtido: 0.5631249729118634\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.65      0.62      0.64       417\n",
      "           0       0.79      0.88      0.84      1273\n",
      "           1       0.21      0.06      0.09       168\n",
      "\n",
      "    accuracy                           0.75      1858\n",
      "   macro avg       0.55      0.52      0.52      1858\n",
      "weighted avg       0.71      0.75      0.72      1858\n",
      "\n",
      "[[ 259  151    7]\n",
      " [ 119 1124   30]\n",
      " [  19  139   10]]\n",
      "oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'criterion': 'gini', 'max_depth': 38, 'min_samples_split': 15, 'min_samples_leaf': 3, 'ccp_alpha': 0.008492931634692088}\n",
      "Melhor F1 obtido: 0.5798134579113285\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.62      0.69      0.65      1273\n",
      "           0       0.47      0.59      0.52      1273\n",
      "           1       0.56      0.36      0.43      1273\n",
      "\n",
      "    accuracy                           0.54      3819\n",
      "   macro avg       0.55      0.54      0.54      3819\n",
      "weighted avg       0.55      0.54      0.54      3819\n",
      "\n",
      "[[875 280 118]\n",
      " [280 749 244]\n",
      " [265 556 452]]\n"
     ]
    }
   ],
   "source": [
    "#word2vec treinado com os nossos dados\n",
    "word2vec_trained_predicted_DT= decision_tree(X_train_w2v_trained, y_train, X_val_w2v_trained, y_val, X_test_w2v_trained, n_trials=30) \n",
    "print(metrics.classification_report(y_test, word2vec_trained_predicted_DT))\n",
    "print(metrics.confusion_matrix(y_test, word2vec_trained_predicted_DT))\n",
    "\n",
    "print(\"oversampling\")\n",
    "word2vec_trained_predicted_DT_over= decision_tree(X_train_over_w2v_trained, y_train_oversampling, X_val_over_w2v_trained, y_val_oversampling,X_test_over_w2v_trained, n_trials=30) \n",
    "print(metrics.classification_report(y_test_oversampling, word2vec_trained_predicted_DT_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, word2vec_trained_predicted_DT_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c77bced6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- GLOVE ----\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'criterion': 'gini', 'max_depth': 22, 'min_samples_split': 13, 'min_samples_leaf': 6, 'ccp_alpha': 0.0004031815005887063}\n",
      "Melhor F1 obtido: 0.42920668292586656\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.39      0.41      0.40       417\n",
      "           0       0.72      0.75      0.74      1273\n",
      "           1       0.08      0.05      0.06       168\n",
      "\n",
      "    accuracy                           0.61      1858\n",
      "   macro avg       0.40      0.40      0.40      1858\n",
      "weighted avg       0.59      0.61      0.60      1858\n",
      "\n",
      "[[172 235  10]\n",
      " [236 959  78]\n",
      " [ 31 129   8]]\n",
      "c/oversampling\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'criterion': 'gini', 'max_depth': 41, 'min_samples_split': 18, 'min_samples_leaf': 4, 'ccp_alpha': 0.0030723986184719456}\n",
      "Melhor F1 obtido: 0.489649414594912\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.48      0.44      0.46      1273\n",
      "           0       0.40      0.40      0.40      1273\n",
      "           1       0.43      0.47      0.45      1273\n",
      "\n",
      "    accuracy                           0.44      3819\n",
      "   macro avg       0.44      0.44      0.44      3819\n",
      "weighted avg       0.44      0.44      0.44      3819\n",
      "\n",
      "[[563 389 321]\n",
      " [310 513 450]\n",
      " [288 393 592]]\n"
     ]
    }
   ],
   "source": [
    "#Glove\n",
    "print (\"---- GLOVE ----\")\n",
    "glove_predicted_DT= decision_tree(X_train_glove, y_train, X_val_glove, y_val, X_test_glove) \n",
    "print(metrics.classification_report(y_test, glove_predicted_DT))\n",
    "print(metrics.confusion_matrix(y_test, glove_predicted_DT))\n",
    "\n",
    "print (\"c/oversampling\")\n",
    "glove_predicted_DT_over= decision_tree(X_train_glove_oversampling, y_train_oversampling, X_val_glove_oversampling, y_val_oversampling, X_test_glove_oversampling) \n",
    "print(metrics.classification_report(y_test_oversampling, glove_predicted_DT_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, glove_predicted_DT_over))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033b484c",
   "metadata": {},
   "source": [
    "## Random Florest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ba3a7257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def objective_random_forest(trial, X_train, X_val, y_train, y_val):\n",
    "    # Hiperparâmetros a otimizar\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 300, step=50)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 5, 50)\n",
    "    max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None])\n",
    "    criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"])\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "\n",
    "    # Criar o modelo\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        max_features=max_features,\n",
    "        criterion=criterion,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    f1 = f1_score(y_val, y_pred, average=\"macro\")\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "def random_florest(X_train, y_train, X_val, y_val, X_test, n_trials=50):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective_random_forest(trial, X_train, X_val, y_train, y_val),\n",
    "                   n_trials=n_trials, n_jobs=-1, show_progress_bar=False)\n",
    "\n",
    "    print(\"Melhores hiperparâmetros Random Forest:\")\n",
    "    print(study.best_params)\n",
    "    print(\"Melhor F1 obtido:\", study.best_value)\n",
    "\n",
    "    best_model = RandomForestClassifier(**study.best_params, random_state=42, n_jobs=-1)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "    return y_pred_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1030113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----COUNT VECTORIZER ----\n"
     ]
    }
   ],
   "source": [
    "# Count Vectorizer\n",
    "print (\"----COUNT VECTORIZER ----\")\n",
    "count_vec_predicted_RF= random_florest(X_train_count_vec, y_train, X_val_count_vec, y_val, X_test_count_vec) \n",
    "print(metrics.classification_report(y_test, count_vec_predicted_RF))\n",
    "print(metrics.confusion_matrix(y_test, count_vec_predicted_RF))\n",
    "\n",
    "print (\"c/oversampling\")\n",
    "count_vec_predicted_RF_over= random_florest(X_train_oversampling_count_vec, y_train_oversampling, X_val_oversampling_count_vec, y_val_oversampling, X_test_oversampling_count_vec) \n",
    "print(metrics.classification_report(y_test_oversampling, count_vec_predicted_RF_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, count_vec_predicted_RF_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80463ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----TF-IDF ----\n",
      "Melhores hiperparâmetros Random Forest:\n",
      "{'n_estimators': 50, 'max_depth': 46, 'max_features': None, 'criterion': 'gini', 'min_samples_split': 16, 'min_samples_leaf': 4}\n",
      "Melhor F1 obtido: 0.5709541203082218\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.79      0.64      0.71       417\n",
      "           0       0.80      0.95      0.87      1273\n",
      "           1       0.50      0.04      0.07       168\n",
      "\n",
      "    accuracy                           0.80      1858\n",
      "   macro avg       0.70      0.54      0.55      1858\n",
      "weighted avg       0.77      0.80      0.76      1858\n",
      "\n",
      "[[ 266  149    2]\n",
      " [  61 1208    4]\n",
      " [   8  154    6]]\n",
      "c/oversampling\n",
      "Melhores hiperparâmetros Random Forest:\n",
      "{'n_estimators': 300, 'max_depth': 45, 'max_features': 'sqrt', 'criterion': 'entropy', 'min_samples_split': 16, 'min_samples_leaf': 9}\n",
      "Melhor F1 obtido: 0.6739397514704909\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.83      0.73      0.78      1273\n",
      "           0       0.58      0.68      0.63      1273\n",
      "           1       0.60      0.57      0.58      1273\n",
      "\n",
      "    accuracy                           0.66      3819\n",
      "   macro avg       0.67      0.66      0.66      3819\n",
      "weighted avg       0.67      0.66      0.66      3819\n",
      "\n",
      "[[932 164 177]\n",
      " [103 869 301]\n",
      " [ 93 457 723]]\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF\n",
    "print (\"----TF-IDF ----\")\n",
    "tfidf_vec_predicted_RF= random_florest(X_train_tfidf_vec, y_train, X_val_tfidf_vec, y_val, X_test_tfidf_vec) \n",
    "print(metrics.classification_report(y_test, tfidf_vec_predicted_RF))\n",
    "print(metrics.confusion_matrix(y_test, tfidf_vec_predicted_RF))\n",
    "\n",
    "#TF-IDF\n",
    "print (\"c/oversampling\")\n",
    "tfidf_vec_predicted_RF_over= random_florest(X_train_oversampling_tfidf_vec, y_train_oversampling, X_val_oversampling_tfidf_vec, y_val_oversampling, X_test_oversampling_tfidf_vec) \n",
    "print(metrics.classification_report(y_test_oversampling, tfidf_vec_predicted_RF_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, tfidf_vec_predicted_RF_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2542f850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count vec + tf idf\n",
      "Melhores hiperparâmetros Random Forest:\n",
      "{'n_estimators': 50, 'max_depth': 50, 'max_features': 'sqrt', 'criterion': 'gini', 'min_samples_split': 16, 'min_samples_leaf': 1}\n",
      "Melhor F1 obtido: 0.5612731912545693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.83      0.67      0.74       417\n",
      "           0       0.80      0.96      0.88      1273\n",
      "           1       0.00      0.00      0.00       168\n",
      "\n",
      "    accuracy                           0.81      1858\n",
      "   macro avg       0.54      0.54      0.54      1858\n",
      "weighted avg       0.74      0.81      0.77      1858\n",
      "\n",
      "[[ 278  139    0]\n",
      " [  50 1223    0]\n",
      " [   8  160    0]]\n",
      "oversampling\n",
      "Melhores hiperparâmetros Random Forest:\n",
      "{'n_estimators': 200, 'max_depth': 14, 'max_features': 'sqrt', 'criterion': 'entropy', 'min_samples_split': 14, 'min_samples_leaf': 9}\n",
      "Melhor F1 obtido: 0.6807992256892508\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.81      0.69      0.74      1273\n",
      "           0       0.59      0.61      0.60      1273\n",
      "           1       0.53      0.60      0.56      1273\n",
      "\n",
      "    accuracy                           0.63      3819\n",
      "   macro avg       0.64      0.63      0.64      3819\n",
      "weighted avg       0.64      0.63      0.64      3819\n",
      "\n",
      "[[875 124 274]\n",
      " [106 776 391]\n",
      " [102 410 761]]\n"
     ]
    }
   ],
   "source": [
    "print(\"count vec + tf idf\")\n",
    "freq_predicted_RF= random_florest(X_train_freq, y_train,X_val_freq,y_val, X_test_freq, n_trials=30) \n",
    "print(metrics.classification_report(y_test,freq_predicted_RF))\n",
    "print(metrics.confusion_matrix(y_test, freq_predicted_RF))\n",
    "\n",
    "print(\"oversampling\")\n",
    "freq_predicted_RF_over= random_florest(X_train_freq_over, y_train_oversampling, X_val_freq_over, y_val_oversampling, X_test_freq_over, n_trials=30) \n",
    "print(metrics.classification_report(y_test_oversampling,freq_predicted_RF_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, freq_predicted_RF_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6516285a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----WORD2VEC ----\n"
     ]
    }
   ],
   "source": [
    "#Word2vec\n",
    "print (\"----WORD2VEC ----\")\n",
    "word2vec_predicted_RF= random_florest(X_train_word2vec, y_train, X_val_word2vec, y_val, X_test_word2vec, n_trials = 30) \n",
    "print(metrics.classification_report(y_test, word2vec_predicted_RF))\n",
    "print(metrics.confusion_matrix(y_test, word2vec_predicted_RF))\n",
    "\n",
    "print (\"c/oversampling\")\n",
    "word2vec_predicted_RF_over= random_florest(X_train_oversampling_word2vec, y_train_oversampling, X_val_oversampling_word2vec, y_val_oversampling, X_test_oversampling_word2vec, n_trials=30) \n",
    "print(metrics.classification_report(y_test_oversampling, word2vec_predicted_RF_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, word2vec_predicted_RF_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa17c8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec treinado com os nossos dados\n",
    "word2vec_trained_predicted_RF= random_florest(X_train_w2v_trained, y_train, X_val_w2v_trained, y_val, X_test_w2v_trained, n_trials=30) \n",
    "print(metrics.classification_report(y_test, word2vec_trained_predicted_RF))\n",
    "print(metrics.confusion_matrix(y_test, word2vec_trained_predicted_RF))\n",
    "\n",
    "print(\"oversampling\")\n",
    "word2vec_trained_predicted_RF_over= svm(X_train_over_w2v_trained, y_train_oversampling, X_val_over_w2v_trained, y_val_oversampling,X_test_over_w2v_trained, n_trials=30) \n",
    "print(metrics.classification_report(y_test_oversampling, word2vec_trained_predicted_RF_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, word2vec_trained_predicted_RF_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7c1907",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Glove\n",
    "print (\"---- GLOVE ----\")\n",
    "glove_predicted_RF= random_florest(X_train_glove, y_train, X_val_glove, y_val, X_test_glove) \n",
    "print(metrics.classification_report(y_test, glove_predicted_DT))\n",
    "print(metrics.confusion_matrix(y_test, glove_predicted_DT))\n",
    "\n",
    "print (\"c/oversampling\")\n",
    "glove_predicted_RF_over= random_florest(X_train_glove_oversampling, y_train_oversampling, X_val_glove_oversampling, y_val_oversampling, X_test_glove_oversampling) \n",
    "print(metrics.classification_report(y_test_oversampling, glove_predicted_RF_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, glove_predicted_RF_over))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01765274",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a144959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import optuna\n",
    "\n",
    "def objective(X_train, y_train, X_val, y_val, trial):\n",
    "    hidden_layer_sizes = trial.suggest_categorical('hidden_layer_sizes', [50, 100, 200])\n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'tanh'])\n",
    "    learning_rate_init = trial.suggest_float('learning_rate_init', 1e-4, 1e-2, log=True)\n",
    "\n",
    "    clf = MLPClassifier(\n",
    "        hidden_layer_sizes=hidden_layer_sizes,\n",
    "        activation=activation,\n",
    "        learning_rate_init=learning_rate_init,\n",
    "        max_iter=300,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_val)\n",
    "    f1 = f1_score(y_val, preds, average='macro')\n",
    "    return f1\n",
    "\n",
    "def neural_network(X_train, y_train, X_val, y_val, X_test):\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(X_train, y_train, X_val, y_val, trial), n_trials=30)\n",
    "    print(\"Melhores parâmetros:\", study.best_params)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_model = MLPClassifier(\n",
    "        hidden_layer_sizes=best_params['hidden_layer_sizes'],\n",
    "        activation=best_params['activation'],\n",
    "        learning_rate_init=best_params['learning_rate_init'],\n",
    "        max_iter=300,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    best_model.fit(X_train, y_train)\n",
    "    previsoes = best_model.predict(X_test)\n",
    "    \n",
    "    return previsoes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0898740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----COUNT VECTORIZER ----\n",
      "Melhores parâmetros: {'hidden_layer_sizes': 200, 'activation': 'relu', 'learning_rate_init': 0.0005399312943774714}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.80      0.70      0.75       417\n",
      "           0       0.83      0.91      0.87      1273\n",
      "           1       0.27      0.17      0.21       168\n",
      "\n",
      "    accuracy                           0.79      1858\n",
      "   macro avg       0.63      0.59      0.61      1858\n",
      "weighted avg       0.77      0.79      0.78      1858\n",
      "\n",
      "[[ 293  106   18]\n",
      " [  60 1154   59]\n",
      " [  13  127   28]]\n",
      "c/oversampling\n",
      "Melhores parâmetros: {'hidden_layer_sizes': 200, 'activation': 'relu', 'learning_rate_init': 0.006632735747280561}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.83      0.69      0.76      1273\n",
      "           0       0.46      0.91      0.61      1273\n",
      "           1       0.76      0.15      0.25      1273\n",
      "\n",
      "    accuracy                           0.58      3819\n",
      "   macro avg       0.68      0.58      0.54      3819\n",
      "weighted avg       0.68      0.58      0.54      3819\n",
      "\n",
      "[[ 884  365   24]\n",
      " [  80 1156   37]\n",
      " [  96  988  189]]\n"
     ]
    }
   ],
   "source": [
    "# Count Vectorizer\n",
    "print (\"----COUNT VECTORIZER ----\")\n",
    "count_vec_predicted_NN= neural_network(X_train_count_vec, y_train, X_val_count_vec, y_val, X_test_count_vec) \n",
    "print(metrics.classification_report(y_test, count_vec_predicted_NN))\n",
    "print(metrics.confusion_matrix(y_test, count_vec_predicted_NN))\n",
    "\n",
    "print (\"c/oversampling\")\n",
    "count_vec_predicted_NN_over= neural_network(X_train_oversampling_count_vec, y_train_oversampling, X_val_oversampling_count_vec, y_val_oversampling, X_test_oversampling_count_vec) \n",
    "print(metrics.classification_report(y_test_oversampling, count_vec_predicted_NN_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, count_vec_predicted_NN_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b4947e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----TF-IDF ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros: {'hidden_layer_sizes': 200, 'activation': 'relu', 'learning_rate_init': 0.005032257075048991}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.72      0.64      0.68       417\n",
      "           0       0.82      0.87      0.84      1273\n",
      "           1       0.29      0.26      0.28       168\n",
      "\n",
      "    accuracy                           0.76      1858\n",
      "   macro avg       0.61      0.59      0.60      1858\n",
      "weighted avg       0.75      0.76      0.76      1858\n",
      "\n",
      "[[ 267  128   22]\n",
      " [  87 1102   84]\n",
      " [  15  109   44]]\n",
      "c/oversampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros: {'hidden_layer_sizes': 200, 'activation': 'relu', 'learning_rate_init': 0.006583884252451114}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.81      0.63      0.71      1273\n",
      "           0       0.46      0.83      0.59      1273\n",
      "           1       0.57      0.25      0.34      1273\n",
      "\n",
      "    accuracy                           0.57      3819\n",
      "   macro avg       0.62      0.57      0.55      3819\n",
      "weighted avg       0.62      0.57      0.55      3819\n",
      "\n",
      "[[ 806  368   99]\n",
      " [  81 1057  135]\n",
      " [ 104  856  313]]\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF\n",
    "print (\"----TF-IDF ----\")\n",
    "tfidf_vec_predicted_NN= neural_network(X_train_tfidf_vec, y_train, X_val_tfidf_vec, y_val, X_test_tfidf_vec) \n",
    "print(metrics.classification_report(y_test, tfidf_vec_predicted_NN))\n",
    "print(metrics.confusion_matrix(y_test, tfidf_vec_predicted_NN))\n",
    "\n",
    "#TF-IDF\n",
    "print (\"c/oversampling\")\n",
    "tfidf_vec_predicted_NN_over= neural_network(X_train_oversampling_tfidf_vec, y_train_oversampling, X_val_oversampling_tfidf_vec, y_val_oversampling, X_test_oversampling_tfidf_vec) \n",
    "print(metrics.classification_report(y_test_oversampling, tfidf_vec_predicted_NN_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, tfidf_vec_predicted_NN_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9242ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count vec + tf idf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:788: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "print(\"count vec + tf idf\")\n",
    "freq_predicted_NN= neural_network(X_train_freq, y_train,X_val_freq,y_val, X_test_freq) \n",
    "print(metrics.classification_report(y_test,freq_predicted_NN))\n",
    "print(metrics.confusion_matrix(y_test, freq_predicted_NN))\n",
    "\n",
    "print(\"oversampling\")\n",
    "freq_predicted_NN_over= neural_network(X_train_freq_over, y_train_oversampling, X_val_freq_over, y_val_oversampling, X_test_freq_over) \n",
    "print(metrics.classification_report(y_test_oversampling,freq_predicted_NN_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, freq_predicted_NN_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03af9815",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2vec\n",
    "print (\"----WORD2VEC ----\")\n",
    "word2vec_predicted_NN= neural_network(X_train_word2vec, y_train, X_val_word2vec, y_val, X_test_word2vec) \n",
    "print(metrics.classification_report(y_test, word2vec_predicted_NN))\n",
    "print(metrics.confusion_matrix(y_test, word2vec_predicted_NN))\n",
    "\n",
    "print (\"c/oversampling\")\n",
    "word2vec_predicted_NN_over= neural_network(X_train_oversampling_word2vec, y_train_oversampling, X_val_oversampling_word2vec, y_val_oversampling, X_test_oversampling_word2vec) \n",
    "print(metrics.classification_report(y_test_oversampling, word2vec_predicted_NN_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, word2vec_predicted_NN_over))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2cd2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec treinado com os nossos dados\n",
    "word2vec_trained_predicted_NN= neural_network(X_train_w2v_trained, y_train, X_val_w2v_trained, y_val, X_test_w2v_trained, n_trials=30) \n",
    "print(metrics.classification_report(y_test, word2vec_trained_predicted_NN))\n",
    "print(metrics.confusion_matrix(y_test, word2vec_trained_predicted_NN))\n",
    "\n",
    "print(\"oversampling\")\n",
    "word2vec_trained_predicted_NN_over= svm(X_train_over_w2v_trained, y_train_oversampling, X_val_over_w2v_trained, y_val_oversampling,X_test_over_w2v_trained, n_trials=30) \n",
    "print(metrics.classification_report(y_test_oversampling, word2vec_trained_predicted_NN_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, word2vec_trained_predicted_NN_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3c3b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Glove\n",
    "print (\"----GLOVE ----\")\n",
    "glove_predicted_NN= neural_network(X_train_glove, y_train, X_val_glove, y_val, X_test_glove) \n",
    "print(metrics.classification_report(y_test, glove_predicted_NN))\n",
    "print(metrics.confusion_matrix(y_test, glove_predicted_NN))\n",
    "\n",
    "print (\"c/oversampling\")\n",
    "glove_predicted_NN_over= neural_network(X_train_glove_oversampling, y_train_oversampling, X_val_glove_oversampling, y_val_oversampling, X_test_glove_oversampling) \n",
    "print(metrics.classification_report(y_test_oversampling, glove_predicted_NN_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, glove_predicted_NN_over))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f827bcd",
   "metadata": {},
   "source": [
    "## Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d051225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "import optuna\n",
    "\n",
    "def objective(X_train, y_train, X_val, y_val, trial):\n",
    "    # Hiperparâmetros a otimizar\n",
    "    C = trial.suggest_float('C', 1e-3, 1e3, log=True)\n",
    "    penalty = trial.suggest_categorical('penalty', ['l1', 'l2'])\n",
    "    solver = trial.suggest_categorical('solver', ['liblinear', 'saga'])  # compatíveis com L1 e L2\n",
    "    max_iter = trial.suggest_int('max_iter', 100, 1000)\n",
    "\n",
    "    model = LogisticRegression(\n",
    "        C=C,\n",
    "        penalty=penalty,\n",
    "        solver=solver,\n",
    "        max_iter=max_iter,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_val)\n",
    "\n",
    "    f1 = f1_score(y_val, preds, average='macro')\n",
    "    return f1\n",
    "\n",
    "\n",
    "def logistic_regression(X_train, y_train, X_val, y_val, X_test):\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(X_train, y_train, X_val, y_val, trial), n_trials=30)\n",
    "\n",
    "    print(\"Melhores parâmetros encontrados:\")\n",
    "    print(study.best_params)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_model = LogisticRegression(\n",
    "        **best_params,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    previsoes = best_model.predict(X_test)\n",
    "\n",
    "    return previsoes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e03c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-13 01:38:47,774] A new study created in memory with name: no-name-f804a6c1-a9ec-4956-92ee-a72dd1f88037\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:38:47,815] Trial 0 finished with value: 0.5759647633129582 and parameters: {'C': 0.5702975431962373, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 434}. Best is trial 0 with value: 0.5759647633129582.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:38:47,838] Trial 1 finished with value: 0.4817582369487045 and parameters: {'C': 0.024178449537411693, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 958}. Best is trial 0 with value: 0.5759647633129582.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----COUNT VECTORIZER ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:38:48,033] Trial 2 finished with value: 0.6556576016190188 and parameters: {'C': 100.62436324687397, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 135}. Best is trial 2 with value: 0.6556576016190188.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:38:48,752] Trial 3 finished with value: 0.6504041189506938 and parameters: {'C': 162.82595003814305, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 567}. Best is trial 2 with value: 0.6556576016190188.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:38:49,360] Trial 4 finished with value: 0.48280955931962644 and parameters: {'C': 0.020068665251334997, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 452}. Best is trial 2 with value: 0.6556576016190188.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:38:49,472] Trial 5 finished with value: 0.6133551317639342 and parameters: {'C': 39.57151379943558, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 615}. Best is trial 2 with value: 0.6556576016190188.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:38:49,508] Trial 6 finished with value: 0.5098039215686274 and parameters: {'C': 0.294397046750876, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 992}. Best is trial 2 with value: 0.6556576016190188.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:38:49,547] Trial 7 finished with value: 0.5705119006104282 and parameters: {'C': 0.20205435998066762, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 776}. Best is trial 2 with value: 0.6556576016190188.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:38:49,559] Trial 8 finished with value: 0.4234293621241545 and parameters: {'C': 0.003557600040672028, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 275}. Best is trial 2 with value: 0.6556576016190188.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:38:49,743] Trial 9 finished with value: 0.6077744553829092 and parameters: {'C': 553.2979230436213, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 537}. Best is trial 2 with value: 0.6556576016190188.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:38:59,219] Trial 10 finished with value: 0.6552789886034468 and parameters: {'C': 9.916768695437854, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 127}. Best is trial 2 with value: 0.6556576016190188.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:39:07,423] Trial 11 finished with value: 0.6647095405734628 and parameters: {'C': 15.282658311116817, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 104}. Best is trial 11 with value: 0.6647095405734628.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:39:16,389] Trial 12 finished with value: 0.6637222489559013 and parameters: {'C': 9.85712118634492, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 114}. Best is trial 11 with value: 0.6647095405734628.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:39:36,151] Trial 13 finished with value: 0.6622131951818232 and parameters: {'C': 14.088989037426721, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 252}. Best is trial 11 with value: 0.6647095405734628.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:39:48,662] Trial 14 finished with value: 0.6770186152390382 and parameters: {'C': 3.4457245926565587, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 277}. Best is trial 14 with value: 0.6770186152390382.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:39:58,891] Trial 15 finished with value: 0.6650737387526919 and parameters: {'C': 2.361814418410887, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 277}. Best is trial 14 with value: 0.6770186152390382.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:40:10,975] Trial 16 finished with value: 0.6650737387526919 and parameters: {'C': 2.3236036584634028, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 308}. Best is trial 14 with value: 0.6770186152390382.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:40:22,669] Trial 17 finished with value: 0.6594394079862055 and parameters: {'C': 1.9281679823370776, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 380}. Best is trial 14 with value: 0.6770186152390382.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:40:23,784] Trial 18 finished with value: 0.4211064817524936 and parameters: {'C': 0.048462891612876466, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 664}. Best is trial 14 with value: 0.6770186152390382.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:40:34,035] Trial 19 finished with value: 0.6677610475806594 and parameters: {'C': 3.068460582723694, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 220}. Best is trial 14 with value: 0.6770186152390382.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:41:07,142] Trial 20 finished with value: 0.6496410997208824 and parameters: {'C': 722.5038133378972, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 366}. Best is trial 14 with value: 0.6770186152390382.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:41:16,031] Trial 21 finished with value: 0.6647007988963143 and parameters: {'C': 2.2792048244156153, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 234}. Best is trial 14 with value: 0.6770186152390382.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:41:16,399] Trial 22 finished with value: 0.47707987986016515 and parameters: {'C': 0.10833353334598418, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 174}. Best is trial 14 with value: 0.6770186152390382.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:41:20,878] Trial 23 finished with value: 0.6345127108226912 and parameters: {'C': 1.121590028412787, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 207}. Best is trial 14 with value: 0.6770186152390382.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:41:37,844] Trial 24 finished with value: 0.6659089482536446 and parameters: {'C': 3.941867775587999, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 334}. Best is trial 14 with value: 0.6770186152390382.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:42:01,263] Trial 25 finished with value: 0.662745322386999 and parameters: {'C': 5.689394883043907, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 374}. Best is trial 14 with value: 0.6770186152390382.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:42:42,737] Trial 26 finished with value: 0.6481329413472314 and parameters: {'C': 56.76384691306071, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 475}. Best is trial 14 with value: 0.6770186152390382.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:42:45,786] Trial 27 finished with value: 0.5909131725514046 and parameters: {'C': 0.5544105718153584, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 322}. Best is trial 14 with value: 0.6770186152390382.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:00,598] Trial 28 finished with value: 0.6622350604364993 and parameters: {'C': 26.528181254690583, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 193}. Best is trial 14 with value: 0.6770186152390382.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:21,725] Trial 29 finished with value: 0.6580010632642211 and parameters: {'C': 4.299736531448158, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 402}. Best is trial 14 with value: 0.6770186152390382.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros encontrados:\n",
      "{'C': 3.4457245926565587, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 277}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:34,711] A new study created in memory with name: no-name-8b78f7ef-34bf-4cd4-ae02-5e635fa2a503\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:34,881] Trial 0 finished with value: 0.677824372328581 and parameters: {'C': 1.6684911551814994, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 894}. Best is trial 0 with value: 0.677824372328581.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.77      0.64      0.70       417\n",
      "           0       0.81      0.91      0.86      1273\n",
      "           1       0.34      0.18      0.24       168\n",
      "\n",
      "    accuracy                           0.78      1858\n",
      "   macro avg       0.64      0.58      0.60      1858\n",
      "weighted avg       0.76      0.78      0.77      1858\n",
      "\n",
      "[[ 268  139   10]\n",
      " [  67 1159   47]\n",
      " [  13  125   30]]\n",
      "c/oversampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-13 01:43:36,823] Trial 1 finished with value: 0.7002086643251745 and parameters: {'C': 0.01636145823939285, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 969}. Best is trial 1 with value: 0.7002086643251745.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:37,006] Trial 2 finished with value: 0.6204439095503163 and parameters: {'C': 41.268624677841466, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 568}. Best is trial 1 with value: 0.7002086643251745.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:37,040] Trial 3 finished with value: 0.6221880187008261 and parameters: {'C': 0.0012823722959344008, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 808}. Best is trial 1 with value: 0.7002086643251745.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:39,220] Trial 4 finished with value: 0.6401537638542519 and parameters: {'C': 122.8421119866404, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 649}. Best is trial 1 with value: 0.7002086643251745.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:39,356] Trial 5 finished with value: 0.6570220088060158 and parameters: {'C': 2.294571552068283, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 665}. Best is trial 1 with value: 0.7002086643251745.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:39,867] Trial 6 finished with value: 0.6667492440822625 and parameters: {'C': 0.0038026738684157663, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 169}. Best is trial 1 with value: 0.7002086643251745.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:40,007] Trial 7 finished with value: 0.7006355543663503 and parameters: {'C': 1.3062076911105878, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 870}. Best is trial 7 with value: 0.7006355543663503.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:40,085] Trial 8 finished with value: 0.7168878035705196 and parameters: {'C': 0.1084583216913827, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 263}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:41,991] Trial 9 finished with value: 0.6401537638542519 and parameters: {'C': 837.6202799019968, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 623}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:42,055] Trial 10 finished with value: 0.6193004947549531 and parameters: {'C': 0.05931491306642158, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 286}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:42,122] Trial 11 finished with value: 0.6989057396281192 and parameters: {'C': 0.13776121438497146, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 394}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:42,207] Trial 12 finished with value: 0.6854682321308969 and parameters: {'C': 0.3379765746526792, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 415}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:42,483] Trial 13 finished with value: 0.6720197336223274 and parameters: {'C': 5.657299044318926, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 102}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:42,754] Trial 14 finished with value: 0.6537956850376825 and parameters: {'C': 10.997328673340208, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 798}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:46,933] Trial 15 finished with value: 0.6862934565285325 and parameters: {'C': 0.5011213472790349, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 448}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:46,999] Trial 16 finished with value: 0.5647387556533549 and parameters: {'C': 0.02273326883194768, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 274}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:47,089] Trial 17 finished with value: 0.6855694753307763 and parameters: {'C': 0.11777143518743549, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 756}. Best is trial 8 with value: 0.7168878035705196.\n",
      "[I 2025-11-13 01:43:47,362] Trial 18 finished with value: 0.473367223901145 and parameters: {'C': 0.011088350124596877, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 995}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:47,539] Trial 19 finished with value: 0.6350408393593395 and parameters: {'C': 16.868490702339628, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 517}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:47,731] Trial 20 finished with value: 0.7082776325765678 and parameters: {'C': 0.7359353818334339, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 338}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:47,938] Trial 21 finished with value: 0.7104264295013126 and parameters: {'C': 0.9945873004294815, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 284}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:48,059] Trial 22 finished with value: 0.6905250105198041 and parameters: {'C': 0.36801697067916017, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 280}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:48,132] Trial 23 finished with value: 0.6276268943316103 and parameters: {'C': 0.060651641125637805, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 214}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:48,541] Trial 24 finished with value: 0.6704927005304944 and parameters: {'C': 4.4153442890075265, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 349}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:48,726] Trial 25 finished with value: 0.6924120556187446 and parameters: {'C': 0.563047911345828, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 193}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:50,030] Trial 26 finished with value: 0.706282382316446 and parameters: {'C': 0.1922270598084053, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 345}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:50,107] Trial 27 finished with value: 0.5882229614576487 and parameters: {'C': 0.04035278692823199, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 497}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:50,172] Trial 28 finished with value: 0.670307613404951 and parameters: {'C': 0.006543860084360959, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 102}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-13 01:43:50,303] Trial 29 finished with value: 0.6835157979056753 and parameters: {'C': 1.625244943582272, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 361}. Best is trial 8 with value: 0.7168878035705196.\n",
      "c:\\Users\\didia\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros encontrados:\n",
      "{'C': 0.1084583216913827, 'penalty': 'l2', 'solver': 'liblinear', 'max_iter': 263}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.84      0.77      0.80      1273\n",
      "           0       0.54      0.83      0.65      1273\n",
      "           1       0.72      0.40      0.51      1273\n",
      "\n",
      "    accuracy                           0.67      3819\n",
      "   macro avg       0.70      0.67      0.66      3819\n",
      "weighted avg       0.70      0.67      0.66      3819\n",
      "\n",
      "[[ 979  219   75]\n",
      " [  96 1058  119]\n",
      " [  89  681  503]]\n"
     ]
    }
   ],
   "source": [
    "# Count Vectorizer\n",
    "print (\"----COUNT VECTORIZER ----\")\n",
    "count_vec_predicted_LR= logistic_regression(X_train_count_vec, y_train, X_val_count_vec, y_val, X_test_count_vec) \n",
    "print(metrics.classification_report(y_test, count_vec_predicted_LR))\n",
    "print(metrics.confusion_matrix(y_test, count_vec_predicted_LR))\n",
    "\n",
    "print (\"c/oversampling\")\n",
    "count_vec_predicted_LR_over= logistic_regression(X_train_oversampling_count_vec, y_train_oversampling, X_val_oversampling_count_vec, y_val_oversampling, X_test_oversampling_count_vec) \n",
    "print(metrics.classification_report(y_test_oversampling, count_vec_predicted_LR_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, count_vec_predicted_LR_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d55ab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "print (\"----TF-IDF ----\")\n",
    "tfidf_vec_predicted_LR= logistic_regression(X_train_tfidf_vec, y_train, X_val_tfidf_vec, y_val, X_test_tfidf_vec) \n",
    "print(metrics.classification_report(y_test, tfidf_vec_predicted_LR))\n",
    "print(metrics.confusion_matrix(y_test, tfidf_vec_predicted_LR))\n",
    "\n",
    "#TF-IDF\n",
    "print (\"c/oversampling\")\n",
    "tfidf_vec_predicted_LR_over= logistic_regression(X_train_oversampling_tfidf_vec, y_train_oversampling, X_val_oversampling_tfidf_vec, y_val_oversampling, X_test_oversampling_tfidf_vec) \n",
    "print(metrics.classification_report(y_test_oversampling, tfidf_vec_predicted_LR_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, tfidf_vec_predicted_LR_over))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc322f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"count vec + tf idf\")\n",
    "freq_predicted_LR= logistic_regression(X_train_freq, y_train,X_val_freq,y_val, X_test_freq) \n",
    "print(metrics.classification_report(y_test,freq_predicted_RF))\n",
    "print(metrics.confusion_matrix(y_test, freq_predicted_RF))\n",
    "\n",
    "print(\"oversampling\")\n",
    "freq_predicted_LR_over= logistic_regression(X_train_freq_over, y_train_oversampling, X_val_freq_over, y_val_oversampling, X_test_freq_over) \n",
    "print(metrics.classification_report(y_test_oversampling,freq_predicted_LR_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, freq_predicted_LR_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d486c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2vec\n",
    "print (\"----WORD2VEC ----\")\n",
    "word2vec_predicted_LR= logistic_regression(X_train_word2vec, y_train, X_val_word2vec, y_val, X_test_word2vec) \n",
    "print(metrics.classification_report(y_test, word2vec_predicted_LR))\n",
    "print(metrics.confusion_matrix(y_test, word2vec_predicted_LR))\n",
    "\n",
    "print (\"c/oversampling\")\n",
    "word2vec_predicted_LR_over= logistic_regression(X_train_oversampling_word2vec, y_train_oversampling, X_val_oversampling_word2vec, y_val_oversampling, X_test_oversampling_word2vec) \n",
    "print(metrics.classification_report(y_test_oversampling, word2vec_predicted_LR_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, word2vec_predicted_LR_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dabf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec treinado com os nossos dados\n",
    "word2vec_trained_predicted_LR= logistic_regression(X_train_w2v_trained, y_train, X_val_w2v_trained, y_val, X_test_w2v_trained) \n",
    "print(metrics.classification_report(y_test, word2vec_trained_predicted_LR))\n",
    "print(metrics.confusion_matrix(y_test, word2vec_trained_predicted_LR))\n",
    "\n",
    "print(\"oversampling\")\n",
    "word2vec_trained_predicted_LR_over= logistic_regression(X_train_over_w2v_trained, y_train_oversampling, X_val_over_w2v_trained, y_val_oversampling,X_test_over_w2v_trained) \n",
    "print(metrics.classification_report(y_test_oversampling, word2vec_trained_predicted_LR_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, word2vec_trained_predicted_LR_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c19df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Glove\n",
    "print (\"----GLOVE ----\")\n",
    "glove_predicted_LR= logistic_regression(X_train_glove, y_train, X_val_glove, y_val, X_test_glove) \n",
    "print(metrics.classification_report(y_test, glove_predicted_LR))\n",
    "print(metrics.confusion_matrix(y_test, glove_predicted_LR))\n",
    "\n",
    "print (\"c/oversampling\")\n",
    "glove_predicted_LR_over= logistic_regression(X_train_glove_oversampling, y_train_oversampling, X_val_glove_oversampling, y_val_oversampling, X_test_glove_oversampling) \n",
    "print(metrics.classification_report(y_test_oversampling, glove_predicted_LR_over))\n",
    "print(metrics.confusion_matrix(y_test_oversampling, glove_predicted_LR_over))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
